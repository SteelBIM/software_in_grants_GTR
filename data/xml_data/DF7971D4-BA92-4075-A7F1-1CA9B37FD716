<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.rcuk.ac.uk/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.rcuk.ac.uk:80/organisation/90051600-6EF2-4093-BA8C-2B4B6F550895"><gtr:id>90051600-6EF2-4093-BA8C-2B4B6F550895</gtr:id><gtr:name>University of Dundee</gtr:name><gtr:department>Psychology</gtr:department><gtr:address><gtr:line1>University of Dundee</gtr:line1><gtr:line2>Nethergate</gtr:line2><gtr:postCode>DD1 4HN</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/90051600-6EF2-4093-BA8C-2B4B6F550895"><gtr:id>90051600-6EF2-4093-BA8C-2B4B6F550895</gtr:id><gtr:name>University of Dundee</gtr:name><gtr:address><gtr:line1>University of Dundee</gtr:line1><gtr:line2>Nethergate</gtr:line2><gtr:postCode>DD1 4HN</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.rcuk.ac.uk:80/person/732CD56F-2710-4819-B445-F80966F17AF3"><gtr:id>732CD56F-2710-4819-B445-F80966F17AF3</gtr:id><gtr:firstName>Benjamin</gtr:firstName><gtr:surname>Tatler</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.rcuk.ac.uk:80/projects?ref=ES%2FH04597X%2F1"><gtr:id>DF7971D4-BA92-4075-A7F1-1CA9B37FD716</gtr:id><gtr:title>The timecourse of utilising high-level information in scene perception</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>ES/H04597X/1</gtr:grantReference><gtr:abstractText>&lt;p>Eye movements are guided by high- and low-level factors. Two key high-level factors are knowledge of where targets are likely to appear and knowledge of target appearance. Our research investigated how these two sources of information guide search.&amp;nbsp;We collected eye movement data as participants searched scenes in which the objects appeared in expected or unexpected locations. We also varied the type of information participants were provided with before each search: either a picture of their search target or a word naming their search target.&amp;nbsp;We found that both object appearance and likely target location guide search. Following a picture of the target, search is not strongly influenced by where the target is located. Following a word naming the target, we often search the scene region in which we expect to find the object even when it is not there.&amp;nbsp;These findings reveal the visual system's strategy in utilising high-level information to guide search: where we expect to find an object has a key influence, but this influence is greatly diminished if we have detailed knowledge about the precise appearance of the target object.&lt;/p></gtr:abstractText><gtr:fund><gtr:end>2012-06-14</gtr:end><gtr:funder url="http://gtr.rcuk.ac.uk:80/organisation/924BE15C-91F2-4AAD-941A-3F338324B6AE"><gtr:id>924BE15C-91F2-4AAD-941A-3F338324B6AE</gtr:id><gtr:name>ESRC</gtr:name></gtr:funder><gtr:start>2011-07-15</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>78164</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>Experiment 1 showed that expectations about both target appearance and placement are utilised to guide eye movements from the beginning of scene inspection, orienting the direction of the first saccade. In both search initiation and subsequent scanning, the availability of a visual template was particularly useful when the spatial context of the scene was misleading. The availability of a reliable scene context facilitated search mainly when the template was verbal. Therefore, the visual system can utilise target template guidance and context guidance flexibly from the beginning of search, depending upon the amount and the quality of the available information supplied by these high-level sources.

Experiment 2 explored in more detail the effects of spatial inconsistency in scene search, teasing apart any effect of attentional prioritisation due to inconsistency (context/local information conflict) from the disrupting impact of unreliable expectations. We replicated the findings of Experiment 1 but also found that spatial violations depending on distractors may diminish search efficiency attracting early attention when these objects are the biggest in the target expected region and the target template is not precise (verbal). This experiment thus showed, for the first time in the visual search domain, that objects are initially selected covertly considering their relative size and that subsequent overt selection is made considering object-scene associations processed in extrafoveal vision.

The project has provided key contribution to our understanding of visual search: 
1. Both expectations about target appearance and expectations about target placement in scenes are used to guide search from the first eye movement. 
2. If precise (visual) target information is known prior to search, object placement within the scene is unimportant. If only imprecise (verbal) information is available, search is disrupted if the target is not where expected. 
3. Matching with template features in working memory and processing of object-scene associations occur to a considerable extent already in extrafoveal vision.

The experiments conducted have acted as catalysts for further work in our lab. Three follow-up studies have been run: 
1.The role of object colour, comparing search following coloured or greyscale templates in coloured or grayscale scenes. 24 participants tested. 
2.The importance of assimilation time after the search template, comparing 100ms/900ms ISIs between template and scene following verbal or visual information. 48 participants tested. 
3.Functional specialisation of the cerebral hemispheres in early search guidance. When targets in scenes are lateralised the hemispheric specialisation for language (LH) or visuospatial information (RH) may result in different search initiation after verbal and visual templates. 104 participants tested across 4 experiments. 

We have created the first publicly available online resource containing not only eye-movement data and experimental scenes but also ratings of scene complexity and the subjective salience, semantic importance and plausibility of placement for target objects (from 10 individuals). This will be a useful resource for researchers studying plausibility of object placement in scenes. Experiment 1 and 2 data and images are available at http://www.activevisionlab.org/.</gtr:description><gtr:exploitationPathways>Our work provides new insights into the role of consistency in eye guidance and visual search. This will be of importance to those working in vision research and psychology and will extend our current scientific knowledge. We are the first to demonstrate an effect of inconsistency that is separable from effects of template guidance and spatial expectation guidance in scenes. Our method in Experiment 2 provides a means of separating these different influences on eye guidance that has not previously been described or used. Thus the method and images will be of benefit to the field. 

Understanding how the eyes are guided to regions in scenes where objects are expected and how inconsistently placed objects capture (or do not capture) attention in scenes has potential wider benefit outside academia. Specifically it may inform understanding of situations in which search for unexpected or misplaced objects is essentially, such as security tasks like CCTV surveillance and baggage screening. In particular if the precision of target knowledge influences the extent to which the eyes are distracted by inconsistency or by spatial expectations about placement, this can be used to refine search protocols in security settings (e.g., providing an image of a search target will help avoid distractions compared to providing a verbal description).</gtr:exploitationPathways><gtr:id>6A2ACB47-D2E0-4E01-AA4A-EB7AEE902B4B</gtr:id><gtr:sectors><gtr:sector>Security and Diplomacy,Transport</gtr:sector></gtr:sectors><gtr:url>http://activevisionlab.org/research/timecourse/</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs><gtr:researchDatabaseAndModelOutput><gtr:description>Dataset of eye movement records for participants viewing images in experiment 1 of the ESRC awarded grant. Raw eye movement records are made available publicly. Data provided are those used in Spotorno et al. (2014) JOV</gtr:description><gtr:id>0B0711C8-3FE9-4F49-B1B4-C90C8BB70ABF</gtr:id><gtr:impact>This is the first publicly available dataset of eye movement behaviour when viewing scenes with inconsistently placed objects. These data will be of use to those working on eye movements, scene viewing and scene-object inconsistency.</gtr:impact><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>Eye movement dataset - experiment 1</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://activevisionlab.org/research/timecourse/</gtr:url><gtr:yearFirstProvided>2014</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput><gtr:researchDatabaseAndModelOutput><gtr:description>Dataset of eye movement records for participants viewing images in experiment 2 of the ESRC awarded grant. Raw eye movement records are made available publicly. Data provided are those used in Spotorno et al. (2015) JOV</gtr:description><gtr:id>1EE90E2A-4CF2-43C5-8BA5-103767223E75</gtr:id><gtr:impact>This is the first publicly available dataset of eye movement behaviour when viewing scenes with inconsistently placed critical objects, placed alongside a number of distracter objects in each of two scene regions. These data will be of use to those working on eye movements, scene viewing and scene-object inconsistency.</gtr:impact><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>Eye movement dataset - Experiment 2</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://activevisionlab.org/research/timecourse/</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput><gtr:researchDatabaseAndModelOutput><gtr:description>Database of images used in Experiment 2. These are photographic scenes with photographic objects inserted. Each scene appears in 2 versions with differing placements of critical objects, along with a number of distracter objects in each of two scene regions. Critical objects are placed plausibly or implausibly in the scene. All scenes and object insertions were tested for a variety of measures by independent raters.</gtr:description><gtr:id>A668C2B7-5238-4DB6-96D7-2251B29CA206</gtr:id><gtr:impact>This is the first image dataset of its kind and will be of use to those working on aspects of scene inspection and in particular on the impact of inconsistency of object placement in scenes.</gtr:impact><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>Image database - Experiment 2</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://activevisionlab.org/research/timecourse/</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput><gtr:researchDatabaseAndModelOutput><gtr:description>Database of images used in experiment 1. These are photographic scenes with photographic objects inserted. Each scene appears in 4 versions with differing placements of objects. Objects are placed plausibly or implausibly in the scene. All scenes and object insertions were tested for a variety of measures by independent raters.</gtr:description><gtr:id>DD6E4971-99B3-461B-9A1E-B6691330C4B7</gtr:id><gtr:impact>This is the first image dataset of its kind and will be of use to those working on aspects of scene inspection and in particular on the impact of inconsistency of object placement in scenes.</gtr:impact><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>Image database - experiment 1</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://activevisionlab.org/research/timecourse/</gtr:url><gtr:yearFirstProvided>2014</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput></gtr:researchDatabaseAndModelOutputs><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/0D0409CA-1A9D-4828-A2F0-C133A9C6C572"><gtr:id>0D0409CA-1A9D-4828-A2F0-C133A9C6C572</gtr:id><gtr:title>Disentangling the effects of spatial inconsistency of targets and distractors when searching in realistic scenes.</gtr:title><gtr:parentPublicationTitle>Journal of vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/91cb3789324fcabc04334d31d66df091"><gtr:id>91cb3789324fcabc04334d31d66df091</gtr:id><gtr:otherNames>Spotorno S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>1534-7362</gtr:issn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/5B427995-B99F-4CF9-A449-565080EE31F2"><gtr:id>5B427995-B99F-4CF9-A449-565080EE31F2</gtr:id><gtr:title>How context information and target information guide the eyes from the first epoch of search in real-world scenes.</gtr:title><gtr:parentPublicationTitle>Journal of vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/91cb3789324fcabc04334d31d66df091"><gtr:id>91cb3789324fcabc04334d31d66df091</gtr:id><gtr:otherNames>Spotorno S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>1534-7362</gtr:issn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/F8DEC988-CCE6-4613-BD05-D8C17B5D1524"><gtr:id>F8DEC988-CCE6-4613-BD05-D8C17B5D1524</gtr:id><gtr:title>Scene context and object information interact during the first epoch of scene inspection</gtr:title><gtr:parentPublicationTitle>Perception</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/13736a921d6bcdf165c1d5ca5392af7a"><gtr:id>13736a921d6bcdf165c1d5ca5392af7a</gtr:id><gtr:otherNames> Sara Spotorno (Co-author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/D7A91B07-D702-4C3C-9B14-00F04234C59D"><gtr:id>D7A91B07-D702-4C3C-9B14-00F04234C59D</gtr:id><gtr:title>The use of scene context and object information during visual search in real-world images</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/03a6688601c94f5afe1fe5fb4229f005"><gtr:id>03a6688601c94f5afe1fe5fb4229f005</gtr:id><gtr:otherNames>Sara Spotorno (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RES">RES-000-22-4098</gtr:identifier><gtr:identifier type="RCUK">ES/H04597X/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>5858EC49-4786-4440-8352-1AB0B6DC5F23</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Psychology</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>5858EC49-4786-4440-8352-1AB0B6DC5F23</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Psychology</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>