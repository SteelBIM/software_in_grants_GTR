<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.rcuk.ac.uk/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.rcuk.ac.uk:80/organisation/E5A82D2C-5AD4-488A-ACFF-566345A5D6DA"><gtr:id>E5A82D2C-5AD4-488A-ACFF-566345A5D6DA</gtr:id><gtr:name>Heriot-Watt University</gtr:name><gtr:department>S of Mathematical and Computer Sciences</gtr:department><gtr:address><gtr:line1>Administration Building</gtr:line1><gtr:line2>Riccarton</gtr:line2><gtr:line3>Ricarton</gtr:line3><gtr:line4>Currie</gtr:line4><gtr:postCode>EH14 4AS</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/E5A82D2C-5AD4-488A-ACFF-566345A5D6DA"><gtr:id>E5A82D2C-5AD4-488A-ACFF-566345A5D6DA</gtr:id><gtr:name>Heriot-Watt University</gtr:name><gtr:address><gtr:line1>Administration Building</gtr:line1><gtr:line2>Riccarton</gtr:line2><gtr:line3>Ricarton</gtr:line3><gtr:line4>Currie</gtr:line4><gtr:postCode>EH14 4AS</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.rcuk.ac.uk:80/person/268676F3-8E18-4E32-95B0-9952D98BEC0C"><gtr:id>268676F3-8E18-4E32-95B0-9952D98BEC0C</gtr:id><gtr:firstName>Oliver</gtr:firstName><gtr:surname>Lemon</gtr:surname><gtr:orcidId>0000-0001-9497-4743</gtr:orcidId><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.rcuk.ac.uk:80/person/A7EE7868-8735-41C3-86F0-305E366E6D52"><gtr:id>A7EE7868-8735-41C3-86F0-305E366E6D52</gtr:id><gtr:firstName>Arash</gtr:firstName><gtr:surname>Eshghi</gtr:surname><gtr:roles><gtr:role><gtr:name>RESEARCHER_COI</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.rcuk.ac.uk:80/projects?ref=EP%2FM01553X%2F1"><gtr:id>14E89B22-BBF6-41D6-B9B8-EE76492B8CAE</gtr:id><gtr:title>Babble: domain-general methods for learning natural spoken dialogue systems</gtr:title><gtr:status>Active</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/M01553X/1</gtr:grantReference><gtr:abstractText>The demand for future conversational speech technologies is estimated to reach a market value of $3 billion by 2020 (Grand View Research, 2014). Our proposed technology will provide vital foundations and impetus for the rapid development of a next-generation of naturally interactive conversational interfaces with deep language understanding, in areas as diverse as healthcare, human-robot interaction, wearables, home automation, education, games, and assistive technologies.

Future conversational speech interfaces should allow users to interact with machines using everyday spontaneous language to achieve everyday needs. A commercial example with quite basic capabilities is Apple's Siri. However, even today's limited speech interfaces are very difficult and time-consuming to develop for new applications: their key components currently need to be tailor-made by experts for specific application domains, relying either on hand-written rules or statistical methods that depend on large amounts of expensive, domain-specific, human-annotated dialogue data. The components thus produced are of little or no use for any new application domain, resulting in expensive and time-consuming development cycles.

One key underlying reason for this status quo is that for spoken dialogue, general, scalable methods for natural language understanding (NLU), dialogue management (DM), and language generation (NLG) are not yet available. Current domain-general methods for language processing are sentence-based and so perform fairly well for processing written text, but they quickly run into difficulties in the case of spoken dialogue, because ordinary conversation is highly fragmentary and incremental: it naturally happens word-by-word, rather than sentence-by-sentence. Real conversation happens bit by bit, using half-starts, suggested add-ons, pauses, interruptions, and corrections -- without respecting the boundaries of sentences. And it is precisely these properties that contribute to the feeling of being engaged in a normal, natural conversation, which current state-of-the-art speech interfaces fail to produce.

We propose to solve these two problems together, by for the first time: 

(1) combining domain-general, incremental, and scalable approaches to NLU, DM, and NLG;

(2) developing machine learning algorithms to automatically create working speech interfaces from data, using (1). 

We propose a new method &amp;quot;BABBLE&amp;quot; in which speech systems can be trained to interact naturally with humans, much like a child who experiments with new combinations of words to discover their usefulness (though doing this offline to avoid annoying real users while doing so!). 
 
BABBLE will be deployed as a developer kit and as mobile speech Apps for public use and engagement, and will also generate large dialogue data sets for scientific and industry use.

This new method will not require expensive data annotation or expert developers, leading to easy creation of new speech interfaces that advance the state-of-the-art in interacting more naturally, and therefore more successfully and engagingly with users. 

New advances have been made in key areas relevant to this proposal: incremental grammars, formal semantic models of dialogue, and sample-efficient machine learning methods. The opportunity to combine and develop these approaches has arisen only recently, and now makes major advances in spoken dialogue technology possible.</gtr:abstractText><gtr:potentialImpactText>There are 3 main arenas of impact for this work: commercial, societal, and academic. 
The major societal impact of this work is in widening access to information technology, and making it more efficient and engaging for users through the use of spoken conversation.
The core technology for natural conversational speech interfaces developed here has the potential to impact on several important groups of users: 

* Internet users, mobile App users, and wearable technology users via improved conversational interfaces for control and search;
* Visually impaired individuals via better speech interfaces; 
* The elderly and disabled, via automated assisted independent living using dialogue-based interaction;
* Users of immersive virtual reality interfaces (using speech for control and search);
* Computer Game players, e.g. via improved dialogue-based interaction with virtual characters; 
* Users of educational technologies, using conversational virtual characters for learning;
* People interacting with robots (spoken Human-Robot Interaction).

In all of the above areas, the important economic impact of the work is also in lowering costs for industry through the automation of speech interface development -- a central objective of this proposal. Companies will benefit through the lowering or removal of costs associated with hiring expert system developers and data annotators. These advances will make speech interfaces a more affordable technology, deployable more rapidly in wider contexts.
Academic and industrial researchers will also benefit via the provision of new, open resources for conversational system development.

To realise the full impact of the research, we will therefore focus on the following pathways:

* BABBLE mobile Apps: public engagement with the developed systems released as speech Apps on smartphones, mobile devices, and wearables (like Siri and Google Now);

* Open data releases: the large anonymised spoken dialogue data-sets generated by the project will be of interest to academic researchers and industry;

* BABBLE Toolkit: software and tools developed will be released for academic and industrial use;

* Interdisciplinary publications and research visits: combining research from two previously disconnected communities: wide-coverage grammars of Natural Language and statistical approaches to automated dialogue systems;

* Demonstrations: showcasing the BABBLE technology at events such as the Scottish Informatics and Computer Science Alliance (SICSA) demofests and EC ICT events, which bring together academia with industrial developers;

* Robotarium demos: showcasing BABBLE systems for human-robot interaction applications at the Robotarium (EPSRC Infrastructure Grant, 2013);

* Impact workshop: organised at the end of the project to amplify these avenues to impact, inviting researchers from both academia and industry;

* Spin-out company: the BABBLE technology will be integrated in to a spin-out speech technology company which is currently being developed at Heriot-Watt by the PI, under an &amp;quot;Impact Acceleration&amp;quot; grant.

To reach the general public as well as interested academic and industry researchers, we will also use a range of social media such as Twitter and YouTube, as we have done in the past with success for a number of our projects, as well as using traditional websites. We will also further develop our contacts with press and media, which have lead to a number of newspaper, radio, and television outputs describing the PI's research. Our advisory board member Dr. Matthew Purver, founder and Chief Data Scientist of Chatterbox, will advise us on technology transfer and commercialisation of dialogue and language technology (see letters of support).</gtr:potentialImpactText><gtr:fund><gtr:end>2017-09-30</gtr:end><gtr:funder url="http://gtr.rcuk.ac.uk:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2015-04-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>278283</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>Poster at Alan Turing Institute Deep Learning workshop</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>DF6C6231-1B3D-45C8-A4FB-FC56110BDF6F</gtr:id><gtr:impact>Edinburgh University hosted the &amp;quot;Alan Turing Institute Deep Learning workshop&amp;quot; in 2015. We presented out work to practitioners and postgraduate students.</gtr:impact><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Professional Practitioners</gtr:primaryAudience><gtr:url>http://workshops.inf.ed.ac.uk/deep/deepATI/</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>TED-x technology demonstration session</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>FA4FD069-7E74-406C-8903-EA9FCF8FD707</gtr:id><gtr:impact>Heriot-Watt hosted the TEDx event described here: http://www.tedxhwu.com/

I did a demo session for participants, who were a mix of business, media, professionals, and students -- showcasing some of our dialogue and robot technology</gtr:impact><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Industry/Business</gtr:primaryAudience><gtr:url>http://www.tedxhwu.com/</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>100000</gtr:amountPounds><gtr:country>United States of America</gtr:country><gtr:currCode>USD</gtr:currCode><gtr:currCountryCode>Ecuador</gtr:currCountryCode><gtr:currLang>es_EC</gtr:currLang><gtr:description>Amazon Alexa Challenge</gtr:description><gtr:fundingOrg>Amazon.com</gtr:fundingOrg><gtr:id>5FDFBA9C-F730-44A9-A494-602021B1DDAD</gtr:id><gtr:sector>Private</gtr:sector><gtr:start>2016-11-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>We are currently involved in several knowledge transfer collaborations with industry, including Speech-Graphics.com and Amazon.com. These collaborations use our expertise and experience in dialogue system development.

We have also created a new MSc programme in AI with Speech and Multimodal Interaction, where methods and techniques developed as part of this project are taught to students in new courses, such as Conversational Agents.</gtr:description><gtr:firstYearOfImpact>2016</gtr:firstYearOfImpact><gtr:id>0CED469C-3C29-4DDD-861A-A68D179CAF45</gtr:id><gtr:impactTypes><gtr:impactType>Cultural,Societal,Economic</gtr:impactType></gtr:impactTypes><gtr:sector>Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>We have developed a method which allows the automatic construction of natural spoken dialogue systems (or conversational agents) from very small amounts of un-annotated data.
This is an advance because previous methods required large amounts of data to be collected and annotated, which is both expensive and time-consuming. 
Moreover, our method supports more natural human conversation than many previous systems, because it processes language word-by-word, rather than waiting for the end of an utterance. 
We have applied this method in a number of dialogue systems, for example one where a robot learns the meanings of words by interacting with a human teacher.</gtr:description><gtr:exploitationPathways>This technology can be used by developers of future speech and dialogue interfaces -- to more rapidly and cheaply develop natural spoken dialogue interfaces to devices and services.
We are also releasing new dialogue data.</gtr:exploitationPathways><gtr:id>99883744-CC3D-49ED-86B9-D72B8123D969</gtr:id><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:sectors><gtr:url>https://sites.google.com/site/hwinteractionlab/babble</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs><gtr:researchDatabaseAndModelOutput><gtr:description>Data for results reported in the paper: Strategic Dialogue Management via Deep Reinforcement Learning, NIPS 2015.</gtr:description><gtr:id>3A427CCB-306E-477F-AD1F-106DED43CC91</gtr:id><gtr:impact>The first application of Deep Reinforcement Learning methods to dialogue management problems.</gtr:impact><gtr:providedToOthers>true</gtr:providedToOthers><gtr:title>NIPS 2015 deep RL data</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:researchDatabaseAndModelOutput></gtr:researchDatabaseAndModelOutputs><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs><gtr:softwareAndTechnicalProductOutput><gtr:description>This is an implementation continuously under development of:

(a) Dynamic Syntax and Type Theory with Records (DS-TTR) (Cann et. al. 2015; Eshghi et. al. 2012), a word-by-word incremental semantic grammar, especially suited to dialogue processing, with semantic and contextual representations produced in the TTR formalism. The implementation contains the following: (1) depth-first and breadth-first parsers and generators, based on hand-crafted domain-specific lexicons that cover a broad range of structures, including relatives clauses and tense; (2) a prototype incremental dialogue system, DyLan, based on Jindigo (Skantze &amp;amp; Hjalmarsson, 2010), but using the Dynamic Syntax (Kempson et. al. 2001, Cann et. al. 2005) parser/generator; (3) a grammar induction module which learns DS incremental grammars from data (Eshghi et. al. 2013). This has been updated with improvements made under the ESPRC BABBLE project, specifically to incorporate an interactive parser based on Eshghi et. al.'s (2015) model of feedback in dialogue.

(b) A Dialogue System for interactive learning of visually grounded language from a human partner that uses DS-TTR for dialogue processing and grounding (Yu et. al. 2016).

(c) An integration of DS-TTR with Reinforcement Learning, allowing incremental dialogue systems to be automatically induced from raw, unannotated dialogue examples (Eshghi &amp;amp; Lemon, 2014; Kalatzis et. al. 2016).

(a) has been updated and improved continuously throughout the BABBLE project. (b) and (c) have been produced exclusively within the BABBLE project.</gtr:description><gtr:id>2783F8F2-5B31-43FE-8E02-DCA0B418D8A4</gtr:id><gtr:impact>Demonstration at SemDial 2015 conference.
Demo at INLG 2016 conference.
Demo at Semdial 2016 conference.</gtr:impact><gtr:openSourceLicense>true</gtr:openSourceLicense><gtr:title>BABBLE software</gtr:title><gtr:type>Software</gtr:type><gtr:url>https://bitbucket.org/dylandialoguesystem</gtr:url><gtr:yearFirstProvided>2017</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>SimpleDS is a simple dialogue system trained with deep reinforcement learning. In contrast to other dialogue systems, this system selects dialogue actions directly from raw (noisy) text of the last system and user responses. The motivation is to train dialogue agents with as little human intervention as possible.</gtr:description><gtr:id>B9689AB2-8013-407C-AB8E-A817C15594DE</gtr:id><gtr:impact>This software is now being used in several projects in our lab. It lead to our NIPS 2015 paper and it was demonstrated at the IWSDS 2016 conference.</gtr:impact><gtr:openSourceLicense>true</gtr:openSourceLicense><gtr:title>SimpleDS -- Deep Reinforcement Learning for dialogue management</gtr:title><gtr:type>Software</gtr:type><gtr:url>https://github.com/cuayahuitl/SimpleDS</gtr:url><gtr:yearFirstProvided>2016</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput></gtr:softwareAndTechnicalProductOutputs><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/81261D24-6A20-4101-8FD4-8F8891F23607"><gtr:id>81261D24-6A20-4101-8FD4-8F8891F23607</gtr:id><gtr:title>Feedback in Conversation as Incremental Semantic Update</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/fae0dffab78916846e6f33e8de090699"><gtr:id>fae0dffab78916846e6f33e8de090699</gtr:id><gtr:otherNames>Arash Eshghi</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/C9622FCE-493C-4725-9CD8-B14999EC0399"><gtr:id>C9622FCE-493C-4725-9CD8-B14999EC0399</gtr:id><gtr:title>Handbook of Contemporary Semantic Theory</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/5121d43ae58b8d4fc074494fc89ed5cd"><gtr:id>5121d43ae58b8d4fc074494fc89ed5cd</gtr:id><gtr:otherNames>Kempson, R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:isbn>978-0-470-67073-6</gtr:isbn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/A9582668-CFD8-4EF0-88C8-32F54AC89065"><gtr:id>A9582668-CFD8-4EF0-88C8-32F54AC89065</gtr:id><gtr:title>The BURCHAK corpus: a Challenge Data Set for Interactive Learning of Visually Grounded Word Meanings</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/33f8b5eefa1739d60bdb71d6a13794da"><gtr:id>33f8b5eefa1739d60bdb71d6a13794da</gtr:id><gtr:otherNames>Yu Y</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/70CE9F8C-7B8A-427F-969C-A9CBB703258C"><gtr:id>70CE9F8C-7B8A-427F-969C-A9CBB703258C</gtr:id><gtr:title>Comparing dialogue strategies for learning grounded language from human tutors</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/12be82ea3e28149d4422c11d88db7b7a"><gtr:id>12be82ea3e28149d4422c11d88db7b7a</gtr:id><gtr:otherNames>Yanchao Yu</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/EF2A4A10-66C9-4DBB-A933-7A7B59D960CC"><gtr:id>EF2A4A10-66C9-4DBB-A933-7A7B59D960CC</gtr:id><gtr:title>Strategic Dialogue Management via Deep Reinforcement Learning</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/f2eb53d3807ae27c945e6e3ec5926658"><gtr:id>f2eb53d3807ae27c945e6e3ec5926658</gtr:id><gtr:otherNames>H Cuayahuitl</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/9663F1C9-CD7A-42CB-87C1-45F58F92DAC1"><gtr:id>9663F1C9-CD7A-42CB-87C1-45F58F92DAC1</gtr:id><gtr:title>Training an adaptive dialogue policy for interactive learning of visually grounded word meanings</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/12be82ea3e28149d4422c11d88db7b7a"><gtr:id>12be82ea3e28149d4422c11d88db7b7a</gtr:id><gtr:otherNames>Yanchao Yu</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/5B4D19A9-AAE6-40D0-9C76-7A9E113CE5AB"><gtr:id>5B4D19A9-AAE6-40D0-9C76-7A9E113CE5AB</gtr:id><gtr:title>Deep Reinforcement Learning for constructing meaning by `babbling'</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/59db37f599a81080f887a17e67f9a019"><gtr:id>59db37f599a81080f887a17e67f9a019</gtr:id><gtr:otherNames>Oliver Lemon</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/230E5D10-4BB5-402A-B34F-00714CC0BF1D"><gtr:id>230E5D10-4BB5-402A-B34F-00714CC0BF1D</gtr:id><gtr:title>Comparing attribute classifiers for interactive language grounding</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/12be82ea3e28149d4422c11d88db7b7a"><gtr:id>12be82ea3e28149d4422c11d88db7b7a</gtr:id><gtr:otherNames>Yanchao Yu</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/F8A4F209-19BD-4ACA-9617-2F28CC84B300"><gtr:id>F8A4F209-19BD-4ACA-9617-2F28CC84B300</gtr:id><gtr:title>Bootstrapping incremental dialogue systems: using linguistic knowledge to learn from minimal data</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/c512856acfc984a39b20b86afe69d93c"><gtr:id>c512856acfc984a39b20b86afe69d93c</gtr:id><gtr:otherNames>Dimitris Kalatzis</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/B9679934-5FE4-4949-94E1-EF2D96D938BE"><gtr:id>B9679934-5FE4-4949-94E1-EF2D96D938BE</gtr:id><gtr:title>Learning how to learn: grounding word meanings through conversation with humans</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/59db37f599a81080f887a17e67f9a019"><gtr:id>59db37f599a81080f887a17e67f9a019</gtr:id><gtr:otherNames>Oliver Lemon</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/E85C61C3-6AC4-406D-93FE-C3D68604C49E"><gtr:id>E85C61C3-6AC4-406D-93FE-C3D68604C49E</gtr:id><gtr:title>An Incremental Dialogue System for Learning Visually Grounded Language (demonstration system)</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/12be82ea3e28149d4422c11d88db7b7a"><gtr:id>12be82ea3e28149d4422c11d88db7b7a</gtr:id><gtr:otherNames>Yanchao Yu</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/3BD12924-BF19-445F-84C3-2D58E61D3095"><gtr:id>3BD12924-BF19-445F-84C3-2D58E61D3095</gtr:id><gtr:title>The Handbook of Contemporary Semantic Theory</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/02bfd4dd6d07d0bdfb9b188e418774b1"><gtr:id>02bfd4dd6d07d0bdfb9b188e418774b1</gtr:id><gtr:otherNames>Lappin Shalom</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:isbn>9780470670736</gtr:isbn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/BFF416CD-9387-4D05-BC09-61631C98ACCE"><gtr:id>BFF416CD-9387-4D05-BC09-61631C98ACCE</gtr:id><gtr:title>DS-TTR: An incremental, semantic, contextual parser for dialogue</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/fae0dffab78916846e6f33e8de090699"><gtr:id>fae0dffab78916846e6f33e8de090699</gtr:id><gtr:otherNames>Arash Eshghi</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/8EEF568D-F3DD-464B-87FE-F75DFE7410EE"><gtr:id>8EEF568D-F3DD-464B-87FE-F75DFE7410EE</gtr:id><gtr:title>Oxford Handbook of Ellipsis.</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/5121d43ae58b8d4fc074494fc89ed5cd"><gtr:id>5121d43ae58b8d4fc074494fc89ed5cd</gtr:id><gtr:otherNames>Kempson, R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/0E9425E4-E3D0-4533-AA03-973F96D25968"><gtr:id>0E9425E4-E3D0-4533-AA03-973F96D25968</gtr:id><gtr:title>Interactive Learning through Dialogue for Multimodal Language Grounding</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/12be82ea3e28149d4422c11d88db7b7a"><gtr:id>12be82ea3e28149d4422c11d88db7b7a</gtr:id><gtr:otherNames>Yanchao Yu</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/7EE43825-FDEF-4A7B-B505-C3DCE6D41307"><gtr:id>7EE43825-FDEF-4A7B-B505-C3DCE6D41307</gtr:id><gtr:title>Incremental Generation of Visually Grounded Language in Situated Dialogue (demonstration system)</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/12be82ea3e28149d4422c11d88db7b7a"><gtr:id>12be82ea3e28149d4422c11d88db7b7a</gtr:id><gtr:otherNames>Yanchao Yu</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/M01553X/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>41593421-CFAC-411D-94A7-E144022B0E6D</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Artificial Intelligence</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>