<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.rcuk.ac.uk/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.rcuk.ac.uk:80/organisation/88C5F7F9-8DCC-41C9-BC4F-F37DA01075C7"><gtr:id>88C5F7F9-8DCC-41C9-BC4F-F37DA01075C7</gtr:id><gtr:name>University of East Anglia</gtr:name><gtr:department>Computing Sciences</gtr:department><gtr:address><gtr:line1>Earlham Road</gtr:line1><gtr:line4>Norwich</gtr:line4><gtr:line5>Norfolk</gtr:line5><gtr:postCode>NR4 7TJ</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/88C5F7F9-8DCC-41C9-BC4F-F37DA01075C7"><gtr:id>88C5F7F9-8DCC-41C9-BC4F-F37DA01075C7</gtr:id><gtr:name>University of East Anglia</gtr:name><gtr:address><gtr:line1>Earlham Road</gtr:line1><gtr:line4>Norwich</gtr:line4><gtr:line5>Norfolk</gtr:line5><gtr:postCode>NR4 7TJ</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/BD2B46FA-C3ED-4F6B-ABAC-8D20F599F757"><gtr:id>BD2B46FA-C3ED-4F6B-ABAC-8D20F599F757</gtr:id><gtr:name>Spectral Edge Ltd</gtr:name><gtr:address><gtr:line1>The Registry</gtr:line1><gtr:line2>University of East Anglia</gtr:line2><gtr:postCode>NR4 7TJ</gtr:postCode><gtr:region>Unknown</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/C3DA1BEA-518B-4CF9-81B3-BD8D12D82A93"><gtr:id>C3DA1BEA-518B-4CF9-81B3-BD8D12D82A93</gtr:id><gtr:name>Apple</gtr:name><gtr:address><gtr:line1>1 Infinite Loop</gtr:line1><gtr:region>Outside UK</gtr:region><gtr:country>United States</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.rcuk.ac.uk:80/person/2A5C593E-43C3-4E03-9530-990C61A80560"><gtr:id>2A5C593E-43C3-4E03-9530-990C61A80560</gtr:id><gtr:firstName>Graham</gtr:firstName><gtr:surname>Finlayson</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.rcuk.ac.uk:80/person/4C3ACFA1-A698-4C8B-8565-4C4CEAB9E32B"><gtr:id>4C3ACFA1-A698-4C8B-8565-4C4CEAB9E32B</gtr:id><gtr:firstName>Roberto</gtr:firstName><gtr:surname>Cippola</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.rcuk.ac.uk:80/person/F9DCCB8A-A182-44F6-BD35-FE1EFBE13C10"><gtr:id>F9DCCB8A-A182-44F6-BD35-FE1EFBE13C10</gtr:id><gtr:firstName>Bastiaan</gtr:firstName><gtr:otherNames>Johannes</gtr:otherNames><gtr:surname>Boom</gtr:surname><gtr:roles><gtr:role><gtr:name>RESEARCHER_COI</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.rcuk.ac.uk:80/projects?ref=EP%2FM001768%2F1"><gtr:id>06D6C832-8995-43BB-BC8B-3322E7F2E3A5</gtr:id><gtr:title>Colour space homography</gtr:title><gtr:status>Active</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/M001768/1</gtr:grantReference><gtr:abstractText>For commercial 'imaging' products like the digital camera in your phone, engineers seek simple and elegant solutions to hard problems. You will have noticed that newer cameras take better pictures than older ones. This is in part due to ever advancing hardware: there are faster processors and more pixels. Yet, there has been some important problem solving insights that make it possible to solve problems that were hitherto intractable. A good example of this is the problem of colour casts (one of the foci of this proposal). When you take a picture with a camera the colours that are recorded are dependent on the colour of objects in the scene and (perhaps surprisingly) on the colour of the light. Physically, your white T-shirt is yellowish and bluish when viewed in direct sunlight or when you are in the shadows (because sunlight and shadows are respectively yellowish and bluish). We do not see the colour casts as our vision system factors out the colour of the light and, likewise, digital cameras process images to (much of the time) remove colour casts. But, if you compare the outputs of cameras today with those 15 years ago, the modern era cameras are much better at removing colour casts. Why? because of clever insights that helped engineers build systems which correctly infer the colour of the prevailing light.

This research project begins with a surprising new observation which we believe will help us improve still further imaging products such as colour cameras. We make, for the first time, a deep link between the colours recorded in image - typically three R,G and B numbers that measure the redness, greenness and blueness of a pixel - and the 3D locations of points in the world and how these points correspond to pixel locations in an image. Thinking about geometry, we are all aware that train tracks appear to converge in the distance even although we know they stay the same distance apart (intrinsically, we understand how the 3D world maps to 2D pictures). In this research we begin by showing that the relationship between the colours - same colourful object viewed under different viewing conditions - is exactly the same as the relationship that links different viewpoints - say of the front of a building - in geometry. Mathematically, this relationship is called an homography.

Importantly, now that we have linked the colours you see in the camera to how the 3D world maps to images we can use this observation - we can exploit lessons learnt in the geometric domain - to help us solve some colour imaging problems. First, by exploiting colour space homography, camera manufacturers (and enthusiasts) will be able to more accurately calibrate their cameras and this will further optimize image colour fidelity. Homographies also play an important role the colour cast removal problem and in this proposal we will develop a homography-based algorithm that will advance the state of the art. Outside of photography, we are interested in problems such as colour based medical diagnosis e.g. we will be aiming to improve the automatic identification of skin lesions. We propose that homographies are also the key to solving high level vision tasks including identifying, manipulating and removing shadows from images.

More novelly, we have found that answering the question &amp;quot;how do we find a coloured filter which, when place in front of a camera makes the camera measure light in a way that is similar to the human visual system' also involves solving for a homography. Homographies have a link to problems such as image fusion - fusing 100s of images into a single colour summary - which process images in the so-called derivative domain. Here images are transformed into an edge representation and then these edges are manipulated before an output is reintegrated. Homographies hold the promise of making the reintegration process faster and less prone to introducing artifacts (a well known problem of existing techniques).</gtr:abstractText><gtr:potentialImpactText>Conventional colour correction proceeds by taking an image of a reference target and then mapping the image colours to reference counterparts. We then apply the same correction to pictures of unknown objects. Unfortunately, this calibration method assumes light intensity is the same across the chart and this is rarely the case. Assuming uniformity of light intensity (when it is not present) can result in twice the correction error. Our research proposal starts with the discovery that finding the correct colour transform, in an intensity invariant way, involves solving for a homography. Moreover, relating colours across viewpoints with a homography is in direct analogy to geometric vision where a homography is the correct mathematical tool for relating planar point sets across images. Further, this proposal casts the problems of filter design, illuminant estimation, image retrieval and derivative domain processing in a homographic framework. 

This project benefits from the close collaboration from two industry partners. Apple Inc have a particular interest in photographic applications and are interested in the potential of our research to help them attain improved colour fidelity and better illuminant estimation. They are also keenly interested in understanding which parts of images are lit by different lights. Spectral Edge Ltd, a UEA spin out in image fusion, has developed technology for fusing different images in the derivative domain and so are interested in how homographies help in derivative domain processing. One aspect of Spectral Edge's work has already been approximated as an optical process (light passing through a specially designed optical filter). So, there is a synergy with our filter design homography research. 

Of course, through our extensive dissemination plans we also plan to publish all our algorithms and data. So, there is an opportunity for wider industry to make use of our research. Detailed in our scientific case is an application to improving the largely colour based automated diagnoses of skin lesions. More widely, colour space homography has the potential to impact of big image data (search) and photo processing amongst other applications.

The research team has a track record of working with industry on colour standards including ISO 17321 (camera characterization). The colour homography work may have application in photography ISO/TC42 and machine vision European Machine Vision Association EMVA 288. The former technical committee will be approached via the Society of Imaging Science and Technology (through Finlayson who is a fellow). Indirectly, through the British Machine Vision Association (Fisher is a fellow) we have links to the EMVA and plan to communicate our work to that organization.

The project provides unrivaled training opportunities for the Research Co-investigator Dr Bastiaan Boom and also the PhD students to be trained along side this grant. Dr Boom will gain expertise in the management side of research and also in supervising research students (both activity supported by targeted courses from UEA's Centre for Staff Education and Development). Dr Boom and the students will have the chance to work with industrial partners at the partner sites. The whole team will have the opportunity to see their research transferred and prototyped in an industrial context.

Regarding outreach to a wider community, this research project begins coincidently at the same time as the EPSRC Network on Biological and Machine Visiion begins its work. Not only is the topic of this network intimately related to the proposed research, one of the founding aims of the network is public engagement. Finlayson is the leader of the 'Appearance: Shadow removal and lightness constancy' theme (which relates strongly to our research proposal). Finlayson will work with the network in delivering workshops and will liaise with interested societies including RPS, IET and IS&amp;amp;T to ensure the widest participation</gtr:potentialImpactText><gtr:fund><gtr:end>2019-02-27</gtr:end><gtr:funder url="http://gtr.rcuk.ac.uk:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2015-02-28</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>434815</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/48A5601E-6C1A-4985-B47D-E1434AF46FF6"><gtr:id>48A5601E-6C1A-4985-B47D-E1434AF46FF6</gtr:id><gtr:title>Illumination and Reflectance Spectra Separation of Hyperspectral Image Data under Multiple Illumination Conditions</gtr:title><gtr:parentPublicationTitle>Electronic Imaging</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/7771ec39b9bd63d704f50fabca5f24ae"><gtr:id>7771ec39b9bd63d704f50fabca5f24ae</gtr:id><gtr:otherNames>Chen X</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/68C520F6-1D54-4041-B222-25E936F03E20"><gtr:id>68C520F6-1D54-4041-B222-25E936F03E20</gtr:id><gtr:title>Interactive removal and ground truth for difficult shadow scenes.</gtr:title><gtr:parentPublicationTitle>Journal of the Optical Society of America. A, Optics, image science, and vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/7998d4ee273850d37439417c15334631"><gtr:id>7998d4ee273850d37439417c15334631</gtr:id><gtr:otherNames>Gong H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>1084-7529</gtr:issn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/6D2EC40D-3D54-4DF7-AB59-03FD05A6763A"><gtr:id>6D2EC40D-3D54-4DF7-AB59-03FD05A6763A</gtr:id><gtr:title>The Reproduction Angular Error for Evaluating the Performance of Illuminant Estimation Algorithms.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on pattern analysis and machine intelligence</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/6a93a01532ef14b3aa1a1b901c40810e"><gtr:id>6a93a01532ef14b3aa1a1b901c40810e</gtr:id><gtr:otherNames>Finlayson GD</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>0098-5589</gtr:issn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/3EBCBC29-19CD-4A9D-AF45-9EBA98C87550"><gtr:id>3EBCBC29-19CD-4A9D-AF45-9EBA98C87550</gtr:id><gtr:title>Color Homography Color Correction</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/6a93a01532ef14b3aa1a1b901c40810e"><gtr:id>6a93a01532ef14b3aa1a1b901c40810e</gtr:id><gtr:otherNames>Finlayson GD</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/C7CAEF73-3D8E-4975-B3CF-EA9E16BE9C51"><gtr:id>C7CAEF73-3D8E-4975-B3CF-EA9E16BE9C51</gtr:id><gtr:title>Color Homography Color Correction</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/6a93a01532ef14b3aa1a1b901c40810e"><gtr:id>6a93a01532ef14b3aa1a1b901c40810e</gtr:id><gtr:otherNames>Finlayson GD</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/50178DDA-18C3-45AE-A459-8F29A820178C"><gtr:id>50178DDA-18C3-45AE-A459-8F29A820178C</gtr:id><gtr:title>Proceedings of the British Machine Vision Conference 2016</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/7998d4ee273850d37439417c15334631"><gtr:id>7998d4ee273850d37439417c15334631</gtr:id><gtr:otherNames>Gong H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/20CC37BD-FB79-457E-BF90-FC17C3C0BE70"><gtr:id>20CC37BD-FB79-457E-BF90-FC17C3C0BE70</gtr:id><gtr:title>Interactive Illuminant Invariance</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/7998d4ee273850d37439417c15334631"><gtr:id>7998d4ee273850d37439417c15334631</gtr:id><gtr:otherNames>Gong H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/M001768/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>