<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.rcuk.ac.uk/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.rcuk.ac.uk:80/organisation/A8967420-49D3-4509-9912-25FB3EC75B74"><gtr:id>A8967420-49D3-4509-9912-25FB3EC75B74</gtr:id><gtr:name>University of Sussex</gtr:name><gtr:department>Sch of Engineering and Informatics</gtr:department><gtr:address><gtr:line1>The Administration</gtr:line1><gtr:line2>Sussex House</gtr:line2><gtr:line3>Falmer</gtr:line3><gtr:line4>Brighton</gtr:line4><gtr:line5>East Sussex</gtr:line5><gtr:postCode>BN1 9RH</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/A8967420-49D3-4509-9912-25FB3EC75B74"><gtr:id>A8967420-49D3-4509-9912-25FB3EC75B74</gtr:id><gtr:name>University of Sussex</gtr:name><gtr:address><gtr:line1>The Administration</gtr:line1><gtr:line2>Sussex House</gtr:line2><gtr:line3>Falmer</gtr:line3><gtr:line4>Brighton</gtr:line4><gtr:line5>East Sussex</gtr:line5><gtr:postCode>BN1 9RH</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.rcuk.ac.uk:80/person/DAEF99A9-72B9-4731-A043-5992F34C2E7A"><gtr:id>DAEF99A9-72B9-4731-A043-5992F34C2E7A</gtr:id><gtr:firstName>Sriram</gtr:firstName><gtr:surname>Subramanian</gtr:surname><gtr:orcidId>0000-0002-5266-8366</gtr:orcidId><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.rcuk.ac.uk:80/person/3A6EFFA7-3655-407C-9FDF-F703DE9B65F8"><gtr:id>3A6EFFA7-3655-407C-9FDF-F703DE9B65F8</gtr:id><gtr:firstName>Bruce</gtr:firstName><gtr:surname>Drinkwater</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.rcuk.ac.uk:80/projects?ref=EP%2FN014197%2F1"><gtr:id>3BD2D8CC-6351-42E7-AA24-0BA869A3DED3</gtr:id><gtr:title>User Interaction with self-supporting free-form physical objects</gtr:title><gtr:status>Active</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/N014197/1</gtr:grantReference><gtr:abstractText>The primary goal of this project is enhance human-computer interaction by dynamically creating and manipulating physical shapes by levitating and moving a large collections of lightweight 3D objects using principles of acoustic levitation. The proposed idea is illustrated in Figure 1 where one ultrasound transducers is placed as a floor mat and a large collection of polystyrene beads create the shape of a dog that can wag its tail. 

This will enable us to represent complex datasets in a physical form and allow users to dynamically manipulate it. We are now moving away from traditional human-computer interaction techniques like buttons, keyboards and mice to touch (e.g., Smartphones and multi-touch gestures) and touchless interactions (as with the Kinect and Leap Motion controllers). The limiting factor in these new forms of interactions is, ironically, the lack of any physicality and lack of any co-located feedback. One has no controller or interface element to physically touch and interact with and the visual feedback that may be available is disconnected from the location of the gesture.

In our vision, the computer will control the existence, form, and appearance of complex levitating objects composed of &amp;quot;levitating atoms&amp;quot;. Users can reach into the levitating matter, feel it, manipulate it, and hear how they deform it with all feedback originating from the levitating object's position in mid-air, as it would with objects in real life. This will change how people use technology as they can interact with technology in the same way they would with real objects in their natural environment.

We see many possible benefits of physicalisations for the individual and society: they make data more accessible by leveraging our perceptual exploration skills via active perception, depth perception, non-visual senses, and intermodal perception; give novel exploration possibilities to the visually impaired; support learning via cognitive benefits of direct manipulation of physical artefacts; bring data to the real world for communication and exhibition and; and finally act as tools for engaging audiences with information.</gtr:abstractText><gtr:potentialImpactText>One of the aims of this project is to explore the roles levitation and physicalisations can play in public engagement with science, including public engagement with areas of policy that have technical content such as energy and climate change. This project has practical roles to play in supporting and fostering dialogue as well as in provoking interest in technical topics. The key will be to integrate the principles learned from WP1 - WP3 to engage the public with good storytelling.

Through the collaboration of Carbon Visuals, an Installation Artist will be responsible for creating novel interactive art installations that exploit the various research tools and systems developed by the project. Our system will empower new approaches to explore, manipulate and understand complex 3D representations of different types of data and interactive objects. By providing direct manipulation of physical objects in a 3D space combined with rich tactile feedback, users will be able to feel rather than simply look at their interactions. Consequently, researchers and interaction designers will be able to take advantage of the increased engagement to offer new powerful active exploration techniques with physical representations that have the potential to stimulate reasoning and analytical skills in new ways. This new interaction paradigm will also diminish the transition cost between novice and expert, allowing users to reach high performance for a small learning investment. Since direct 3D manipulation relies on our natural human skills for interacting with physical objects, the new interaction techniques will be easier to learn and master than their desktop computer counterparts using mice or keyboards. This will extend users' abilities to resolve complex 3D spatial-based problems quickly. 

Our vision enables one to represent complex datasets in a physical form allowing users to analyse and perceive complex time-varying 3D structures through physicalisations. This idea to represent data in physical form is not new. Physicalisations as stone or pebble tokens have already been used thousands of years ago. More than 7000 years ago, the Sumerians, for example, used clay tokens to represent quantitative data. Even today, physicalisations in the form of data sculptures are created and used by designers and artists as alternate methods for conveying datasets. They have projected communicative impact, quickly and easily helping users to understand, for example, how crime rates vary across a city or how an urban development will change the environment. Similarly, scientists use and have used physicalisations to help analyse and perceive complex time-varying 3D structures. For example, James Maxwell famously created physicalisations of thermodynamic surfaces to help perceive isopiestics and isothermals. This helped Maxwell visualize Gibbs' thermodynamic surface, which expressed the relationship between the volume, entropy, and energy of a substance at different temperatures and pressures.

However physicalisations in the past have lacked dynamicity, automation, and computation all of which can be enabled by this project. We see many possible benefits of physicalisations for the individual and society: they make data more accessible by leveraging our perceptual exploration skills via active perception, depth perception, non-visual senses, and intermodal perception; give novel exploration possibilities to the visually impaired; support learning via cognitive benefits of direct manipulation of physical artefacts; bring data to the real world for communication and exhibition and; and finally act as tools for engaging audiences with information.</gtr:potentialImpactText><gtr:fund><gtr:end>2019-08-31</gtr:end><gtr:funder url="http://gtr.rcuk.ac.uk:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2016-03-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>1023446</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs><gtr:artisticAndCreativeProductOutput><gtr:description>Tractor beams are mysterious rays that can grab and attract objects. The concept has been shown in science-fiction movies such as Star Wars or Star Trek and scientists have developed the theory using lasers. Recently, sound was used to create a working tractor beam that can move heavier objects made of different materials and that operates both in air or water without damaging the trapped objects. 

Our team, led by Dr. Asier Marzo, created a simplified tractor beam using readily available parts with a total cost of less than &amp;pound;70 and used it to engage with children and the general public to capture their imagination on the possibilities of acoustic waves.</gtr:description><gtr:id>E7F12757-CD99-4349-8E17-CD36D4E3AC85</gtr:id><gtr:impact>This was demonstrated as part of the Star Trek 50th anniversary event. Prof. Drinkwater was invited to discuss the science of Star Trek at a live recording of the Cosmic Shed podcast. Before the discussion there was a screening of The Wrath of Khan organised by Sunset Cinema. This was all done to celebrate the 50th anniversary of Star Trek. The event was held in the planetarium of the @Bristol science museum in September 2016. At the end of the recording, Prof. Drinkwater gave the first ever live demo of a sonic tractor beam.</gtr:impact><gtr:title>A portable acoustic tractor beam</gtr:title><gtr:type>Artefact (including digital)</gtr:type><gtr:url>https://www.youtube.com/watch?v=0nh2IftOcI0</gtr:url><gtr:yearFirstProvided>2017</gtr:yearFirstProvided></gtr:artisticAndCreativeProductOutput></gtr:artisticAndCreativeProductOutputs><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>There is growing interest from the University of Sussex and Bristol to explore joint exploitation of the outcomes of our study. More details of this will be reported in the next reporting period.</gtr:description><gtr:firstYearOfImpact>2016</gtr:firstYearOfImpact><gtr:id>57FDF4BF-4AC5-4490-B456-2701CB837223</gtr:id><gtr:impactTypes><gtr:impactType>Economic</gtr:impactType></gtr:impactTypes><gtr:sector>Creative Economy,Digital/Communication/Information Technologies (including Software),Electronics</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>To date the project has mostly been exploring new ways in which we can control an acoustic field to enable different sorts of levitation. 
a) The team from Bristol (led by Prof. Drinkwater) published a recent article that was published in Applied Physical Letters and widely reported in the media. The article explored how any DIY community could make its own sonic tractor beam using a 3D printer. 
b) The collaborative effort between Bristol and Sussex has recently led to an article that uses 3D printed metamaterials to recreate complex acoustic fields. The work was published on Feb 27th and it is a bit early to describe its impact on the non-scientific community</gtr:description><gtr:exploitationPathways>Controlling acoustic fields is crucial in diverse applications such as loudspeaker design, ultrasound imaging and therapy or acoustic particle manipulation. The current approaches use fixed lenses or expensive phased arrays. Our work on controlling complex sound fields using metamaterials paves the way for a range of applications that do not rely on expensive phased arrays. 

The work will be beneficial to a range of communities from consumer electronics industry to nondestructive testing and medical imaging communities.</gtr:exploitationPathways><gtr:id>884C5DFE-8E05-45F3-8264-CAD36F53B7F0</gtr:id><gtr:sectors><gtr:sector>Creative Economy,Digital/Communication/Information Technologies (including Software),Electronics,Manufacturing, including Industrial Biotechology</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/2C15162D-A004-458D-ACE3-250580603A84"><gtr:id>2C15162D-A004-458D-ACE3-250580603A84</gtr:id><gtr:title>Quantised acoustic meta-surfaces using metamaterial bricks</gtr:title><gtr:parentPublicationTitle>The Journal of the Acoustical Society of America</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/dfd6ade44eabf6326b0a4856385650cc"><gtr:id>dfd6ade44eabf6326b0a4856385650cc</gtr:id><gtr:otherNames>Memoli G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/E64090A9-A6B2-472B-91DE-64DB91276489"><gtr:id>E64090A9-A6B2-472B-91DE-64DB91276489</gtr:id><gtr:title>Realization of compact tractor beams using acoustic delay-lines</gtr:title><gtr:parentPublicationTitle>Applied Physics Letters</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/01b11f12ed7d4e7000f48b42574e0b0b"><gtr:id>01b11f12ed7d4e7000f48b42574e0b0b</gtr:id><gtr:otherNames>Marzo A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/B08D6F8F-D46A-4420-8BEE-56EC4A9757C7"><gtr:id>B08D6F8F-D46A-4420-8BEE-56EC4A9757C7</gtr:id><gtr:title>Taming tornadoes: Controlling orbits inside acoustic vortex traps</gtr:title><gtr:parentPublicationTitle>The Journal of the Acoustical Society of America</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/01b11f12ed7d4e7000f48b42574e0b0b"><gtr:id>01b11f12ed7d4e7000f48b42574e0b0b</gtr:id><gtr:otherNames>Marzo A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/4A310BD3-F834-42D2-9CCF-B2FF9025594D"><gtr:id>4A310BD3-F834-42D2-9CCF-B2FF9025594D</gtr:id><gtr:title>Realization of compact tractor beams using acoustic delay-lines</gtr:title><gtr:parentPublicationTitle>Applied Physics Letters</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/01b11f12ed7d4e7000f48b42574e0b0b"><gtr:id>01b11f12ed7d4e7000f48b42574e0b0b</gtr:id><gtr:otherNames>Marzo A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/6E9B838E-255C-4D98-8E43-69B05675D34D"><gtr:id>6E9B838E-255C-4D98-8E43-69B05675D34D</gtr:id><gtr:title>Metamaterial bricks and quantization of meta-surfaces.</gtr:title><gtr:parentPublicationTitle>Nature communications</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/dfd6ade44eabf6326b0a4856385650cc"><gtr:id>dfd6ade44eabf6326b0a4856385650cc</gtr:id><gtr:otherNames>Memoli G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>2041-1723</gtr:issn></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/N014197/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>75</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>772CD758-53CD-407F-9B2C-F2B861E86155</gtr:id><gtr:percentage>25</gtr:percentage><gtr:text>Mechanical Engineering</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>B148AA4C-48F9-4B18-90F6-641750F725A1</gtr:id><gtr:percentage>25</gtr:percentage><gtr:text>Acoustics</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>0AD041FC-DCB2-46BB-B9CC-ADDFF2FA5E17</gtr:id><gtr:percentage>75</gtr:percentage><gtr:text>Human-Computer Interactions</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>