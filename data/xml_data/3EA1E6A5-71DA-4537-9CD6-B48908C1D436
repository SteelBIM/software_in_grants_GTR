<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.rcuk.ac.uk/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.rcuk.ac.uk:80/organisation/A037C26F-2172-4927-91DA-921F67B7EF39"><gtr:id>A037C26F-2172-4927-91DA-921F67B7EF39</gtr:id><gtr:name>University of the Arts London</gtr:name><gtr:department>Central Saint Martin's College</gtr:department><gtr:address><gtr:line1>Research Management &amp; Admininistration</gtr:line1><gtr:line2>University of the Arts London</gtr:line2><gtr:line3>6th Floor 272 High Holborn</gtr:line3><gtr:postCode>WC1V 7EY</gtr:postCode><gtr:region>Unknown</gtr:region></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/A037C26F-2172-4927-91DA-921F67B7EF39"><gtr:id>A037C26F-2172-4927-91DA-921F67B7EF39</gtr:id><gtr:name>University of the Arts London</gtr:name><gtr:address><gtr:line1>Research Management &amp; Admininistration</gtr:line1><gtr:line2>University of the Arts London</gtr:line2><gtr:line3>6th Floor 272 High Holborn</gtr:line3><gtr:postCode>WC1V 7EY</gtr:postCode><gtr:region>Unknown</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.rcuk.ac.uk:80/person/297B7775-F92F-47EC-A1EE-96339A7FB1E6"><gtr:id>297B7775-F92F-47EC-A1EE-96339A7FB1E6</gtr:id><gtr:firstName>Marco</gtr:firstName><gtr:surname>Aurisicchio</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.rcuk.ac.uk:80/person/0A79DA5D-908F-47C6-AA80-7DE448FCF01F"><gtr:id>0A79DA5D-908F-47C6-AA80-7DE448FCF01F</gtr:id><gtr:firstName>Sharon</gtr:firstName><gtr:surname>Baurley</gtr:surname><gtr:orcidId>0000-0003-1760-759X</gtr:orcidId><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.rcuk.ac.uk:80/person/DDA42B8C-0D81-45A2-9576-951E9A5BDA1C"><gtr:id>DDA42B8C-0D81-45A2-9576-951E9A5BDA1C</gtr:id><gtr:firstName>Penelope</gtr:firstName><gtr:otherNames>Ann</gtr:otherNames><gtr:surname>Watkins</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.rcuk.ac.uk:80/person/89A8C45D-1EE1-4412-B9C6-7C920B0986E1"><gtr:id>89A8C45D-1EE1-4412-B9C6-7C920B0986E1</gtr:id><gtr:firstName>Mike</gtr:firstName><gtr:surname>Chantler</gtr:surname><gtr:orcidId>0000-0002-8381-1751</gtr:orcidId><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.rcuk.ac.uk:80/person/098AF839-3F94-4990-87CB-3D0D0BAA0D76"><gtr:id>098AF839-3F94-4990-87CB-3D0D0BAA0D76</gtr:id><gtr:firstName>Nadia</gtr:firstName><gtr:otherNames>Luisa</gtr:otherNames><gtr:surname>Berthouze</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.rcuk.ac.uk:80/projects?ref=EP%2FH007083%2F1"><gtr:id>3EA1E6A5-71DA-4537-9CD6-B48908C1D436</gtr:id><gtr:title>Sandpit: Digital Sensoria : Design through digital perceptual experience</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/H007083/1</gtr:grantReference><gtr:abstractText>The sensorial experience of products in the digital environment is a much neglected area. Consumers' perceptions and attitudes towards products are formed and changed daily through interaction and experience with goods and services. A new language to communicate sensory stimuli has the potential to enable consumers to capture digitally their perceptions of products and use them to create value. With changes in consumer demand - namely that it is more complex and diffuse; with consumers demanding products that provide scope for greater personalisation, and that fulfil emotional needs - comes a need to gain a better understanding of consumer sensory preferences on levels hitherto unseen. Such a sensory or perceptual language would add value to a number of applications: A way to capture new types of market intelligence to help identify gaps in the market and areas for improvement and innovation; more effective communication of the attributes of products in e-retail. This level of knowledge would also support the communication between consumers and designers, perhaps leading to new transactional relationships between them.To do this we need new methods. Through this project we are proposing to give developers and consumers a means to articulate what is difficult to articulate: people's sensory perceptions of the different textiles commonly used in clothing products, thereby connecting the body to the digital world. Furthermore, we are proposing to explore this unique design space using a set of techniques that are established, but will be synthesised in a way hitherto unseen. Using design as a generative tool we will endeavour to mobilise people's tacit knowledge about, and new understandings around, sensory perceptions of textiles. We will use physiological sensing technology to obtain physiological responses to sensory materials, obtain self-report from people on those perceptions using generative co-design techniques to produce perceptual labels, and produce rich media representations of those same perceptions to provide the basic elements of our perceptual language.This digital perceptual language will be used as a method to communicate more effectively richer perceptual presentations of products, and to develop semantic tags which people can use to self-organise themselves around in social network web environments. Through these applications we aim to gain insight into: The effectiveness of the perceptual language as a way for brands to communicate rich sensory information about a product online, and therefore, a new method to add value; whether consumers are better able to make more informed choices due to better quality of information about a product, and whether they are better able to articulate what they want. Using semantic and social networking tools we also aim to gain insight into whether there is potential for new transactional and co-design relationships between users and developers/designers (thereby challenging the designer/user hierarchy) leading to new business models, e.g., market research through crowd-sourcing, and collaborative social design; and by extension whether new social and design community paradigms will emerge.</gtr:abstractText><gtr:fund><gtr:end>2010-04-30</gtr:end><gtr:funder url="http://gtr.rcuk.ac.uk:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2009-11-02</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>633617</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>Digital capture of sensory perceptions: An experimental design framework was used to create a complete picture of the perceptual space in which tactile properties of textiles are experienced by different parties (trained Designer, un-trained Consumer and Machine). 
EEG measure of hedonic tactile experience was used to measure people's subjective responses to tactile perception of fabrics. The results showed the possibility of dissociating a mere response to the physical touch of a fabric from the hedonic experience its touch produces. We also found a beta-band response in the right-hemisphere that correlated well with the subjective valence of tactile caressing. These results point to the possibility of building brain-computer interfaces that could be used to measure consumers' subjective tactile experiences. 

Crowdsoursing tactile subjective experience: Using a purpose built website (wardrobemalfunction.org.uk ) that combines and cross references multi-modal descriptive media, including representations of physiological states, we investigated the use of crowdsourcing mechanisms to engage consumers in providing and sharing enriched descriptions of their tactile experiences of garments. The results showed that a combination of modalities provide consumers with a greater understanding of, and engagement with, clothes online, and a desire of consumers to better understand themselves and others in relation to clothes. 

Digitally representing sensory perceptions: Design and evaluation of tools to digitally represent textiles in a multi-touch screen environment: (1) Design and evaluation of a gestural language for digitally handling fabrics, (2) Development and evaluation of design methods to create digital fabrics that better convey fabric properties (choreography of manipulations of textiles and development of filming techniques to film these manipulations for the multi-touch tablet environment), (3) Design and evaluation of the quality of digital sound produced by the digital fabric. The experiments resulted in the design by the consortium of a tool (i-Shoogle) to handle digital fabrics. 
The development of the digital tool ShoogleIT.com which allows inexpert users to use their browser to upload short film clips and turn these into interactive objects for use on smartphone, desktop or tablet computers. Currently the tool has more than 2000 interactive objects stored in its database. The objects have been viewed over 250,000 times by over 40,000 unique visitors from around 5,000 cities in over 150 countries. This tool has been further adapted to investigate the effect of sound on users' perceptions of these digital objects. 
A new multi-gesture, multi-touch application for tablet computers (i-Shoogle) has been developed. This uses the same database and in-browser film-clip processing infrastructure, but exploits multi-touch gesture recognition to allow users to 'pinch', 'stroke' and 'scrunch' digital objects. The result being that extremely engaging interactive media can be quickly produced by non-experts that react photorealistically to users' multiple gestures. These 'interactive objects' are particularly suited to communicating the properties of deformable materials.

Communication and processing of perceptions on the web: Developed design Visual Understanding Environment (which is a file-based software application, which enables collaborative design by a team or community of individuals interested in developing a product idea. The results of designVUE evaluation have shown that participants were able to use the tool to explore a brief, capture alternative solutions through text and visual representations, and elicit their perceptions of them. The files generated through designVUE enabled the creation of shared mental models of the design task, and supported the planning and execution of the project. The software has had approximately 750 downloads in the period from 01 Dec 2011 to 26 Oct 2012.</gtr:description><gtr:exploitationPathways>The Shoogle-it Iplayer could be adopted by online retail.</gtr:exploitationPathways><gtr:id>F83CA56E-DDEA-41AC-ACF5-144E537BAC4E</gtr:id><gtr:sectors><gtr:sector>Creative Economy,Digital/Communication/Information Technologies (including Software),Retail</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs><gtr:softwareAndTechnicalProductOutput><gtr:description>Touch screen interactive player to simulate movement of textile materials.</gtr:description><gtr:id>CCC569F1-4765-4E53-827E-952CE91EA45A</gtr:id><gtr:impact>None</gtr:impact><gtr:title>Shoogleit Player</gtr:title><gtr:type>Software</gtr:type><gtr:url>https://itunes.apple.com/us/app/shoogleit-player/id427085804?mt=8</gtr:url><gtr:yearFirstProvided>2012</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput></gtr:softwareAndTechnicalProductOutputs><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/73C2C3D0-DEFD-4E11-AA40-A5043A912A43"><gtr:id>73C2C3D0-DEFD-4E11-AA40-A5043A912A43</gtr:id><gtr:title>The future of textiles Sourcing: Exploring the potential for digital tools</gtr:title><gtr:parentPublicationTitle>9th International Conference on Design &amp; Emotion 2014</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/45734fcdb2cb0f0eed9c5b8d5da756a3"><gtr:id>45734fcdb2cb0f0eed9c5b8d5da756a3</gtr:id><gtr:otherNames>Petreca, B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/56F5AC6D-5E85-4A59-A2A6-7706FCD4B105"><gtr:id>56F5AC6D-5E85-4A59-A2A6-7706FCD4B105</gtr:id><gtr:title>The brain's response to pleasant touch: an EEG investigation of tactile caressing.</gtr:title><gtr:parentPublicationTitle>Frontiers in human neuroscience</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/573f5022b2bdd30e0ecb6241763d2a7e"><gtr:id>573f5022b2bdd30e0ecb6241763d2a7e</gtr:id><gtr:otherNames>Singh H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>1662-5161</gtr:issn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/E9F99D14-D476-45ED-B535-92B2EAAE5A8E"><gtr:id>E9F99D14-D476-45ED-B535-92B2EAAE5A8E</gtr:id><gtr:title>Affective Computing and Intelligent Interaction</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/019c4091ed74e233312a546e2b92af2e"><gtr:id>019c4091ed74e233312a546e2b92af2e</gtr:id><gtr:otherNames>Wu D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:isbn>978-3-642-24599-2</gtr:isbn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/36F9F1D4-AC08-4D79-9BBC-0815E8EC6EF2"><gtr:id>36F9F1D4-AC08-4D79-9BBC-0815E8EC6EF2</gtr:id><gtr:title>Archiving and Simulation of Fabrics with Multi-Gesture Interfaces</gtr:title><gtr:parentPublicationTitle>MobileHCI</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/748f40998a0e3cd981ea9db04aba88f0"><gtr:id>748f40998a0e3cd981ea9db04aba88f0</gtr:id><gtr:otherNames>Orzechowski, P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/F768D544-CB7A-4B71-A30F-EEC53FED244C"><gtr:id>F768D544-CB7A-4B71-A30F-EEC53FED244C</gtr:id><gtr:title>Tactile perceptions of digital textiles</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/e8d154ab1ab2ee47eef2db26be292b89"><gtr:id>e8d154ab1ab2ee47eef2db26be292b89</gtr:id><gtr:otherNames>Atkinson D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:isbn>9781450318990</gtr:isbn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/E0178C1F-AD27-4ECE-96CE-DCE8DFC1334E"><gtr:id>E0178C1F-AD27-4ECE-96CE-DCE8DFC1334E</gtr:id><gtr:title>Crowdsourcing an emotional wardrobe</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/2ab752163c053d4b81caca5a80f175f1"><gtr:id>2ab752163c053d4b81caca5a80f175f1</gtr:id><gtr:otherNames>Hughes L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>9781450310161</gtr:isbn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/E08E643B-B68E-4E77-9DC3-E078FEBCDA2E"><gtr:id>E08E643B-B68E-4E77-9DC3-E078FEBCDA2E</gtr:id><gtr:title>An Embodiment Perspective of Affective Touch Behaviour in Experiencing Digital Textiles</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/8819dcad793024ca936b3928878fa810"><gtr:id>8819dcad793024ca936b3928878fa810</gtr:id><gtr:otherNames>Petreca B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/A14B3918-A4C9-47FC-AD92-445B813EF16F"><gtr:id>A14B3918-A4C9-47FC-AD92-445B813EF16F</gtr:id><gtr:title>Interactivity to enhance perception</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/21bcb7b3006f05137a179842c632b7e6"><gtr:id>21bcb7b3006f05137a179842c632b7e6</gtr:id><gtr:otherNames>Orzechowski P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date><gtr:isbn>9781450305419</gtr:isbn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/F7B7A253-7969-45FD-80B0-9F8F5D429DB8"><gtr:id>F7B7A253-7969-45FD-80B0-9F8F5D429DB8</gtr:id><gtr:title>Synthesising design methodologies for the transmission of tactile qualities in digital media</gtr:title><gtr:parentPublicationTitle>Digital Engagement '11</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/bf41c51daab04fad7953f46c113bc4ec"><gtr:id>bf41c51daab04fad7953f46c113bc4ec</gtr:id><gtr:otherNames>Atkinson, D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2011-01-01</gtr:date></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/H007083/1</gtr:identifier></gtr:identifiers><gtr:projectHierarchy><gtr:parents><gtr:parent><gtr:id>3EA1E6A5-71DA-4537-9CD6-B48908C1D436</gtr:id><gtr:grantRef>EP/H007083/1</gtr:grantRef><gtr:amount>633617.98</gtr:amount><gtr:start>2009-11-02</gtr:start><gtr:end>2010-04-30</gtr:end><gtr:children><gtr:child rel="Transfer"><gtr:id>6C70731C-CC53-4852-9F7A-F043FB821BFC</gtr:id><gtr:grantRef>EP/H007083/2</gtr:grantRef><gtr:amount>525374.78</gtr:amount><gtr:start>2010-05-01</gtr:start><gtr:end>2012-07-31</gtr:end><gtr:children/></gtr:child></gtr:children></gtr:parent></gtr:parents></gtr:projectHierarchy><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>0AD041FC-DCB2-46BB-B9CC-ADDFF2FA5E17</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Human-Computer Interactions</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>541CA340-B127-4309-84C6-51C40A48B4DA</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Vision &amp; Senses - ICT appl.</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>