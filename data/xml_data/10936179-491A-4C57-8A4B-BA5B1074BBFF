<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.rcuk.ac.uk/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.rcuk.ac.uk:80/organisation/468F2797-5295-4912-BDED-8F3402CE246A"><gtr:id>468F2797-5295-4912-BDED-8F3402CE246A</gtr:id><gtr:name>New York University</gtr:name><gtr:address><gtr:line1>7 East 12th Street</gtr:line1><gtr:region>Outside UK</gtr:region><gtr:country>United States</gtr:country></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.rcuk.ac.uk:80/organisation/BA2685AB-275C-4612-8454-D90790672A46"><gtr:id>BA2685AB-275C-4612-8454-D90790672A46</gtr:id><gtr:name>York University Canada</gtr:name><gtr:address><gtr:line1>4700 Keele Street</gtr:line1><gtr:line4>Toronto</gtr:line4><gtr:line5>Ontario M3J 1P3</gtr:line5><gtr:region>Outside UK</gtr:region><gtr:country>Canada</gtr:country></gtr:address></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.rcuk.ac.uk:80/organisation/30A429E3-83B7-4E41-99C0-14A144F07DFE"><gtr:id>30A429E3-83B7-4E41-99C0-14A144F07DFE</gtr:id><gtr:name>University of Southampton</gtr:name><gtr:department>School of Psychology</gtr:department><gtr:address><gtr:line1>Administration Building</gtr:line1><gtr:line2>Highfield</gtr:line2><gtr:line4>Southampton</gtr:line4><gtr:line5>Hampshire</gtr:line5><gtr:postCode>SO17 1BJ</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/30A429E3-83B7-4E41-99C0-14A144F07DFE"><gtr:id>30A429E3-83B7-4E41-99C0-14A144F07DFE</gtr:id><gtr:name>University of Southampton</gtr:name><gtr:address><gtr:line1>Administration Building</gtr:line1><gtr:line2>Highfield</gtr:line2><gtr:line4>Southampton</gtr:line4><gtr:line5>Hampshire</gtr:line5><gtr:postCode>SO17 1BJ</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/468F2797-5295-4912-BDED-8F3402CE246A"><gtr:id>468F2797-5295-4912-BDED-8F3402CE246A</gtr:id><gtr:name>New York University</gtr:name><gtr:address><gtr:line1>7 East 12th Street</gtr:line1><gtr:region>Outside UK</gtr:region><gtr:country>United States</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/BA2685AB-275C-4612-8454-D90790672A46"><gtr:id>BA2685AB-275C-4612-8454-D90790672A46</gtr:id><gtr:name>York University Canada</gtr:name><gtr:address><gtr:line1>4700 Keele Street</gtr:line1><gtr:line4>Toronto</gtr:line4><gtr:line5>Ontario M3J 1P3</gtr:line5><gtr:region>Outside UK</gtr:region><gtr:country>Canada</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/67B34D05-3A52-4A09-8FF9-F2398D60E246"><gtr:id>67B34D05-3A52-4A09-8FF9-F2398D60E246</gtr:id><gtr:name>University of Surrey</gtr:name><gtr:address><gtr:line1>Registry</gtr:line1><gtr:line2>Stag Hill</gtr:line2><gtr:line4>Guildford</gtr:line4><gtr:line5>Surrey</gtr:line5><gtr:postCode>GU2 7XH</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/BAD3780F-6F65-4292-A69C-923CAE91A7BB"><gtr:id>BAD3780F-6F65-4292-A69C-923CAE91A7BB</gtr:id><gtr:name>British Broadcasting Corporation - BBC</gtr:name><gtr:address><gtr:line1>British Broadcasting Corporation</gtr:line1><gtr:line2>Broadcasting House</gtr:line2><gtr:line3>Portland Place</gtr:line3><gtr:line4>London</gtr:line4><gtr:postCode>W1A 1AA</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.rcuk.ac.uk:80/person/6937133D-9AC9-448F-86EF-5533FF4D6984"><gtr:id>6937133D-9AC9-448F-86EF-5533FF4D6984</gtr:id><gtr:firstName>Wendy</gtr:firstName><gtr:otherNames>Jo</gtr:otherNames><gtr:surname>Adams</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.rcuk.ac.uk:80/person/DF685395-CC3A-47F8-80D9-918B3D0B781F"><gtr:id>DF685395-CC3A-47F8-80D9-918B3D0B781F</gtr:id><gtr:firstName>Erich</gtr:firstName><gtr:otherNames>William</gtr:otherNames><gtr:surname>Graf</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.rcuk.ac.uk:80/person/CA8DE00E-9769-4371-A48E-D9C4056D74C6"><gtr:id>CA8DE00E-9769-4371-A48E-D9C4056D74C6</gtr:id><gtr:firstName>Julian</gtr:firstName><gtr:surname>Leyland</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.rcuk.ac.uk:80/projects?ref=EP%2FK005952%2F1"><gtr:id>10936179-491A-4C57-8A4B-BA5B1074BBFF</gtr:id><gtr:title>Human Vision: Relationship to Three-Dimensional Surface Statistics of Natural Scenes</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/K005952/1</gtr:grantReference><gtr:abstractText>The human visual system has been fine-tuned over generations of evolution to operate effectively in our particular environment, allowing us to form rich 3D representations of the objects around us. The scenes that we encounter on a daily basis produce 2D retinal images that are complex and ambiguous. From this input, how does the visual system achieve the immensely difficult goal of recovering our surroundings, in such an impressively fast and robust way? 
To achieve this feat, humans must use two types of information about their environment. First, we must learn the probabilistic relationships between 3D natural scene properties and the 2D image cues these produce. Second, we must learn which scene structures (shapes, distances, orientations) are most common, or probable in our 3D environment. This statistical knowledge about natural 3D scenes and their projected images allows us to maximize our perceptual performance. To better understand 3D perception, therefore, we must study the environment that we have evolved to process. A key goal of our research is to catalogue and evaluate the statistical structure of the environment that guides human depth perception. We will sample the range of scenes that humans frequently encounter (indoor and outdoor environments over different seasons and lighting conditions). For each scene, state-of-the-art ground based Light Detection and Ranging (LiDAR) technology will be used to measure the physical distance to all objects (trees, ground, etc.) from a single location - a 3D map of the scene. We will also take High Dynamic Range (HDR) photographs of the same scene, from the same vantage point. By collating this paired 3D and 2D data across numerous scenes we will create a comprehensive database of our environment, and the 2D images that it produces. By making the database publicly available it will facilitate not just our own work, but research by human and computer vision scientists around the world who are interested in a range of pure and applied visual processes.
There is great potential for computer vision to learn from the expert processor that is the human visual system: computer vision algorithms are easily out-performed by humans for a range of tasks, particularly when images correspond to more complex, realistic scenes. We are still far from understanding how the human visual system handles the kind of complex natural imagery that defeats computer vision algorithms. However, the robustness of the human visual system appears to hinge on: 1) exploiting the full range of available depth cues and 2) incorporating statistical 'priors': information about typical scene configurations. We will employ psychophysical experiments, guided by our analyses of natural scenes and their images, to develop valid and comprehensive computational models of human depth perception. We will concentrate our analysis and experimentation on key tasks in the process of recovering scene structure - estimating the location, orientation and curvature of surface segments across the environment. Our project addresses the need for more complex and ecologically valid models of human perception by studying how the brain implicitly encodes and interprets depth information to guide 3D perception.
Virtual 3D environments are now used in a range of settings, such as flight simulation and training systems, rehabilitation technologies, gaming, 3D movies and special effects. Perceptual biases are particularly influential when visual input is degraded, as they are in some of these simulated environments. To evaluate and improve these technologies we require a better understanding of 3D perception. In addition, the statistical models and inferential algorithms developed in the project will facilitate the development of computer vision algorithms for automatic estimation of depth structure in natural scenes. These algorithms have many applications, such as 2D to 3D film conversion, visual surveillance and biometrics.</gtr:abstractText><gtr:potentialImpactText>Our proposed work sits at the interface of human and computer vision. In essence, it asks how humans and computers infer 3D structure from 2D images in realistic, complex environments. Beyond these academic arenas, our work has clear implications for those working in applied computer vision and in the visual media industry. The latter two groups will exploit our work in technologies such as 2D to 3D conversion, special effects generation and creating virtual reality environments for applications such as gaming and training. The recent NextGen Review (2011) of the skill requirements (and current shortfall) for the UK's video games and visual effects industries highlighted the importance of those industries to the UK economy. In 2008, the global sales of video games created by UK companies reached &amp;pound;2 billion, contributing &amp;pound;1 billion in GDP, making the UK the third largest games developer in the world. The UK visual effects industry is also on the rise, contributing to blockbuster movies like Harry Potter, Inception and Batman. This sector grew by 17% between 2006 and 2008, with four of the worlds largest visual effects companies based in London.
Our work has a clear role in maintaining the lead role that the UK currently holds in these growing industries. These world-leading industries could benefit substantially from the input of experts in vision science. For example, many of the industry applications described above involve the inference of 3D scene structure from 2D images, but often have to rely on human hand segmentation and depth labeling of images to complement current computational algorithms. In contrast, humans are remarkably adept and robust in reconstructing their 3D world. Our work will expand current understanding of the structure of natural scenes, and how this statistical structure is exploited by the human visual system to efficiently recover depth. This ecological, natural scenes approach is critical to bridging the gap between human performance a current efforts to replicate it in computer vision applications. To ensure that this impact is realized, vision scientists must engage with those involved in gaming and visual effects. Currently, there is a lack of communication between vision researchers, and these industrial groups. Dr. Adams' discussions with attendees at the recent Conference on Visual Media Production (CVMP) in London made clear the potential for human vision to inform algorithms for visual media production that are efficient, and produce content that is realistic and enjoyable for the end user. For example, certain well-known strategies within human vision for recovering shape from shading are not exploited within technologies that capture and create 3D content. Discussions with our project partners (Hilton: applied computer vision and Grau: 3D media production, BBC) have identified particular areas where our work will inform current problems within applied computer vision and visual media production; the potential impact of our work is reflected in their Letters of Support.
The NextGen report identified an immediate need to change current practice in ICT training in schools and Universities to better reflect the skill needs of the gaming and visual media industries. This move is critical to ensure that these industries continue to lead the world market. Vision science relies heavily on the key skills highlighted in the report, including mathematics, physics, computer programming and design. By using visual illusions to explain key concepts in human vision, and demonstrating the mathematical and computational challenges in developing visual stimuli, we will design activities (for our website and for the science roadshow) that will engage young people and foster an interest in mathematics, perceptual psychology and its applications. By making use of the engaging aspects of visual perception we hope to inspire future generations of scientists and industry professionals.</gtr:potentialImpactText><gtr:fund><gtr:end>2016-12-29</gtr:end><gtr:funder url="http://gtr.rcuk.ac.uk:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2013-06-30</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>505830</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>York University, Canada</gtr:collaboratingOrganisation><gtr:country>Canada</gtr:country><gtr:description>Depth and scene gist</gtr:description><gtr:id>396B3B41-DC5A-4F6A-99CB-CCB99E7B19B1</gtr:id><gtr:impact>None yet</gtr:impact><gtr:partnerContribution>Addition of expertise in stereo depth processing from Professor Laurie Wilcox</gtr:partnerContribution><gtr:piContribution>A collaborative research project, I am conducting the research using the SYNS dataset that was created as a key outcome of the EPSRC grant</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2016-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>New York University</gtr:collaboratingOrganisation><gtr:country>United States of America</gtr:country><gtr:description>Light fields and perceived gloss</gtr:description><gtr:id>625A512F-5059-4DA1-B3F4-2145CEF56745</gtr:id><gtr:impact>Two conference presentations (1 poster, 1 talk, both with published conference abstracts).</gtr:impact><gtr:partnerContribution>The graduate student is conducting some of the research under my supervision</gtr:partnerContribution><gtr:piContribution>I am working in collaboration with Professor Mike Landy and his graduate student, Gizem Kucukoglu, on two research projects.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2014-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>Cheltenham Science Festival</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>8E38B9A8-5586-4702-9C8E-7DE3D5A5CB2F</gtr:id><gtr:impact>Our research group had a stand at the festival. Of approximately 45000 attendees, we engaged directly with 7500 people. 
Visitors engaged in visual illusion activities, talked to researchers, and took away handout activities.</gtr:impact><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Glastonbury Festival</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>27014CCE-8E7C-4CD4-B0CD-FF0F7415AFA8</gtr:id><gtr:impact>We had a display stand about human visual perception, which included activities, information, and take away activities. 
5600 people engaged with the science display from our research group. 
There were many questions and discussions.</gtr:impact><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Thomas Hardye School visit, Dorchester</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>Regional</gtr:geographicReach><gtr:id>34CFC7A1-D1D8-4508-811A-C081F46B07B8</gtr:id><gtr:impact>Visit to the school to speak to and do activities with GCSE and AS level students about visual processing. 
500 students engaged with our display, participating in activities, taking away illusion-based handouts.</gtr:impact><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Schools</gtr:primaryAudience><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Bestival</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>1D8E6729-73E9-4F7A-8208-500DE05F6B18</gtr:id><gtr:impact>We had a interactive stand on the topic of human visual processing. 
5600 people engaged with the activities from our research group. 
There were many questions and discussions.</gtr:impact><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Science And Engineering Day</gtr:description><gtr:form>Participation in an open day or visit at my research institution</gtr:form><gtr:geographicReach>Regional</gtr:geographicReach><gtr:id>9A3DD382-88C5-4D80-92AA-85AD8581D168</gtr:id><gtr:impact>4000 people attended the University of Southampton Science and Engineering day.
Many questions and discussions about visual processing.</gtr:impact><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>http://www.southampton.ac.uk/per/university/festival/science-and-engineering-day.page</gtr:url><gtr:year>2015,2016</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>500</gtr:amountPounds><gtr:country>Unknown</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>ViiHM collaboration grant</gtr:description><gtr:end>2015-09-02</gtr:end><gtr:fundingOrg>Visual image interpretation in humans and machines (ViiHM)</gtr:fundingOrg><gtr:id>13CE345B-0F9C-4987-AC91-F12C57118AA5</gtr:id><gtr:sector>Academic/University</gtr:sector><gtr:start>2015-09-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>As detailed in the 'engagement activities', we have used our work in a number of outreach and education activities. We have taken details of our research project, alongside more accessible information about human vision processing, to a number of schools, festivals and outreach events. We have engaged with tens of thousands of members of the public and school children. We have worked with the Winchester Science museum to create a suite of exhibits on the topic of human vision processing, with associated education materials for use in schools. These exhibitions have been hugely popular with visitors
In addition, our natural scenes dataset (SYNS) is now public, and has around 150 active users, who have made nearly 2000 downloads from the dataset. These include academics from all over the world, but also users from industry, and public health.</gtr:description><gtr:firstYearOfImpact>2015</gtr:firstYearOfImpact><gtr:id>12CDD2DF-B54B-4FBC-A56B-E084B34A8F8D</gtr:id><gtr:impactTypes><gtr:impactType>Cultural,Economic</gtr:impactType></gtr:impactTypes><gtr:sector>Digital/Communication/Information Technologies (including Software),Education,Healthcare,Security and Diplomacy</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>We are interested in the structure of our natural environment, how this shapes human perception, and how the information can be exploited in computer vision (for example, estimating 3D structure from a 2D image.
One substantial key finding is our set of measurements of scenes, sampled from the environment within Hampshire, UK. This Southampton-York Natural Scenes Dataset (SYNS) is now a public dataset that researchers and industrial users (working in virtual reality, computer vision) are downloading and using for their research, or for product development.

The measurements taken at each scene include measurements of the 3D structure of the scene, a high dynamic range spherical image of the scene, and a panorama of stereo image pairs.

In addition, we have analysed the 3D structure of the scenes, to show how surface attitude (slant and tilt) varies across different types of scenes and elevations.

We have also used the dataset to investigate a number of different ways in which human vision is tuned to the statistics of the natural environment. For example, we have shown how this knowledge effects the perception of gloss. We have also shown how natural scene statistics bias our judgements of slant and tilt.

We are also using the dataset to show how edges in an image can be categorised, for example, to segment objects from their background.</gtr:description><gtr:exploitationPathways>Our work can be taken forward in two key ways:
1) Other groups can use the public dataset as a critical tool in testing computer vision algorithms, or understanding human perception. As noted elsewhere, our dataset already has around 150 users, and we expect many more.
2) Other groups can build on our research findings - how natural statistics shape perception - to further understanding of human sensory perception.</gtr:exploitationPathways><gtr:id>6E5DB546-DFB5-4C2D-A578-4307E2ECFE5F</gtr:id><gtr:sectors><gtr:sector>Digital/Communication/Information Technologies (including Software),Education,Security and Diplomacy</gtr:sector></gtr:sectors><gtr:url>https://syns.soton.ac.uk</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs><gtr:researchDatabaseAndModelOutput><gtr:description>Creating a database of natural scenes is a significant milestone for this EPSRC project. For each natural scene, we provide three types of data (i) 3D point cloud data from LiDAR, (ii) high dynamic range spherical images and (iii) stereoscopic, high resolution image pairs.</gtr:description><gtr:id>DB0846D2-205A-471E-83E7-E81E93A96FAE</gtr:id><gtr:impact>The data will be publicly available to all research groups within the next few months. I gave an invited talk at a recent conference (ViiHM), where I presented our work on this database, and some analyses of the point cloud data.
There was a great deal of interest from other research groups (both human and computer vision scientists). We predict that the database will be widely used by other researchers who wish to understand human vision, or develop computer vision algorithms, for various problems such as image segmentation and depth estimation. 

Please note that the website at the URL provided is not yet fully functional.</gtr:impact><gtr:providedToOthers>false</gtr:providedToOthers><gtr:title>SYNS</gtr:title><gtr:type>Database/Collection of data</gtr:type><gtr:url>http://synsdata.soton.ac.uk</gtr:url></gtr:researchDatabaseAndModelOutput></gtr:researchDatabaseAndModelOutputs><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/F6E6ADA6-59B9-4CF0-A2EC-432D54112EE9"><gtr:id>F6E6ADA6-59B9-4CF0-A2EC-432D54112EE9</gtr:id><gtr:title>Visual discrimination of surface attitude from texture</gtr:title><gtr:parentPublicationTitle>PERCEPTION</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/d6c20bed7ecaa0311846cbbdffc975a0"><gtr:id>d6c20bed7ecaa0311846cbbdffc975a0</gtr:id><gtr:otherNames>Blusseau Samy</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>0301-0066</gtr:issn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/385DDC3F-36D5-4316-B027-094DE82531BD"><gtr:id>385DDC3F-36D5-4316-B027-094DE82531BD</gtr:id><gtr:title>Perception of 3D structure and natural scene statistics: The Southampton-York Natural Scenes (SYNS) dataset.</gtr:title><gtr:parentPublicationTitle>Journal of Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/7f466b0ff6f39f128b1568dbd2c3f0d0"><gtr:id>7f466b0ff6f39f128b1568dbd2c3f0d0</gtr:id><gtr:otherNames>Adams W</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>1534-7362</gtr:issn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/D0A5778B-0960-4512-B3DC-ECDEAC797371"><gtr:id>D0A5778B-0960-4512-B3DC-ECDEAC797371</gtr:id><gtr:title>Gloss constancy across changes in illumination</gtr:title><gtr:parentPublicationTitle>Journal of Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/f3e020be0d5c24265aa21076b23d09e6"><gtr:id>f3e020be0d5c24265aa21076b23d09e6</gtr:id><gtr:otherNames>Kucukoglu G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>1534-7362</gtr:issn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/967F02E0-9050-4E2B-BE1A-DD71ACE12294"><gtr:id>967F02E0-9050-4E2B-BE1A-DD71ACE12294</gtr:id><gtr:title>The effect of the bounding contour on the perception of surface shape</gtr:title><gtr:parentPublicationTitle>Journal of Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/60086e35176abd1a1fa517b1f236e216"><gtr:id>60086e35176abd1a1fa517b1f236e216</gtr:id><gtr:otherNames>Graf E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>1534-7362</gtr:issn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/1C18031E-724C-434B-82E1-D25BAB6DB97D"><gtr:id>1C18031E-724C-434B-82E1-D25BAB6DB97D</gtr:id><gtr:title>Interactions between slant and tilt perception</gtr:title><gtr:parentPublicationTitle>PERCEPTION</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/8d025bad54ea013c469b1f6b5e0c2b40"><gtr:id>8d025bad54ea013c469b1f6b5e0c2b40</gtr:id><gtr:otherNames>Lugtigheid A.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>0301-0066</gtr:issn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/0C75FC89-9954-4721-97AE-A2B4B25403EE"><gtr:id>0C75FC89-9954-4721-97AE-A2B4B25403EE</gtr:id><gtr:title>Estimating local surface attitude from 3D point cloud data.</gtr:title><gtr:parentPublicationTitle>Journal of Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/da99919b0cfba3a6fcce657634c3700a"><gtr:id>da99919b0cfba3a6fcce657634c3700a</gtr:id><gtr:otherNames>Muryy A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/813D047A-05CC-4DE9-A808-30A6B4574B0B"><gtr:id>813D047A-05CC-4DE9-A808-30A6B4574B0B</gtr:id><gtr:title>Estimating 3D surface properties of natural scenes</gtr:title><gtr:parentPublicationTitle>Journal of Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/da99919b0cfba3a6fcce657634c3700a"><gtr:id>da99919b0cfba3a6fcce657634c3700a</gtr:id><gtr:otherNames>Muryy A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>1534-7362</gtr:issn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/C5F61133-326F-428A-A95E-6A4E99CE65BF"><gtr:id>C5F61133-326F-428A-A95E-6A4E99CE65BF</gtr:id><gtr:title>The Southampton-York Natural Scenes (SYNS) dataset: Statistics of surface attitude.</gtr:title><gtr:parentPublicationTitle>Scientific reports</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/e62b9f7075de7c2b7263480eafa33149"><gtr:id>e62b9f7075de7c2b7263480eafa33149</gtr:id><gtr:otherNames>Adams WJ</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>2045-2322</gtr:issn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/5B4134C2-4916-4732-9536-0F4694C1CA8D"><gtr:id>5B4134C2-4916-4732-9536-0F4694C1CA8D</gtr:id><gtr:title>Biases in perceived slant and tilt of real surfaces</gtr:title><gtr:parentPublicationTitle>Journal of Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/d4b0a4319bfa923297522d9faeb48769"><gtr:id>d4b0a4319bfa923297522d9faeb48769</gtr:id><gtr:otherNames>Lugtigheid A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>1534-7362</gtr:issn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/C27E48CA-BF26-4355-8490-82A4ED35C011"><gtr:id>C27E48CA-BF26-4355-8490-82A4ED35C011</gtr:id><gtr:title>Effects of specular highlights on perceived surface convexity.</gtr:title><gtr:parentPublicationTitle>PLoS computational biology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/e62b9f7075de7c2b7263480eafa33149"><gtr:id>e62b9f7075de7c2b7263480eafa33149</gtr:id><gtr:otherNames>Adams WJ</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>1553-734X</gtr:issn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/04C8F4D1-9B67-4AC0-8582-7A08D5567485"><gtr:id>04C8F4D1-9B67-4AC0-8582-7A08D5567485</gtr:id><gtr:title>SYNS dataset of natural scenes measurements</gtr:title><gtr:parentPublicationTitle>PERCEPTION</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/6ee29a4df960c5166a162e117e8eaeb8"><gtr:id>6ee29a4df960c5166a162e117e8eaeb8</gtr:id><gtr:otherNames>Muryy A.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>0301-0066</gtr:issn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/52081584-9E36-4D32-82A3-E25E9E3FF1D3"><gtr:id>52081584-9E36-4D32-82A3-E25E9E3FF1D3</gtr:id><gtr:title>A model of local adaptation</gtr:title><gtr:parentPublicationTitle>ACM Transactions on Graphics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/a50d2bdfc64d4f38f5d2ea1ac1ff7df2"><gtr:id>a50d2bdfc64d4f38f5d2ea1ac1ff7df2</gtr:id><gtr:otherNames>Vangorp P</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/BF82CB9C-FDA3-4680-92FD-2E7ABDA6C4A0"><gtr:id>BF82CB9C-FDA3-4680-92FD-2E7ABDA6C4A0</gtr:id><gtr:title>Natural scene statistics and estimation of shape and reflectance.</gtr:title><gtr:parentPublicationTitle>Journal of Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/7f466b0ff6f39f128b1568dbd2c3f0d0"><gtr:id>7f466b0ff6f39f128b1568dbd2c3f0d0</gtr:id><gtr:otherNames>Adams W</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/6F675987-1CF7-4D36-AC68-A5FA0B21F2BA"><gtr:id>6F675987-1CF7-4D36-AC68-A5FA0B21F2BA</gtr:id><gtr:title>Touch influences perceived gloss.</gtr:title><gtr:parentPublicationTitle>Scientific reports</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/e62b9f7075de7c2b7263480eafa33149"><gtr:id>e62b9f7075de7c2b7263480eafa33149</gtr:id><gtr:otherNames>Adams WJ</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>2045-2322</gtr:issn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/3AB2704A-97C6-407F-B935-3023D9F73C42"><gtr:id>3AB2704A-97C6-407F-B935-3023D9F73C42</gtr:id><gtr:title>Interactions between illumination, shape and reflectance</gtr:title><gtr:parentPublicationTitle>PERCEPTION</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/0db82cfb589e75ffb86224925f456960"><gtr:id>0db82cfb589e75ffb86224925f456960</gtr:id><gtr:otherNames>Adams Wendy J.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>0301-0066</gtr:issn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/47F53B75-DEC1-462F-A31D-BBFD7999E21E"><gtr:id>47F53B75-DEC1-462F-A31D-BBFD7999E21E</gtr:id><gtr:title>The Southampton York natural scenes (SYNS) dataset</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/7f466b0ff6f39f128b1568dbd2c3f0d0"><gtr:id>7f466b0ff6f39f128b1568dbd2c3f0d0</gtr:id><gtr:otherNames>Adams W</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:isbn>9781450335607</gtr:isbn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/2A4EC755-C11A-426F-A535-1B96AD1F6D88"><gtr:id>2A4EC755-C11A-426F-A535-1B96AD1F6D88</gtr:id><gtr:title>Joint estimation of surface gloss and 3D shape</gtr:title><gtr:parentPublicationTitle>Journal of Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/f3e020be0d5c24265aa21076b23d09e6"><gtr:id>f3e020be0d5c24265aa21076b23d09e6</gtr:id><gtr:otherNames>Kucukoglu G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/K005952/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>541CA340-B127-4309-84C6-51C40A48B4DA</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Vision &amp; Senses - ICT appl.</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>