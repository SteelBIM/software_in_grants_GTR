<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.rcuk.ac.uk/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.rcuk.ac.uk:80/organisation/8044C2F3-70A8-44B3-A919-3A5ADEFD2311"><gtr:id>8044C2F3-70A8-44B3-A919-3A5ADEFD2311</gtr:id><gtr:name>Francis Crick Institute</gtr:name></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.rcuk.ac.uk:80/organisation/3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9"><gtr:id>3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9</gtr:id><gtr:name>University of Oxford</gtr:name><gtr:department>Oxford Physics</gtr:department><gtr:address><gtr:line1>University Chest</gtr:line1><gtr:line2>Wellington Square</gtr:line2><gtr:line4>Oxford</gtr:line4><gtr:postCode>OX1 2JD</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9"><gtr:id>3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9</gtr:id><gtr:name>University of Oxford</gtr:name><gtr:address><gtr:line1>University Chest</gtr:line1><gtr:line2>Wellington Square</gtr:line2><gtr:line4>Oxford</gtr:line4><gtr:postCode>OX1 2JD</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/8044C2F3-70A8-44B3-A919-3A5ADEFD2311"><gtr:id>8044C2F3-70A8-44B3-A919-3A5ADEFD2311</gtr:id><gtr:name>Francis Crick Institute</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.rcuk.ac.uk:80/person/BD018BD2-A7E8-4BBB-9721-EDB258398D88"><gtr:id>BD018BD2-A7E8-4BBB-9721-EDB258398D88</gtr:id><gtr:firstName>Christopher</gtr:firstName><gtr:otherNames>John</gtr:otherNames><gtr:surname>Lintott</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.rcuk.ac.uk:80/projects?ref=ST%2FN003179%2F1"><gtr:id>81F5B7E7-E00D-46EC-BD33-2F6C3C25D6AB</gtr:id><gtr:title>Human-Machine Classification for Astrophysical Projects</gtr:title><gtr:status>Active</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>ST/N003179/1</gtr:grantReference><gtr:abstractText>A major challenge for twenty-first century science is in learning to deal with the large datasets which are rapidly produced by a wide variety of modern surveys and experiments. This is especially true for astrophysicists, who not only deal with large and diverse datasets but who often have to make rapid decisions about which objects to target with future observations. Two separate sets of solution have been proposed. The first relies on advances in computer vision and machine learning to automate the process of astronomical classification, but much of the recent progress in these fields relies on the availability of large training sets of already classified data, something that is difficult to supply in many cases. In many cases, the datasets are so large that even very accurate computer classification will leave an enormous dataset to be sifted through.

The other solution has been through citizen science, collaboration with volunteers in order to work through large datasets. This has been enormously successful for a wide variety of astronomical problems, from the discovery of extra-solar planets to galaxy classification and space weather forecasting. However, the size of datasets expected from new astronomical surveys will overwhelm even the largest and most enthusiastic volunteer base.

This proposal aims to demonstrate a flexible system which can combine these two methods. It builds on the successful Zooniverse platform, which has been responsible for many of the most successful citizen science data analysis projects. We will :

1. Produce more efficient citizen science projects by being smart about assigning tasks to particular volunteers. At present, for example, images which are to be classified are shown randomly to volunteers instead of allowing experts to review more difficult cases. Our preliminary research shows this may increase our efficiency by a factor of ten. 
2. Include machine and human classifiers together in the same project. As volunteers work their way through a dataset, so machines can learn from them. This allows an increasing proportion of the dataset to be automatically processed, reducing the burden on the volunteers. 
3. Combine both of these new facilities allowing us to sort through data in a new way. We will establish a hierarchy of classification tasks, so that volunteers look for the most common types of object first, before moving on to rarer objects. This will allow a cycle of human and machine classification to rapidly search through large and diverse datasets, and critically will allow us to search for categories of interest that develop during the classification process.

For this demonstration project, we will run an example which makes use of all of these features in a real astronomical survey. This will allow us to demonstrate and measure the efficiencies achieved by these improvements, as well as producing valuable science in its own right. As the Zooniverse platform already supports projects across many disciplines, these tools will be made available for use by a wide range of scientists and researchers, accelerating their progress and making the time invested by more than 1.3 million volunteers more useful.</gtr:abstractText><gtr:potentialImpactText>Citizen science through the Zooniverse has already proven to be a transformational way of engaging the public in science. Since the projects' beginnings in 2007, more than a million people have used our platform to make a real contribution to science. Participants have explored galaxies, worked with ecologists in Antartica and the Serengeti, and uncovered hidden texts in historical archives. By engaging with the researchers who are leading the projects they are participating in, these volunteers gain a real sense of ownership over the research process; our studies show that Zooniverse volunteers are overwhelmingly more likely to engage with scientific content after encountering our projects. This is especially important because research shows that, having begun, volunteers are equally likely to go on to substantial participation whatever their initial educational level; rather than finding an audience already excited about science, the Zooniverse is creating a new crowd of hungry participants in research.

Nor is this impact limited to the participants themselves. Our projects have featured in museums around the world, and play a regular starring role in the BBC's Stargazing Live series, which reaches an audience of millions. The projects are heavily engaged on social media, with one - Planet Hunters - being amongst the most visited science pages on Facebook. Our volunteers enjoy communicating with their friends and colleagues about their scientific adventures, making them powerful advocates for the scientific process. 

We have recently redeveloped our core platform to make it easier for people to build projects, and are already seeing the adoption of citizen science by new audiences. Partnerships with Cancer Research UK - who used our platform to build science-filled games, and with the Natural History Museum demonstrate the uses to which our software can be put. Collaboration with Microsoft Research, and with researchers at Google, inform our understanding of how participants behave in these projects, and how we can do better. Companies like Imperative Space are adapting Zooniverse's astronomy projects for use in the classroom, giving schools a taste of cutting-edge science, and our platform also supports school-led experiments at CERN. 

The platform can also be used for more than research. A recent partnership with an NGO, Rescue Global, and the Earth observation company Planet Labs allowed rescuers to quickly generate new maps of settlements in Nepal following the tragic earthquake there. The work contained in this proposal - which aims at efficient, rapid classification - will be key in enabling us to expand this disaster relief work for future crises. 

Zooniverse is a project whose primary goal is to aid science. However, uniquely, at its core is the need to engage a very large company of volunteers, and a methodology which allows for long-lasting and effective transformation in attitudes to science. It is effective science, and highly exciting public engagement.</gtr:potentialImpactText><gtr:fund><gtr:end>2017-09-30</gtr:end><gtr:funder url="http://gtr.rcuk.ac.uk:80/organisation/D7F4F462-0518-4784-908A-D12633C139B3"><gtr:id>D7F4F462-0518-4784-908A-D12633C139B3</gtr:id><gtr:name>STFC</gtr:name></gtr:funder><gtr:start>2016-10-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>114203</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>Francis Crick Institute</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Partnership with Crick Institute</gtr:description><gtr:id>B388EB1F-BF33-4608-BC8E-EC629A45A8FB</gtr:id><gtr:impact>Projects are currently in beta.</gtr:impact><gtr:partnerContribution>Crick are providing data and expertise on machine learning for a suite of such projects.</gtr:partnerContribution><gtr:piContribution>We are working on a combined human/machine classification scheme for generic high resolution microscopy data which will make use of ConSciCom's understanding of user communities around medical and clinical communities.</gtr:piContribution><gtr:sector>Charity/Non Profit</gtr:sector><gtr:start>2016-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>Talk: Hammersmith Apollo</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>D24E425B-CD1B-48A6-AA1D-229CA2B082D5</gtr:id><gtr:impact>Talk as part of event at Hammersmith Apollo on discovering the unexpected.</gtr:impact><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Twitter</gtr:description><gtr:form>Engagement focused website, blog or social media channel</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>72081CB9-E01B-4A7F-B74A-4CFD08EA6B8A</gtr:id><gtr:impact>Lintott's twitter feed covers items of interest in citizen science, and now has more than 25000 followers</gtr:impact><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>http://twitter.com/chrislintott</gtr:url><gtr:year>2017</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs><gtr:softwareAndTechnicalProductOutput><gtr:description>NERO is the software built on top of the Zooniverse API which provides task assignment and allocation for projects. It is released under an open source license.</gtr:description><gtr:id>0A7572B9-D10E-46DD-8E0D-97F5AAAAB41C</gtr:id><gtr:impact>The tool has already made possible our partnership with Stargazing Live</gtr:impact><gtr:title>NERO</gtr:title><gtr:type>Webtool/Application</gtr:type><gtr:url>https://github.com/zooniverse/nero</gtr:url><gtr:yearFirstProvided>2017</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput></gtr:softwareAndTechnicalProductOutputs><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/2961A3F9-5D82-4B65-82A0-8E939F33D0FC"><gtr:id>2961A3F9-5D82-4B65-82A0-8E939F33D0FC</gtr:id><gtr:title>Planet Hunters IX. KIC 8462852 - where's the flux?</gtr:title><gtr:parentPublicationTitle>Monthly Notices of the Royal Astronomical Society</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/560ce6bde4c08d0ddc4fa4f9b998f689"><gtr:id>560ce6bde4c08d0ddc4fa4f9b998f689</gtr:id><gtr:otherNames>Boyajian T. S.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/8E23D6A4-73A3-4A8D-8026-025EC85AE6FC"><gtr:id>8E23D6A4-73A3-4A8D-8026-025EC85AE6FC</gtr:id><gtr:title>Gravity Spy: Integrating Advanced LIGO Detector Characterization, Machine Learning, and Citizen Science</gtr:title><gtr:parentPublicationTitle>ArXiv e-prints</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/dceb752493c0b45c5c36d64f17fb979f"><gtr:id>dceb752493c0b45c5c36d64f17fb979f</gtr:id><gtr:otherNames>Zevin Michael</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/6045895F-845E-4C66-81E1-58F76A0052F0"><gtr:id>6045895F-845E-4C66-81E1-58F76A0052F0</gtr:id><gtr:title>Galaxy Zoo: morphological classifications for 120 000 galaxies in HST legacy imaging</gtr:title><gtr:parentPublicationTitle>Monthly Notices of the Royal Astronomical Society</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/e182afcbab0a37aaee9b68b8641a1684"><gtr:id>e182afcbab0a37aaee9b68b8641a1684</gtr:id><gtr:otherNames>Willett Kyle W.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/4BB0B80C-DB33-4EA1-A349-8DD1F3FF317E"><gtr:id>4BB0B80C-DB33-4EA1-A349-8DD1F3FF317E</gtr:id><gtr:title>Galaxy Zoo: quantitative visual morphological classifications for 48 000 galaxies from CANDELS</gtr:title><gtr:parentPublicationTitle>Monthly Notices of the Royal Astronomical Society</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/c0ced7bbf6caadf3c4c823f4910b3158"><gtr:id>c0ced7bbf6caadf3c4c823f4910b3158</gtr:id><gtr:otherNames>Simmons B. D.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">ST/N003179/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>615BB43C-305A-47B9-B728-7A9BB0BBE1A4</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Astronomy - observation</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>C5529360-827A-4C08-881B-88F10B12ADDD</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Data Handling &amp; Storage</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>