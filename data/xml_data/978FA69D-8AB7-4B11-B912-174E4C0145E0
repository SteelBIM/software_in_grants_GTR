<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.rcuk.ac.uk/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.rcuk.ac.uk:80/organisation/D64641C7-D9A6-4B41-9C8F-03F7396CB8DA"><gtr:id>D64641C7-D9A6-4B41-9C8F-03F7396CB8DA</gtr:id><gtr:name>University of Lincoln</gtr:name><gtr:address><gtr:line1>Brayford Pool</gtr:line1><gtr:postCode>LN6 7TS</gtr:postCode><gtr:region>East Midlands</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.rcuk.ac.uk:80/organisation/936D002F-A8D1-4A93-AE5D-825ED0903D8D"><gtr:id>936D002F-A8D1-4A93-AE5D-825ED0903D8D</gtr:id><gtr:name>University of Nottingham</gtr:name><gtr:department>School of Computer Science</gtr:department><gtr:address><gtr:line1>University Park</gtr:line1><gtr:line4>Nottingham</gtr:line4><gtr:line5>Nottinghamshire</gtr:line5><gtr:postCode>NG7 2RD</gtr:postCode><gtr:region>East Midlands</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/936D002F-A8D1-4A93-AE5D-825ED0903D8D"><gtr:id>936D002F-A8D1-4A93-AE5D-825ED0903D8D</gtr:id><gtr:name>University of Nottingham</gtr:name><gtr:address><gtr:line1>University Park</gtr:line1><gtr:line4>Nottingham</gtr:line4><gtr:line5>Nottinghamshire</gtr:line5><gtr:postCode>NG7 2RD</gtr:postCode><gtr:region>East Midlands</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/D64641C7-D9A6-4B41-9C8F-03F7396CB8DA"><gtr:id>D64641C7-D9A6-4B41-9C8F-03F7396CB8DA</gtr:id><gtr:name>University of Lincoln</gtr:name><gtr:address><gtr:line1>Brayford Pool</gtr:line1><gtr:postCode>LN6 7TS</gtr:postCode><gtr:region>East Midlands</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.rcuk.ac.uk:80/person/A909F95F-7867-4C3A-B73A-ADC22A5CC579"><gtr:id>A909F95F-7867-4C3A-B73A-ADC22A5CC579</gtr:id><gtr:firstName>GEORGIOS</gtr:firstName><gtr:surname>TZIMIROPOULOS</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.rcuk.ac.uk:80/projects?ref=EP%2FM02153X%2F1"><gtr:id>978FA69D-8AB7-4B11-B912-174E4C0145E0</gtr:id><gtr:title>Facial Deformable Models of Animals</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/M02153X/1</gtr:grantReference><gtr:abstractText>Although the automatic monitoring of animals and their behaviour is of great importance to the field of animal health and welfare, developing computational tools for this purpose has received little attention by the scientific community. Aside the emotional value that they may have to people, animals are also important to the society and the economy, and developing such tools will be a big, transformative step with direct impact on all these areas. Towards this end, F.D.M.A. will develop novel tools for detecting and tracking animal facial behaviour, and in particular, for learning and fitting facial deformable models of animals to unconstrained images/video. Although algorithms for detecting and tracking of human faces have been recently shown capable of coping to some extent with unseen variations (e.g. pose, expression, illumination, background and occlusion), there is much more variability in the face of animals that the current solutions have not yet addressed. F.D.M.A. sets out to challenge the current state-of-the-art methods in face alignment and tracking, and develop learning and fitting algorithms that can deal with very large shape and appearance variations, typically encountered in animal faces. To the best of our knowledge, this problem has never been explored in the past by the Computer Vision community. It is significantly more difficult and different than prior work on human faces, as animal faces exhibit a much larger degree of variability in shape and appearance as well as in pose and expression.

The tools to be developed by F.D.M.A. will enable the automatic analysis and understanding of animal facial behaviour which is of growing importance to animal health and welfare. The potential benefits of enhanced animal health and welfare are great; for animals, their owners, society, public health and the economy. Cats and dogs, the two species chosen by F.D.M.A., are the most popular companion animals, worldwide and of enormous societal and economic importance. To the best of our knowledge, there is no prior work in computer vision on detecting and tracking the facial deformable shape and motion of animals in images and videos. F.D.M.A. sets out to develop such tools that will enable automatic facial animal behaviour understanding. Aside animal health and welfare, the Computer Vision tools to be developed by F.D.M.A. can be used to facilitate research in other scientific disciplines, such as Animal Behaviour, Vision and Robotics.</gtr:abstractText><gtr:potentialImpactText>The proposed research finds application in promoting animal welfare and health and opens up new directions in basic research for example in animal behaviour, vision, and robotics.

Animal health and welfare. Changes in behaviour are often an early sign of potential welfare problems and it has been recognised since the time of Darwin that the face is a focus for the expression of emotions in animals and man. Thus the face provides a single potential focus for identifying a range of states of concern, such as pain, fear, frustration etc. However, a major challenge to exploiting the potential of focusing on the face is the ability to address the challenges outlined in this proposal. If F.D.M.A. can be developed then this opens up an entirely new way to assess animal welfare by computers.

Cats and dogs. F.D.M.A. will focus on dog and cats that are arguably the most popular companion animals in the UK and worldwide. Aside the emotional value that companion animals have to people, there is also an associated &amp;pound; multi-Billion pet industry.

Animal behaviour. Developing computer vision tools for detecting and tracking animal facial behaviour opens up tremendous potential to objectively measure indicators of animal emotion that before now have been subject to great subjectivity. As a proof of concept, the tools to be developed by F.D.M.A. will be applied to the problem of detecting pain in cats from videos.

Vision. A common practice in research on the primate visual system is to have the head of the animal (e.g. macaques) fixated to make it focus on the stimuli and allow for recording. This setting causes discomfort to the animal. F.D.M.A. aims to develop tools for the automatic detection and tracking of animal facial behaviour (including gaze), and hence will assist research on primate vision. Dissemination to the Vision community will be achieved through our link with the Pack Lab, a world-leading group on Visual Neurophysiology at McGill University

Robotics. A significnt portion of robotics research has concentrated on human-robot interaction and, in particular, on developing robots capable of understanding human behaviour. However, little research has been conducted on how the robot can interact with animals in domestic and working environments as well as collaborate with them to assist their owners (e.g. elderly, disabled people). Dissemination to the Robotics community will be achieved through the Lincoln Centre for Autonomous Systems, and in particular through contacts provided by Dr. Marc Hanheide and Dr. Nicola Bellotto who are experts on human-robot interaction.

Commercial exploitation. We believe that the technology developed in this project has high potential for commercialization. There are a number of face detection and tracking technologies that have recently found their ways to the market, many of them developed here in the UK who is world-leader in computer vision research. (e.g. Image Metrics, Vision Metric, Aurora, OmiPerception). However, to the best of our knowledge, there is no company that offers face detection and tracking technologies for animal faces. With this in mind, we will exploit opportunities for commercialisation of the developed technology. To ensure the potential for commercial exploitation, we will protect the developed IP where appropriate. To this end, we will seek advice from Enterprise@Lincoln.</gtr:potentialImpactText><gtr:fund><gtr:end>2016-12-13</gtr:end><gtr:funder url="http://gtr.rcuk.ac.uk:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2015-09-14</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>98600</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>University of Lincoln</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:department>School of Life Sciences</gtr:department><gtr:description>Prof. Daniel Mills</gtr:description><gtr:id>82123E6B-9F6B-4E99-BDCA-94CB5A3BC6CC</gtr:id><gtr:impact>Work in progress; no outputs to report yet. 

This is a multi disciplinary collaboration: Prof. Mills is an expert on Animal Behaviour and I work on Computer Vision.</gtr:impact><gtr:partnerContribution>Prof. Mills is an expert on Animal Behaviour and an advisor for EP/M02153X/1 &amp;quot;Facial Deformable Models on Animals&amp;quot;.</gtr:partnerContribution><gtr:piContribution>I collaborate with Prof. Mills on UCCProject, which is related to EP/M02153X/1 &amp;quot;Facial Deformable Models on Animals&amp;quot;.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2014-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>55000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>UCPPROJECT</gtr:description><gtr:end>2016-09-02</gtr:end><gtr:fundingOrg>Feline Friends</gtr:fundingOrg><gtr:id>6821EC4C-3260-4E8B-8242-FD4B97AC556F</gtr:id><gtr:sector>Charity/Non Profit</gtr:sector><gtr:start>2015-10-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>Face alignment is the process of localising a set of facial landmarks such as the tip of the nose, the corner of the eyes, the pupils etc. Face alignment is a key component for facial recognition technologies such as face identification, facial expression analysis (e.g. happy, surprise) and facial behaviour analysis (e.g. pain). While a large amount of work has been devoted to human face alignment, there has been very little work on animal face alignment, a problem which is significantly more challenging due to the large appearance variations of animal faces (as opposed to human faces). 

In this project we explored a number of techniques for face alignment under large appearance variations, and identified Deep Neural Networks as the most prominent and impactful approach for solving the problem. Our recent work proposed algorithms which are now considered the state-of-the-art in face alignment, winning also the first prize in the 3D Face Alignment in the Wild Challenge.</gtr:description><gtr:exploitationPathways>We have been collaborating with Prof. Daniel Mills to apply this technology for the detection of visible signs of feline pain. Also, our results and techniques are general enough to be applied to other domains and structured objects beyond faces: our methods have produced state-of-the-art results for the problem of localising the joints of the human body. We will be looking opportunities to apply our methods to the medical domain, too.</gtr:exploitationPathways><gtr:id>B7C15483-3E9E-43EC-9630-679FF090FF04</gtr:id><gtr:sectors><gtr:sector>Creative Economy,Digital/Communication/Information Technologies (including Software),Healthcare</gtr:sector></gtr:sectors><gtr:url>https://fdmaproject.wordpress.com</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/A2478F99-4E67-422F-AC8A-19F927CB38BF"><gtr:id>A2478F99-4E67-422F-AC8A-19F927CB38BF</gtr:id><gtr:title>Convolutional aggregation of local evidence for large pose face alignment</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/719e4ac90d80f76fc63f42ab7c872808"><gtr:id>719e4ac90d80f76fc63f42ab7c872808</gtr:id><gtr:otherNames>Adrian Bulat</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/FB73C670-D5F7-444F-88C5-799EE0A154B5"><gtr:id>FB73C670-D5F7-444F-88C5-799EE0A154B5</gtr:id><gtr:title>Fast Algorithms for Fitting Active Appearance Models to Unconstrained Images</gtr:title><gtr:parentPublicationTitle>International Journal of Computer Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/e677ef7dfabf32bb2e2bff02fb394403"><gtr:id>e677ef7dfabf32bb2e2bff02fb394403</gtr:id><gtr:otherNames>Tzimiropoulos G</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/356B1CA1-AE28-488E-B696-2D9501B54438"><gtr:id>356B1CA1-AE28-488E-B696-2D9501B54438</gtr:id><gtr:title>The First Facial Landmark Tracking in-the-Wild Challenge: Benchmark and Results</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/4450bf4bff722ea337dcea38ddf8f00c"><gtr:id>4450bf4bff722ea337dcea38ddf8f00c</gtr:id><gtr:otherNames>J. Shen</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/M02153X/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>