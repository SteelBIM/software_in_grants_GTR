<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.rcuk.ac.uk/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.rcuk.ac.uk:80/organisation/D1774113-D5D2-4B7C-A412-66A90FE4B96F"><gtr:id>D1774113-D5D2-4B7C-A412-66A90FE4B96F</gtr:id><gtr:name>University of Cambridge</gtr:name><gtr:department>Engineering</gtr:department><gtr:address><gtr:line1>Lensfield Road</gtr:line1><gtr:line4>Cambridge</gtr:line4><gtr:postCode>CB2 1EW</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/D1774113-D5D2-4B7C-A412-66A90FE4B96F"><gtr:id>D1774113-D5D2-4B7C-A412-66A90FE4B96F</gtr:id><gtr:name>University of Cambridge</gtr:name><gtr:address><gtr:line1>Lensfield Road</gtr:line1><gtr:line4>Cambridge</gtr:line4><gtr:postCode>CB2 1EW</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/D96CFA43-6AF9-4CBE-BF4F-B3FA159758CC"><gtr:id>D96CFA43-6AF9-4CBE-BF4F-B3FA159758CC</gtr:id><gtr:name>Addenbrooke's Hospital NHS Trust</gtr:name><gtr:address><gtr:line1>Addenbrooke's Hospital NHS Trust</gtr:line1><gtr:line2>Hills Road</gtr:line2><gtr:line4>Cambridge</gtr:line4><gtr:postCode>CB2 0QQ</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/5195F775-F0E4-443A-AE02-3E6B51BB33A7"><gtr:id>5195F775-F0E4-443A-AE02-3E6B51BB33A7</gtr:id><gtr:name>Phonak Hearing Systems</gtr:name><gtr:address><gtr:line1>Laubisrutistrasse 28</gtr:line1><gtr:line4>8712 Stafa</gtr:line4><gtr:region>Outside UK</gtr:region><gtr:country>Switzerland</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/8B182D77-0118-491D-8CAD-A591AFB626C7"><gtr:id>8B182D77-0118-491D-8CAD-A591AFB626C7</gtr:id><gtr:name>Chear</gtr:name><gtr:address><gtr:line1>30 Fowlmere Road</gtr:line1><gtr:line2>Shepreth</gtr:line2><gtr:postCode>SG8 6QS</gtr:postCode><gtr:region>Unknown</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/C514A3A9-E140-4888-AFB2-5814D127FD79"><gtr:id>C514A3A9-E140-4888-AFB2-5814D127FD79</gtr:id><gtr:name>MRC Centre Cambridge</gtr:name><gtr:address><gtr:line1>Hills Road</gtr:line1><gtr:line4>Cambridge</gtr:line4><gtr:postCode>CB2 0QH</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.rcuk.ac.uk:80/person/465A95CE-B9CC-4267-AA3E-CB35040AFDDD"><gtr:id>465A95CE-B9CC-4267-AA3E-CB35040AFDDD</gtr:id><gtr:firstName>Richard</gtr:firstName><gtr:otherNames>Eric</gtr:otherNames><gtr:surname>Turner</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.rcuk.ac.uk:80/person/60912449-F348-4256-B78A-66FF3060DCD6"><gtr:id>60912449-F348-4256-B78A-66FF3060DCD6</gtr:id><gtr:firstName>Brian</gtr:firstName><gtr:otherNames>Cecil</gtr:otherNames><gtr:surname>Moore</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.rcuk.ac.uk:80/projects?ref=EP%2FM026957%2F1"><gtr:id>7FFE4E8F-BE2D-4FAB-9D67-92DD3CA2CCD2</gtr:id><gtr:title>Machine Learning for Hearing Aids: Intelligent Processing and Fitting</gtr:title><gtr:status>Active</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/M026957/1</gtr:grantReference><gtr:abstractText>Current hearing aids suffer from two major limitations:

1) hearing aid audio processing strategies are inflexible and do not adapt sufficiently to the listening environment,
2) hearing tests and hearing aid fitting procedures do not allow reliable diagnosis of the underlying nature of the hearing loss and frequently lead to poor fitting of devices.

This research programme will use new machine learning methods to revolutionise both of these aspects of hearing aid technology, leading to intelligent hearing devices and testing procedures which actively learn about a patient's hearing loss enabling more personalised fitting. 

Intelligent audio processing

The optimal audio processing strategy for a hearing aid depends on the acoustic environment. A conversation held in a quiet office, for example, should be processed in a different way from one held in a busy reverberant restaurant. Current high-end hearing aids do switch between a small number of different processing strategies based upon a simple acoustic environment classification system that monitors simple aspects of the incoming audio. However, the classification accuracy is limited, which is one of the reasons why hearing devices perform very poorly in noisy multi-source environments. Future intelligent devices should be able to recognise a far larger and more diverse set of audio environments, possibly using wireless communication with a smart phone. Moreover, the hearing aid should use this information to inform the way the sound is processed in the hearing aid. The purpose of the first arm of the project is to develop algorithms that will facilitate the development of such devices.

One of the focuses will be on a class of sounds called audio textures, which are richly structured, but temporally homogeneous signals. Examples include: diners babbling at a restaurant; a train rattling along a track; wind howling through the trees; water running from a tap. Audio textures are often indicative of the environment and they therefore carry valuable information about the scene that could be harnessed by a hearing aid. Moreover, textures often corrupt target signals and their suppression can help the hearing impaired. We will develop efficient texture recognition systems that can identify the noises present in an environment. Then we will design and test bespoke real-time noise reduction strategies that utilise information about the audio textures present in the environment.


Intelligent hearing devices

Sensorineural hearing loss can be associated with many underlying causes. Within the cochlea there may be dysfunction of the inner hair cells (IHCs) or outer hair cells (OHCs), metabolic disturbance, and structural abnormalities. Ideally, audiologists should fit a patient's hearing aid based on detailed knowledge of the underlying cause of the hearing loss, since this determines the optimal device settings or whether to proceed with the intervention at. Unfortunately, the hearing test employed in current fitting procedures, called the audiogram, is not able to reliably distinguish between many different forms of hearing loss. 

More sophisticated hearing tests are needed, but it has proven hard to design them. In the second arm of the project we propose a different approach that refines a model of the patient's hearing loss after each stage of the test and uses this to automatically design and select stimuli for the next stage that are particularly informative. These tests will be be fast, accurate and capable of determining the form of the patient's specific underlying dysfunction. The model of a patient's hearing loss will then be used to setup hearing devices in an optimal way, using a mixture of computer simulation and listening test.</gtr:abstractText><gtr:potentialImpactText>Hearing aid users will be a major beneficiary of the new technologies developed in this project. The impact on this user-group will be mediated through impacts on hearing aid manufactures (who will incorporate the new intelligent audio processing technologies into their devices) and audiologists (who will adopt the new intelligent listening tests).

Industrial and Societal Impact

Hearing aid manufacturers are one of the key beneficiaries for the intelligent audio processing technologies developed in the first arm of the project. We anticipate that the audio processing strategies we develop will translate to improved performance of these devices in noisy environments, currently a major limitation. Our project partner Dr.~Stefan Launer, Senior Vice President in charge of research at Phonak, will identify possible opportunities for commercialisation of the new techniques. In addition, Co-PI Prof. Brian Moore has strong links with other hearing aid companies including GNReSound, Starkey, and Oticon and he has a track record of commercialising academically developed technology.

Signal processing and machine learning are keystone technologies: the development of new techniques and methodologies triggers advances in a range of downstream application domains. The new methods developed in our proposal for audio recognition and noise removal have the potential to influence the broader field of machine hearing. For example, audio-recognition can be used to to tag sound tracks for audio- and video-search applications, whilst noise-suppression methods can be used for audio restoration for the digital industries. The PI has on going collaborations with industrial partners, including Google, and will consider potential applications of the methods developed in a broad context, not limited to hearing impairment.

Public Health and Societal Impact

Audiologists are a key user-group for the intelligent listening tests developed in the second arm of the project. We will make current tests faster and more accurate than current tests, freeing up the time of the audiologist. Importantly we will also develop new more powerful tests that will reveal the precise dysfunction that underlies a patient's hearing loss. Our project partner Dr.~David Baguley is Head of the Audiology Department and Cochlear Implant Centre at Addenbrookes Hospital and Honorary Professor of Audiology at Anglia Ruskin University. He will help ensure that the new hearing tests we develop will have a large impact on the field of audiology, both through his influence on clinical practice and his influence on teaching new audiologists.

This research grant is focused in improving hearing aid technology. However, cochlear implant technology and fitting suffers from a similar set of limitations as hearing aid technology and fitting. The advanced audio processing schemes, listening tests and fitting methods developed in the grant also have the potential to advance cochlear implant technology, and fitting. Project Partner Dr. Carlyon's role is to help identify and exploit these opportunities. Dr.Carlyon works closely with several cochlear implant companies including Cochlear and Advanced Bionics.</gtr:potentialImpactText><gtr:fund><gtr:end>2018-11-30</gtr:end><gtr:funder url="http://gtr.rcuk.ac.uk:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2015-12-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>565346</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>80000</gtr:amountPounds><gtr:country>United States of America</gtr:country><gtr:currCode>USD</gtr:currCode><gtr:currCountryCode>Ecuador</gtr:currCountryCode><gtr:currLang>es_EC</gtr:currLang><gtr:description>Google Research Focussed Grant</gtr:description><gtr:fundingOrg>Google</gtr:fundingOrg><gtr:id>2ED0B8CE-549E-4AB7-BB14-CB5633C88AC3</gtr:id><gtr:sector>Private</gtr:sector><gtr:start>2017-01-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>50000</gtr:amountPounds><gtr:country>United States of America</gtr:country><gtr:currCode>USD</gtr:currCode><gtr:currCountryCode>Ecuador</gtr:currCountryCode><gtr:currLang>es_EC</gtr:currLang><gtr:description>Facebook AI Partnership (GPU Server Gift)</gtr:description><gtr:fundingOrg>Facebook</gtr:fundingOrg><gtr:id>600A699D-E15D-44AF-9141-12A6AE508009</gtr:id><gtr:sector>Private</gtr:sector><gtr:start>2017-03-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>We have developed a suite of new intelligent hearing tests that we are currently testing on hearing impaired listeners. Preliminary experiments indicate that the new tests are much more efficient that those currently used by audiologists -- they take far fewer trials to achieve the same accuracy -- and they do not require manual intervention, being fully automated. The more sophisticated tests attempt to diagnose more complex forms of hearing impairment that current tests which will be useful when fitting hearing aids.</gtr:description><gtr:exploitationPathways>We plan to engage audiologists in order to establish how to get these tests into clinic.</gtr:exploitationPathways><gtr:id>8F8FA1D3-91DF-4F63-BA26-7BDD70EE9D12</gtr:id><gtr:sectors><gtr:sector>Healthcare</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/316B9D40-182E-47E2-8783-9705D1E62D95"><gtr:id>316B9D40-182E-47E2-8783-9705D1E62D95</gtr:id><gtr:title>Tonotopic representation of loudness in the human cortex.</gtr:title><gtr:parentPublicationTitle>Hearing research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/c62dbab583a3afa1e0b5e69815a4615e"><gtr:id>c62dbab583a3afa1e0b5e69815a4615e</gtr:id><gtr:otherNames>Thwaites A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>0378-5955</gtr:issn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/A3332852-3786-4390-8242-D01A3BE4F002"><gtr:id>A3332852-3786-4390-8242-D01A3BE4F002</gtr:id><gtr:title>Black-Box a-divergence minimization</gtr:title><gtr:parentPublicationTitle>33rd International Conference on Machine Learning, ICML 2016</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/66cc3297c3dea073795869f584f065ae"><gtr:id>66cc3297c3dea073795869f584f065ae</gtr:id><gtr:otherNames>Hern?ndez-Lobato J.M.</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/9D895512-B6C2-4578-B4C2-04F06E4284F9"><gtr:id>9D895512-B6C2-4578-B4C2-04F06E4284F9</gtr:id><gtr:title>The Multivariate Generalised von Mises Distribution: Inference and Applications</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/a8bc975367333bca90b1f969d8be308c"><gtr:id>a8bc975367333bca90b1f969d8be308c</gtr:id><gtr:otherNames>Navarro A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/77E001F0-D215-4CDE-B7B2-E87841B05387"><gtr:id>77E001F0-D215-4CDE-B7B2-E87841B05387</gtr:id><gtr:title>Perception of stochastic envelopes by normal-hearing and cochlear-implant listeners.</gtr:title><gtr:parentPublicationTitle>Hearing research</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/0c873d7815fe3b8929e3cff270f0ed89"><gtr:id>0c873d7815fe3b8929e3cff270f0ed89</gtr:id><gtr:otherNames>Gomersall PA</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>0378-5955</gtr:issn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/7E9BC9AF-44D1-4CAB-937E-567A1EA22197"><gtr:id>7E9BC9AF-44D1-4CAB-937E-567A1EA22197</gtr:id><gtr:title>R&amp;eacute;nyi Divergence Variational Inference</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/bd75dbe105014309cbdded985198a393"><gtr:id>bd75dbe105014309cbdded985198a393</gtr:id><gtr:otherNames>Li Y</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/7B30D277-F179-4BC6-A702-7A4545E5A02F"><gtr:id>7B30D277-F179-4BC6-A702-7A4545E5A02F</gtr:id><gtr:title>Effects of Sound-Induced Hearing Loss and Hearing Aids on the Perception of Music</gtr:title><gtr:parentPublicationTitle>Journal of the Audio Engineering Society</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/c3d6859227bbfd910e35f9d8fefcecdc"><gtr:id>c3d6859227bbfd910e35f9d8fefcecdc</gtr:id><gtr:otherNames>Moore B</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>15494950</gtr:issn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/5B831026-DDDD-4A56-9B5A-5B3327AD6546"><gtr:id>5B831026-DDDD-4A56-9B5A-5B3327AD6546</gtr:id><gtr:title>Discrimination of amplitude-modulation depth by subjects with normal and impaired hearing.</gtr:title><gtr:parentPublicationTitle>The Journal of the Acoustical Society of America</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/4efc41ec213d61f789aacf00dbeac420"><gtr:id>4efc41ec213d61f789aacf00dbeac420</gtr:id><gtr:otherNames>Schlittenlacher J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>0001-4966</gtr:issn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/CE5088F4-3F35-4B2E-A2FD-D9CE7C023BF9"><gtr:id>CE5088F4-3F35-4B2E-A2FD-D9CE7C023BF9</gtr:id><gtr:title>A Loudness Model for Time-Varying Sounds Incorporating Binaural Inhibition.</gtr:title><gtr:parentPublicationTitle>Trends in hearing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/ce59cdcf5804161d2d51e9977d85410d"><gtr:id>ce59cdcf5804161d2d51e9977d85410d</gtr:id><gtr:otherNames>Moore BC</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>2331-2165</gtr:issn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/CAD61C03-76BF-4A07-88F9-4A55352AA706"><gtr:id>CAD61C03-76BF-4A07-88F9-4A55352AA706</gtr:id><gtr:title>Q-PROP: SAMPLE-EFFICIENT POLICY GRADIENT WITH AN OFF-POLICY CRITIC</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/67d7cd2e85ac22147b965ce0d0147b7f"><gtr:id>67d7cd2e85ac22147b965ce0d0147b7f</gtr:id><gtr:otherNames>Gu S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/2CE37501-CECF-44EB-ADB0-B7A648F3EF95"><gtr:id>2CE37501-CECF-44EB-ADB0-B7A648F3EF95</gtr:id><gtr:title>Evaluation of a method for enhancing interaural level differences at low frequencies.</gtr:title><gtr:parentPublicationTitle>The Journal of the Acoustical Society of America</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/ce59cdcf5804161d2d51e9977d85410d"><gtr:id>ce59cdcf5804161d2d51e9977d85410d</gtr:id><gtr:otherNames>Moore BC</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>0001-4966</gtr:issn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/064AADF7-04C4-4EDE-81C6-0ECE662D27B9"><gtr:id>064AADF7-04C4-4EDE-81C6-0ECE662D27B9</gtr:id><gtr:title>Preferred Compression Speed for Speech and Music and Its Relationship to Sensitivity to Temporal Fine Structure.</gtr:title><gtr:parentPublicationTitle>Trends in hearing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/ce59cdcf5804161d2d51e9977d85410d"><gtr:id>ce59cdcf5804161d2d51e9977d85410d</gtr:id><gtr:otherNames>Moore BC</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>2331-2165</gtr:issn></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/M026957/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>12FC01EE-4952-4AE4-883A-D3E83A89C5C6</gtr:id><gtr:percentage>40</gtr:percentage><gtr:text>Digital Signal Processing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>089C8106-E4C8-4473-A5AB-F932AF4EC07C</gtr:id><gtr:percentage>40</gtr:percentage><gtr:text>Music &amp; Acoustic Technology</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>541CA340-B127-4309-84C6-51C40A48B4DA</gtr:id><gtr:percentage>20</gtr:percentage><gtr:text>Vision &amp; Senses - ICT appl.</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>