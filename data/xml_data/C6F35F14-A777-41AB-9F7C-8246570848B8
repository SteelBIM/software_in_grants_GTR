<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.rcuk.ac.uk/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.rcuk.ac.uk:80/organisation/A9B1866C-EE3E-4D81-A83C-118745E9433E"><gtr:id>A9B1866C-EE3E-4D81-A83C-118745E9433E</gtr:id><gtr:name>Goldsmiths College</gtr:name><gtr:department>Computing Department</gtr:department><gtr:address><gtr:line1>Lewisham Way</gtr:line1><gtr:line2>New Cross</gtr:line2><gtr:line4>London</gtr:line4><gtr:postCode>SE14 6NW</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/A9B1866C-EE3E-4D81-A83C-118745E9433E"><gtr:id>A9B1866C-EE3E-4D81-A83C-118745E9433E</gtr:id><gtr:name>Goldsmiths College</gtr:name><gtr:address><gtr:line1>Lewisham Way</gtr:line1><gtr:line2>New Cross</gtr:line2><gtr:line4>London</gtr:line4><gtr:postCode>SE14 6NW</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.rcuk.ac.uk:80/person/7BC9A903-9FEA-492B-9E8F-FF9096A5700C"><gtr:id>7BC9A903-9FEA-492B-9E8F-FF9096A5700C</gtr:id><gtr:firstName>Marco</gtr:firstName><gtr:surname>Gillies</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.rcuk.ac.uk:80/projects?ref=EP%2FH02977X%2F1"><gtr:id>C6F35F14-A777-41AB-9F7C-8246570848B8</gtr:id><gtr:title>Performance based expressive virtual characters</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/H02977X/1</gtr:grantReference><gtr:abstractText>Creating believable, expressive interactive characters is one of the great, and largely unsolved, technical challenges of interactive media. Human-like characters appear throughout interactive media, virtual worlds and games and are vital to the social and narrative aspects of these media, but they rarely have the psychological depth of expression found in other media. This proposal is for the development of research into a new approach to creating interactive characters which identifies the central problem of current methods as being the fact that creating the interactive behaviour, or Artificial Intelligence (AI), of a character is still primarily a programming task, and therefore in the hands of people with a technical rather than an artistic training. Our hypothesis is that the actors' artistic understanding of human behaviour will bring an individuality, subtlety and nuance to the character that it would be difficult to create in hand authored models. This will help interactive media represent more nuanced social interaction, thus broadening their range of application. The proposed research will use information from an actor's performance to determine the parameters of a character's behaviour software. We will use Motion Capture to record an actor interacting with another person. The recorded data will be used as input to a machine learning algorithm that will infer the parameters of a behavioural control model for the character. This model will then be used to control a real time animated character in interaction with a person. The interaction will be a full body interaction involving motion tracking of posture and/or gestures, and voice input.In entertainment this method will enable more social genres and help improve the current limited demographic. It will also enable a number of new applications in education, rehabilitation, media and marketing. Putting actors in charge of creating character AI will also make production pipelines more efficient be requiring less input from programmers This project is timely in that it brings together a number of active and developing research fields including expressive virtual characters, motion capture based animation and machine learning. It has the potential to transform current research in expressive virtual characters and present new research problems for machine learning and motion capture based animation. It is novel in that it proposes a fundamentally new approach to creating interactive characters and it combines disciplines such as animation, statistical machine learning, performance, affective computing, human computer interaction and psychology. The use of both machine learning and performance for virtual characters is particularly novel.</gtr:abstractText><gtr:potentialImpactText>Who will benefit? The primary beneficiaries outside of academia will be the creators of 3D interactive media, particularly computer games. This will also benefit the users of similar technology in specific areas such as education, psychotherapy and the arts. Academics in areas such as computer animation and virtual reality are also likely to benefit. How will they benefit? There are two major benefits of the proposed method: 1. Integrating artists more directly into the content pipeline for character AI, thus improving the efficiency of the production process 2. Creating more expressive and subtle behaviour of characters, thus increasing the range of genres and markets available to interactive media. What will be done to ensure they benefit? Our strategy will be 3 fold: 1. knowledge dissemination: primarily through academic journals and conferences, but also potentially directly through industry through training programmes (Gillies has participated in training schemes for Electronic Arts). 2. Further development: as the proposed research is still new, it is unlikely that the results will be ready for commercial exploitation by the end of this project. Therefore, if the results are positive, further funding will be sought to develop the proposed method in an academic research context 3. Direct exploitation: in the longer term, when the method is sufficiently developed, commercial exploitation will be sought. The most likely route will be through a partnership with an existing company working in games or interactive media via knowledge transfer or commercialization funding. Intermediary software may also be released to the community if this does not conflict with commercial exploitation.</gtr:potentialImpactText><gtr:fund><gtr:end>2012-03-01</gtr:end><gtr:funder url="http://gtr.rcuk.ac.uk:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2010-09-21</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>100635</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>Wired Article: Actors teach game characters the subtleties of body language</gtr:description><gtr:form>A magazine, newsletter or online publication</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>6750E7C2-0415-4EEB-B44E-DED48AD94E85</gtr:id><gtr:impact>An article was published in the technology magazine Wired following a press release and interview with Marco Gillies



http://www.wired.co.uk/news/archive/2012-08/15/goldsmiths-motion-behaviour.

Following a press release by Goldsmiths and the dissemination of video of the work, a number of articles were published online about the results of the research project. The most notable was in the popular technology site wired, which resulted from an interview given by Marco Gillies:



http://www.wired.co.uk/news/archive/2012-08/15/goldsmiths-motion-behaviour

This has enabled the research to reach a wide audience.</gtr:impact><gtr:partOfOfficialScheme>true</gtr:partOfOfficialScheme><gtr:primaryAudience>Media (as a channel to the public)</gtr:primaryAudience><gtr:url>http://www.wired.co.uk/news/archive/2012-08/15/goldsmiths-motion-behaviour</gtr:url><gtr:year>2012</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Body Language Based Gameplay</gtr:description><gtr:form>A magazine, newsletter or online publication</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>25057F99-0AB4-4785-B6D1-CBD070697797</gtr:id><gtr:impact>A film describing the final study of the project.

A short documentary film has been produced about the second study of the research project, which involved working with physical performer Emanuele Nargi. The video has attracted considerable attention, as of december 2012 it has had 1971 view on youtube. The video has been profiled in a number of popular technology sites including wired, develop (the UK game develop magazine) and Kurtweil Accelerating Intelligence. It has currently had 2,518 views (October 2014)

This has provided both a means to reach a wide audience (particularly mediated via articles in various online media) and also a way of easily communicating the research to other researchers and possible collaborators. For example, presenting part of the video at the recent London Virtual Social Interaction workshop (september 2014) has resulted two possible new collaborative projects.</gtr:impact><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>https://www.youtube.com/watch?v=2nqhSwhsWOs</gtr:url><gtr:year>2012</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>A variety of talks at Games industry events</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>Regional</gtr:geographicReach><gtr:id>38602380-5F73-4969-9BB8-022C7BED8F57</gtr:id><gtr:impact>I gave a number of talks at games and VR industry events which included research from the projects. They were at PlayHubs (Somerset House), Bossa Studios and the DevelopVR conference.</gtr:impact><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Industry/Business</gtr:primaryAudience><gtr:url>http://www.developvr.co.uk</gtr:url><gtr:year>2016</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>2500000</gtr:amountPounds><gtr:country>European Union (EU)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>PRAISE: Practice and peRformance Analysis Inspiring Social Education</gtr:description><gtr:end>2015-09-02</gtr:end><gtr:fundingOrg>European Commission (EC)</gtr:fundingOrg><gtr:id>97D2C1CC-B0A9-408D-BE90-D30A313FA49C</gtr:id><gtr:sector>Public</gtr:sector><gtr:start>2012-09-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>2500000</gtr:amountPounds><gtr:country>European Union (EU)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>PRAISE: Practice and peRformance Analysis Inspiring Social Education</gtr:description><gtr:end>2015-09-02</gtr:end><gtr:fundingOrg>European Commission (EC)</gtr:fundingOrg><gtr:id>DA85C086-1367-413B-B8E0-A9FB590787D7</gtr:id><gtr:sector>Public</gtr:sector><gtr:start>2012-09-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>21903</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Research project grant</gtr:description><gtr:end>2020-12-02</gtr:end><gtr:fundingOrg>The Leverhulme Trust</gtr:fundingOrg><gtr:id>FB5E832E-36FE-48F0-AA79-93EAD44896DE</gtr:id><gtr:sector>Academic/University</gtr:sector><gtr:start>2017-01-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>The findings of the research project provide the foundation for further research into interactive machine learning for virtual characters and has lead to several possible new research collabroations as well as high profile public engagement. 

Recently the advent of low cost VR such as the Oculus Rift have made the results of this project even more relevant. I an currently in the process of developing the software from the project into a platform usable by independent games developers via the unity game engine.</gtr:description><gtr:firstYearOfImpact>2012</gtr:firstYearOfImpact><gtr:id>8FAF78C7-3CAD-4D0F-8F70-DECF019599C8</gtr:id><gtr:impactTypes/><gtr:sector>Digital/Communication/Information Technologies (including Software)</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>This project has developed a new approach to designing video game characters that can respond to our body movements and body language. Rather than trying to program explicit rules for behavior, which would make it hard to capture the subtleties of body language, our software allows people to design movements directly by moving and interacting. 



We have developed two game environments in which people can customize characters' behaviour based on their own movements. 



The first is a 3D version of the classic video game Pong, in which players can control the paddles and their avatars with their own movements. They were able to customize their avatars responses to winning or loosing points based on their own movements. In our user tests, our participants found that performing the actions themselves helped them understand the game better and made it easier for them to design the avatar's responses.



The second examples is a 3D character that responds to a players body movements via the Microsoft Kinect motion tracking system. The character's responses are designed based on motion capture of a real interaction between people. Two people can play the roles of the video game character and the player, showing how the character should respond by acting out the movements themselves. This allows them to design movements in a natural way, by moving, rather than having to think about mathematical rules. The motion of both participants are recorded and synchronized. This data is then used as input to a machine learning algorithm which learns an algorithm for automatically controlling a video game character so that is responds in the same way as the people designing it. 



This style of design is particularly well suited to actors and performers who have a deep understanding of movement and body language. We did an in depth case study with physical theatre performer Emanuele Nargi, who used our software to design an interactive character based on his interactions with a number of members of the public.</gtr:description><gtr:exploitationPathways>The use of interactive machine learning could be integrated within Virtual Reality development tools such as Unity3D. I am currently working on a project to do so. This would provide better tools for SMEs to create interactive, social VR experiences, creating better experiences more cheaply.</gtr:exploitationPathways><gtr:id>6144443C-6AF1-4BC4-A1C9-85B27BBBB092</gtr:id><gtr:sectors><gtr:sector>Creative Economy,Digital/Communication/Information Technologies (including Software),Education</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs><gtr:softwareAndTechnicalProductOutput><gtr:description>A Hidden Markov Model Library for the Weka Machine Learning Toolkit</gtr:description><gtr:id>F4AE1CD8-F4F9-4F0E-9F8F-65DB820EFED0</gtr:id><gtr:title>HMMWeka</gtr:title><gtr:type>Software</gtr:type><gtr:url>http://doc.gold.ac.uk/~mas02mg/software/hmmweka/</gtr:url><gtr:yearFirstProvided>2011</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput><gtr:softwareAndTechnicalProductOutput><gtr:description>Gestyour us a plugin for the unity3d game engine that aims to make designing movement and gesture interfaces easier. It uses Interactive Machine Learning with a visualisation to support user in debugging their interfaces.</gtr:description><gtr:id>69BD2E7A-F0EA-4398-8A3A-4A7E61F737D9</gtr:id><gtr:title>Gestyour</gtr:title><gtr:type>Software</gtr:type><gtr:url>https://www.doc.gold.ac.uk/~mas02mg/gestyour/</gtr:url><gtr:yearFirstProvided>2014</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput></gtr:softwareAndTechnicalProductOutputs><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/787B5037-9014-4A4E-9BF4-8086F2B9DB58"><gtr:id>787B5037-9014-4A4E-9BF4-8086F2B9DB58</gtr:id><gtr:title>Embodied design of full bodied interaction with virtual humans</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/aabad2f727b0127fcf04debdf270c41a"><gtr:id>aabad2f727b0127fcf04debdf270c41a</gtr:id><gtr:otherNames>Gillies M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:isbn>9781450334570</gtr:isbn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/3EE0255C-7003-4E39-BED1-38BB161E6CA6"><gtr:id>3EE0255C-7003-4E39-BED1-38BB161E6CA6</gtr:id><gtr:title>Embodied Design of Dance Visualisations</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/1ad3438f3d268b4cf8ac9aa4e39eb8ab"><gtr:id>1ad3438f3d268b4cf8ac9aa4e39eb8ab</gtr:id><gtr:otherNames>Brenton H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:isbn>9781450328142</gtr:isbn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/AB2C1D09-7C3F-4FAA-B69F-A323893DA71C"><gtr:id>AB2C1D09-7C3F-4FAA-B69F-A323893DA71C</gtr:id><gtr:title>Customizing by doing for responsive video game characters</gtr:title><gtr:parentPublicationTitle>International Journal of Human-Computer Studies</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/00176f26b9cab2b267b0e81540f695fa"><gtr:id>00176f26b9cab2b267b0e81540f695fa</gtr:id><gtr:otherNames>Kleinsmith A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/EF696348-EEAC-4336-BAC9-AC159406DFB0"><gtr:id>EF696348-EEAC-4336-BAC9-AC159406DFB0</gtr:id><gtr:title>Applying the CASSM Framework to Improving End User Debugging of Interactive Machine Learning</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/aabad2f727b0127fcf04debdf270c41a"><gtr:id>aabad2f727b0127fcf04debdf270c41a</gtr:id><gtr:otherNames>Gillies M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:isbn>9781450333061</gtr:isbn></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/H02977X/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>41593421-CFAC-411D-94A7-E144022B0E6D</gtr:id><gtr:percentage>33</gtr:percentage><gtr:text>Artificial Intelligence</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>A704B681-6133-41A6-8D93-905FFEC6353B</gtr:id><gtr:percentage>34</gtr:percentage><gtr:text>Computer Graphics &amp; Visual.</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>0F8B7B13-F2F5-42B3-95C6-EF12D7877319</gtr:id><gtr:percentage>33</gtr:percentage><gtr:text>Multimedia</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>