<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.rcuk.ac.uk/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.rcuk.ac.uk:80/organisation/D5337A10-AC8A-402A-8164-C5F9CC6B0140"><gtr:id>D5337A10-AC8A-402A-8164-C5F9CC6B0140</gtr:id><gtr:name>Queen Mary, University of London</gtr:name><gtr:department>Sch of Electronic Eng &amp; Computer Science</gtr:department><gtr:address><gtr:line1>Mile End Road</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>E1 4NS</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/D5337A10-AC8A-402A-8164-C5F9CC6B0140"><gtr:id>D5337A10-AC8A-402A-8164-C5F9CC6B0140</gtr:id><gtr:name>Queen Mary, University of London</gtr:name><gtr:address><gtr:line1>Mile End Road</gtr:line1><gtr:line4>London</gtr:line4><gtr:postCode>E1 4NS</gtr:postCode><gtr:region>London</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.rcuk.ac.uk:80/person/DB8CF661-5A9D-4C40-8401-51C641BA965C"><gtr:id>DB8CF661-5A9D-4C40-8401-51C641BA965C</gtr:id><gtr:firstName>Andrea</gtr:firstName><gtr:surname>Cavallaro</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.rcuk.ac.uk:80/person/B64B1682-70DA-4488-89F2-1B7A0A412DE8"><gtr:id>B64B1682-70DA-4488-89F2-1B7A0A412DE8</gtr:id><gtr:firstName>Joshua</gtr:firstName><gtr:otherNames>Daniel</gtr:otherNames><gtr:surname>Reiss</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.rcuk.ac.uk:80/projects?ref=EP%2FK007491%2F1"><gtr:id>C4938E7F-CC9A-4B28-930C-2CEA7BC52825</gtr:id><gtr:title>Multisource audio-visual production from user-generated content</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/K007491/1</gtr:grantReference><gtr:abstractText>The pervasiveness of amateur media recorders either embedded in smartphones or as stand-alone devices is revolutionizing the way events are captured and reported. The aim of this project is to devise intelligent editing and production algorithms based on new signal processing techniques for processing multi-view user-generated content. 

The explosion of shared video content offers the opportunity for new ways of not only analysing but also timely reporting stories, ranging from disaster scenes and protests to music concerts and sports events. However, the large amount of data increasingly available and their varying quality makes the selection and editing of appropriate multimedia items in a timely manner very difficult thus strongly limiting the opportunity to harvest this data for security, cultural and entertainment applications. There is an urgent need to investigate and develop new ways to help or replace what used to be the role of a producer/director in this rapidly changing landscape. In particular, there is the need to automate production tasks and to generate new and high-quality content from multiple views. 

The key aspect of the project is the integration of audio and visual inputs that support each other in reaching objectives that would otherwise be impossible using only one modality. We will focus on a set of relevant event-types: sports, music shows and crowd scenes. We will devise novel multisource processing techniques to improve audiovisual production and to enable synchronisation processing. This will in turn allow generation of novel and higher quality audio-visual rendering of captured events.</gtr:abstractText><gtr:potentialImpactText>This project will have major impact on the use and quality of user-generated videos. It will lead to new, intelligent content production algorithms that enable user-generated content to be seamlessly and rapidly integrated into multimedia items. It also supports the development of video-based citizen journalism and other participatory media, where members of the public play an active role in the process of news reporting and community-based content creation. 

Dissemination will be through journal and conference publications as well as an up-to-date project website, which will also provide a service for uploading and producing content. We will build on existing links and engage with new beneficiaries and user groups by publishing the research at conferences (especially those with strong industry presence) and in high impact journals, visits to other groups and institutions.

Videos related to published results will also be disseminated through the Investigators' YouTube channels (that have already attracted 10,000 views to date) and websites. Our tools and accomplishments will be promoted to the multimedia signal processing community via press releases sent to mailing lists and magazines. Halfway through the project, we will host a one day Workshop on multisource signal processing, bringing together leaders in this emerging field. Near the end of the project, a Workshop on systems for intelligent production of user generated content will also be held, in which we will discuss and promote the outcomes of our research. 

To enhance engagement with the public and improve communication of project results, the Queen Mary EECS Public Engagement team will advise and mentor the research team to deliver high impact public engagement activities, assist with writing and editing articles for web and magazine, integrating messages from the project in events for schools and the general public and contribute towards distribution costs of magazines including project results. We will participate in communication activities by exhibiting prototypes developed in the project, contributing our technologies for use in creative projects (c4dmpresents.org) and disseminating results through the school magazines Audio! and CS4Fun (Computer Science for Fun).

The Investigators will lead Impact-related activities, although all researchers will be involved. We will use the services and expertise available from the QMUL Communications Officer for drafting press releases and promotional materials, and from the QM Innovation Marketing Manager for producing materials aimed at industry and user groups with a commercial interest. Descriptions of the project and its outcomes will be included in promotional material from the Investigators' research groups, the Computer Vision group and the Centre for Digital Music</gtr:potentialImpactText><gtr:fund><gtr:end>2016-01-31</gtr:end><gtr:funder url="http://gtr.rcuk.ac.uk:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2013-02-08</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>362895</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>Availability of prototype for users to upload their user-generated videos and to receive an automatically generated final cut - see https://gifu.eecs.qmul.ac.uk/index.php/groups</gtr:description><gtr:firstYearOfImpact>2015</gtr:firstYearOfImpact><gtr:id>ECE2F99D-C32E-4967-BFBC-EE7B0A246B2D</gtr:id><gtr:impactTypes><gtr:impactType>Cultural</gtr:impactType></gtr:impactTypes><gtr:sector>Digital/Communication/Information Technologies (including Software),Leisure Activities, including Sports, Recreation and Tourism,Culture, Heritage, Museums and Collections</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>The grant led to new models and tools for the synchronisation of audiovisual files captured by ad-hoc networks of sensors, such as smartphones; to new models and tools for the localisation of the sensors based on the collected data only; and to new models and tools for the automated processing and editing of the multi-viewpoint audiovisual recordings</gtr:description><gtr:exploitationPathways>A prototype system is available to user to automatically edit user-generated videos.</gtr:exploitationPathways><gtr:id>564661F6-FEAB-448F-B0CE-C060FB9DC6F1</gtr:id><gtr:sectors><gtr:sector>Creative Economy,Digital/Communication/Information Technologies (including Software),Culture, Heritage, Museums and Collections,Other</gtr:sector></gtr:sectors><gtr:url>http://www.eecs.qmul.ac.uk/~andrea/mavip</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/3EF63D60-B7F7-4ADF-9589-8564EBE91999"><gtr:id>3EF63D60-B7F7-4ADF-9589-8564EBE91999</gtr:id><gtr:title>Self-Localization of Ad-Hoc Arrays Using Time Difference of Arrivals</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Signal Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/c4fb28c7b7f759ed5da54d4f587d639a"><gtr:id>c4fb28c7b7f759ed5da54d4f587d639a</gtr:id><gtr:otherNames>Wang L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/CD9C4B23-4AF5-4BFF-BDF0-09934E9EDC63"><gtr:id>CD9C4B23-4AF5-4BFF-BDF0-09934E9EDC63</gtr:id><gtr:title>Multiview Matching of Articulated Objects</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Circuits and Systems for Video Technology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/a72c2c21bc7db70dc4a6a737e33a80e9"><gtr:id>a72c2c21bc7db70dc4a6a737e33a80e9</gtr:id><gtr:otherNames>Zini L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/6D9EC68F-1820-4597-AF65-D160251B8C79"><gtr:id>6D9EC68F-1820-4597-AF65-D160251B8C79</gtr:id><gtr:title>ViComp: composition of user-generated videos</gtr:title><gtr:parentPublicationTitle>Multimedia Tools and Applications</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/3f7e4c36e5e4e496cb8104517c97bb54"><gtr:id>3f7e4c36e5e4e496cb8104517c97bb54</gtr:id><gtr:otherNames>Bano S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/55745AD6-26E4-4777-9C70-A8559348BD07"><gtr:id>55745AD6-26E4-4777-9C70-A8559348BD07</gtr:id><gtr:title>An Iterative Approach to Source Counting and Localization Using Two Distant Microphones</gtr:title><gtr:parentPublicationTitle>IEEE/ACM Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/c4fb28c7b7f759ed5da54d4f587d639a"><gtr:id>c4fb28c7b7f759ed5da54d4f587d639a</gtr:id><gtr:otherNames>Wang L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/32E25195-16E1-4359-912B-07AD22A30289"><gtr:id>32E25195-16E1-4359-912B-07AD22A30289</gtr:id><gtr:title>Discovery and organization of multi-camera user-generated videos of the same event</gtr:title><gtr:parentPublicationTitle>Information Sciences</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/3f7e4c36e5e4e496cb8104517c97bb54"><gtr:id>3f7e4c36e5e4e496cb8104517c97bb54</gtr:id><gtr:otherNames>Bano S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/85329986-8ECB-4F3C-B446-3B75465F4276"><gtr:id>85329986-8ECB-4F3C-B446-3B75465F4276</gtr:id><gtr:title>Resource Allocation for Personalized Video Summarization</gtr:title><gtr:parentPublicationTitle>IEEE Transactions on Multimedia</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/0d6d9336d7d5bb8fef7cda202d6fb6c3"><gtr:id>0d6d9336d7d5bb8fef7cda202d6fb6c3</gtr:id><gtr:otherNames>Chen F</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/A1A612E3-2660-4322-AA8C-D7D6C55A1D09"><gtr:id>A1A612E3-2660-4322-AA8C-D7D6C55A1D09</gtr:id><gtr:title>Audio Fingerprinting for Multi-Device Self-Localization</gtr:title><gtr:parentPublicationTitle>IEEE/ACM Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/55ad40a41ab93b37466a402bfcefcb36"><gtr:id>55ad40a41ab93b37466a402bfcefcb36</gtr:id><gtr:otherNames>Hon T</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/A7B89E70-CB88-47BB-8863-3BB3B2DF989C"><gtr:id>A7B89E70-CB88-47BB-8863-3BB3B2DF989C</gtr:id><gtr:title>Over-Determined Source Separation and Localization Using Distributed Microphones</gtr:title><gtr:parentPublicationTitle>IEEE/ACM Transactions on Audio, Speech, and Language Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/c4fb28c7b7f759ed5da54d4f587d639a"><gtr:id>c4fb28c7b7f759ed5da54d4f587d639a</gtr:id><gtr:otherNames>Wang L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/407B6B4E-9C5E-481F-A9E2-3D5BE0B31E73"><gtr:id>407B6B4E-9C5E-481F-A9E2-3D5BE0B31E73</gtr:id><gtr:title>Audio-visual events for multi-camera synchronization</gtr:title><gtr:parentPublicationTitle>Multimedia Tools and Applications</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/3344d71b5e56e8331676367ff2a8daaf"><gtr:id>3344d71b5e56e8331676367ff2a8daaf</gtr:id><gtr:otherNames>Llagostera Casanovas A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/K007491/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>12FC01EE-4952-4AE4-883A-D3E83A89C5C6</gtr:id><gtr:percentage>40</gtr:percentage><gtr:text>Digital Signal Processing</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>60</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>