<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.rcuk.ac.uk/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.rcuk.ac.uk:80/organisation/88C5F7F9-8DCC-41C9-BC4F-F37DA01075C7"><gtr:id>88C5F7F9-8DCC-41C9-BC4F-F37DA01075C7</gtr:id><gtr:name>University of East Anglia</gtr:name><gtr:department>Computing Sciences</gtr:department><gtr:address><gtr:line1>Earlham Road</gtr:line1><gtr:line4>Norwich</gtr:line4><gtr:line5>Norfolk</gtr:line5><gtr:postCode>NR4 7TJ</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/88C5F7F9-8DCC-41C9-BC4F-F37DA01075C7"><gtr:id>88C5F7F9-8DCC-41C9-BC4F-F37DA01075C7</gtr:id><gtr:name>University of East Anglia</gtr:name><gtr:address><gtr:line1>Earlham Road</gtr:line1><gtr:line4>Norwich</gtr:line4><gtr:line5>Norfolk</gtr:line5><gtr:postCode>NR4 7TJ</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/FD66C955-5923-42A3-A9A8-8D66CA2E5942"><gtr:id>FD66C955-5923-42A3-A9A8-8D66CA2E5942</gtr:id><gtr:name>Home Office Sci Dev't Branch</gtr:name><gtr:address><gtr:line1>Home Office Sci Dev't Branch Langhurst</gtr:line1><gtr:line2>Langhurst House</gtr:line2><gtr:line3>Langhurstwood Road</gtr:line3><gtr:line4>Horsham</gtr:line4><gtr:line5>West Sussex</gtr:line5><gtr:postCode>RH12 4WX</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.rcuk.ac.uk:80/person/E0C4E21A-B228-4F3B-8A67-A6D43FC0C6A2"><gtr:id>E0C4E21A-B228-4F3B-8A67-A6D43FC0C6A2</gtr:id><gtr:firstName>Barry-John</gtr:firstName><gtr:surname>Theobald</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.rcuk.ac.uk:80/person/46117A48-1C74-4CC8-911D-1BEF2878020D"><gtr:id>46117A48-1C74-4CC8-911D-1BEF2878020D</gtr:id><gtr:firstName>Richard</gtr:firstName><gtr:surname>Harvey</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole><gtr:personRole url="http://gtr.rcuk.ac.uk:80/person/4A387D6B-E08F-4191-86CB-E960B8C48229"><gtr:id>4A387D6B-E08F-4191-86CB-E960B8C48229</gtr:id><gtr:firstName>Stephen</gtr:firstName><gtr:surname>Cox</gtr:surname><gtr:roles><gtr:role><gtr:name>CO_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.rcuk.ac.uk:80/projects?ref=EP%2FE028047%2F1"><gtr:id>83C84AC4-6753-4787-B94D-24C5F2E31849</gtr:id><gtr:title>LILiR2 - Language Independent Lip Reading</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/E028047/1</gtr:grantReference><gtr:abstractText>It is known that humans can, and do, lip-read but not much is known about exactly what visual information is needed for effective lip-reading, particularly in non-laboratory environments. This project will collect data for lip-reading and use it to build automatic lip-reading systems: machines that convert videos of lip-motions into text. To be effective such systems must accurately track the head over a variety of poses; extract numbers, or features, that describe the lips and then learn what features correspond to what text. To tackle the problem we will need to use information collected from audio speech. So this project will also investigate how to use the extensive information known about audio speech to recognise visual speech.The project is a collaboration between the University of East Anglia who have previously developed state-of-the-art speech reading systems; the University of Surrey who built accurate and reliable face and lip-trackers and the Home Office Scientific Branch who wish to investigate the feasibility of this approach for crime fighting.</gtr:abstractText><gtr:fund><gtr:end>2010-09-29</gtr:end><gtr:funder url="http://gtr.rcuk.ac.uk:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2007-05-31</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>391814</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>2580000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:department>Home Office</gtr:department><gtr:description>Home Office</gtr:description><gtr:end>2013-03-02</gtr:end><gtr:fundingOrg>Government of the UK</gtr:fundingOrg><gtr:id>ADC808D0-88AB-4B38-AC26-ADEAD3F86FFE</gtr:id><gtr:sector>Public</gtr:sector><gtr:start>2011-01-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>228000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:department>Home Office</gtr:department><gtr:description>Home Office</gtr:description><gtr:end>2013-03-02</gtr:end><gtr:fundingOrg>Government of the UK</gtr:fundingOrg><gtr:fundingRef>7020739</gtr:fundingRef><gtr:id>9B62CE89-AC16-410A-BF59-FF41FD3D7ABE</gtr:id><gtr:sector>Public</gtr:sector><gtr:start>2010-06-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>Our work is formed the basis for a number of public talks, press pieces and is currently under-going some proof-of-concept commercialisation.</gtr:description><gtr:firstYearOfImpact>2010</gtr:firstYearOfImpact><gtr:id>A057CED6-B37A-4E22-A4B8-1044C3F4D3C7</gtr:id><gtr:impactTypes><gtr:impactType>Cultural,Societal</gtr:impactType></gtr:impactTypes><gtr:sector>Aerospace, Defence and Marine,Security and Diplomacy</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>It is known that humans can, and do, lip-read but not much is known about exactly what visual information is needed for effective lip-reading. This project collected new datasets for lip-reading and used these to build automatic lip-reading systems: machines that convert videos of lip-motions into text. It also compared human performance against automatic performance on the same dataset. 



To be effective at automatic lip reading, systems must accurately track the head over a variety of poses; extract features, that describe the lips and then learn what features correspond to what text. To this end the project developed a state-of- the-art facial feature tracking system that could track any set of facial features on any person, in any environement in real- time. This tracking system has resulted in high quality international publications and interest from a variety of industrial sectors from government through to the post production industries. The project also developed several feature extraction/representations that could be used in the recognition of words and a general recognition framework for lip- reading. It made significant advances in person dependant recognition with accuracies approaching the level of speech recognition and it made significant new progress in the more challenging problem of person independent recognition i.e recognising people speaking who have never been seen by the system before. 



The project also developed approaches to language identification which allows the recognition of language to be performed just by the motion of the lips and the recognition of expression and non verbal communication, the subtle facial expressions that humans use intuitively to supplement the information provided by a speaker about subtle aspects of communication such as their interest in a topic of conversation.</gtr:description><gtr:exploitationPathways>Our findings have been used as input to at least four subsequent grants on lip-reading. 

After much subsequent work (not funded by EPSRC) we have moved to more robust tracking, off-axis lip-reading, and some evidence of person-independence (the key problem not tackled in this grant).</gtr:exploitationPathways><gtr:id>1198ECE0-C38C-43CC-ADD5-E0F6EC5F79EB</gtr:id><gtr:sectors><gtr:sector>Aerospace, Defence and Marine,Digital/Communication/Information Technologies (including Software),Healthcare,Government, Democracy and Justice,Retail,Security and Diplomacy</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/E231B897-EBF7-41BF-8F4F-74EC5A5EB49F"><gtr:id>E231B897-EBF7-41BF-8F4F-74EC5A5EB49F</gtr:id><gtr:title>Insights into machine lip reading</gtr:title><gtr:parentPublicationTitle>2012 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2012, Kyoto, Japan, March 25-30, 2012</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/8acdbd340400bc86f147347c47feedb0"><gtr:id>8acdbd340400bc86f147347c47feedb0</gtr:id><gtr:otherNames>Lan Y</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/91CDC9FD-ECE3-4553-8565-DBF9E9599A75"><gtr:id>91CDC9FD-ECE3-4553-8565-DBF9E9599A75</gtr:id><gtr:title>The challenge of multispeaker lip-reading</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/d87ca8b13584a58c99d59a74d733b041"><gtr:id>d87ca8b13584a58c99d59a74d733b041</gtr:id><gtr:otherNames>Barry-John Theobald</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2008-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/E839FA75-55AB-4EC3-8CF9-FDE25CF18AD4"><gtr:id>E839FA75-55AB-4EC3-8CF9-FDE25CF18AD4</gtr:id><gtr:title>View Independent Computer Lip-Reading</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/8acdbd340400bc86f147347c47feedb0"><gtr:id>8acdbd340400bc86f147347c47feedb0</gtr:id><gtr:otherNames>Lan Y</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>978-1-4673-1659-0</gtr:isbn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/5D70A73D-E562-4DE3-8E46-1930FB3D1628"><gtr:id>5D70A73D-E562-4DE3-8E46-1930FB3D1628</gtr:id><gtr:title>View Independent Computer Lip-Reading</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/8acdbd340400bc86f147347c47feedb0"><gtr:id>8acdbd340400bc86f147347c47feedb0</gtr:id><gtr:otherNames>Lan Y</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>978-1-4673-1659-0</gtr:isbn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/18A768BF-EA81-40F0-AF25-4163481EB239"><gtr:id>18A768BF-EA81-40F0-AF25-4163481EB239</gtr:id><gtr:title>Improving Lip-reading performance for robust audiovisual speech recognition using dynamic neural networks</gtr:title><gtr:parentPublicationTitle>Proceedings of the 1st Joint Conference on Facial Analysis, Animation and Auditory-Visual Speech Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/1ae4d389e225931ba4910982d1908b98"><gtr:id>1ae4d389e225931ba4910982d1908b98</gtr:id><gtr:otherNames>Thangthai K</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/4B772868-775F-4FF3-B97F-0F3905EBDDC3"><gtr:id>4B772868-775F-4FF3-B97F-0F3905EBDDC3</gtr:id><gtr:title>Finding phonemes: improving machine lip-reading</gtr:title><gtr:parentPublicationTitle>Proceedings of the 1st Joint Conference on Facial Analysis, Animation and Auditory-Visual Speech Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/39d7490e19504604a61a86743c1f73a3"><gtr:id>39d7490e19504604a61a86743c1f73a3</gtr:id><gtr:otherNames>Bear H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/C448DFD7-9C2B-41B8-A106-AA3E9128782D"><gtr:id>C448DFD7-9C2B-41B8-A106-AA3E9128782D</gtr:id><gtr:title>In Pursuit of Visemes</gtr:title><gtr:parentPublicationTitle>Proceedings on Auditory-Visual Speech Processing (AVSP)</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/a2a680f7660e59df3322c8c5043da158"><gtr:id>a2a680f7660e59df3322c8c5043da158</gtr:id><gtr:otherNames>Hilder S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/3F762E85-24C8-4D68-9E8A-B27590D8D385"><gtr:id>3F762E85-24C8-4D68-9E8A-B27590D8D385</gtr:id><gtr:title>Is automated conversion of video to text a reality?</gtr:title><gtr:parentPublicationTitle>Proceedings SPIE 8546, Optics and Photonics for Counterterrorism, Crime Fighting and Defence VIII</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/ce0d37f6c831118dcb2b42a83ae26a0a"><gtr:id>ce0d37f6c831118dcb2b42a83ae26a0a</gtr:id><gtr:otherNames>Bowden R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/948C92A9-2FC4-4EFD-8E48-516007F51D1E"><gtr:id>948C92A9-2FC4-4EFD-8E48-516007F51D1E</gtr:id><gtr:title>In pursuit of visemes</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/d87ca8b13584a58c99d59a74d733b041"><gtr:id>d87ca8b13584a58c99d59a74d733b041</gtr:id><gtr:otherNames>Barry-John Theobald</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/A2D3126F-6C83-44BD-8601-6695651D2DF4"><gtr:id>A2D3126F-6C83-44BD-8601-6695651D2DF4</gtr:id><gtr:title>Speaker independent visual-only language identification</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/dd7bf73c013c8ba3590cb756919497ab"><gtr:id>dd7bf73c013c8ba3590cb756919497ab</gtr:id><gtr:otherNames>Newman J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date><gtr:isbn>978-1-4244-4295-9</gtr:isbn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/798ED6B7-1C11-4053-8D89-E66D2003D8D4"><gtr:id>798ED6B7-1C11-4053-8D89-E66D2003D8D4</gtr:id><gtr:title>View Independent Computer Lip-Reading</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/8acdbd340400bc86f147347c47feedb0"><gtr:id>8acdbd340400bc86f147347c47feedb0</gtr:id><gtr:otherNames>Lan Y</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>978-1-4673-1659-0</gtr:isbn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/7D6D4DAA-96BE-425F-9F74-9AD836F33E18"><gtr:id>7D6D4DAA-96BE-425F-9F74-9AD836F33E18</gtr:id><gtr:title>Speaker-independent machine lip-reading with speaker-dependent viseme classifiers</gtr:title><gtr:parentPublicationTitle>Proceedings of the 1st Joint Conference on Facial Analysis, Animation and Auditory-Visual Speech Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/39d7490e19504604a61a86743c1f73a3"><gtr:id>39d7490e19504604a61a86743c1f73a3</gtr:id><gtr:otherNames>Bear H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/720370DB-91FF-4D30-A176-22C80D9ACA5B"><gtr:id>720370DB-91FF-4D30-A176-22C80D9ACA5B</gtr:id><gtr:title>Is automated conversion of video to text a reality?</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/ce0d37f6c831118dcb2b42a83ae26a0a"><gtr:id>ce0d37f6c831118dcb2b42a83ae26a0a</gtr:id><gtr:otherNames>Bowden R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/4F885A86-B832-4730-B427-A1BBE0C3BEAB"><gtr:id>4F885A86-B832-4730-B427-A1BBE0C3BEAB</gtr:id><gtr:title>Comparing visual features for lipreading</gtr:title><gtr:parentPublicationTitle>Proceesings International Conference on Auditory-Visual Speech Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/8acdbd340400bc86f147347c47feedb0"><gtr:id>8acdbd340400bc86f147347c47feedb0</gtr:id><gtr:otherNames>Lan Y</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/D9A808D8-EA53-403E-80C9-4B1608465B0D"><gtr:id>D9A808D8-EA53-403E-80C9-4B1608465B0D</gtr:id><gtr:title>Which phoneme-to-viseme maps best improve visual-only computer lip-reading?</gtr:title><gtr:parentPublicationTitle>Advances in Visual Computing: Proceedings 10th International Symposium, ISVC 2014</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/39d7490e19504604a61a86743c1f73a3"><gtr:id>39d7490e19504604a61a86743c1f73a3</gtr:id><gtr:otherNames>Bear H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/3D1EFD5D-4ED0-4BD2-A13C-BB5AD4FB24D9"><gtr:id>3D1EFD5D-4ED0-4BD2-A13C-BB5AD4FB24D9</gtr:id><gtr:title>Resolution limits on visual speech recognition</gtr:title><gtr:parentPublicationTitle>Proceedings of IEEE International Conference on Image Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/39d7490e19504604a61a86743c1f73a3"><gtr:id>39d7490e19504604a61a86743c1f73a3</gtr:id><gtr:otherNames>Bear H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/AB0FECC7-768D-4D72-A2AF-1689B30843D1"><gtr:id>AB0FECC7-768D-4D72-A2AF-1689B30843D1</gtr:id><gtr:title>Robust facial feature tracking using selected multi-resolution linear predictors</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/f1d770686de08b20027dea3bcaef3cb1"><gtr:id>f1d770686de08b20027dea3bcaef3cb1</gtr:id><gtr:otherNames>Ong E</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2009-01-01</gtr:date><gtr:isbn>978-1-4244-4420-5</gtr:isbn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/FBE80B07-AAAE-4DF4-9328-10A73A7FDF95"><gtr:id>FBE80B07-AAAE-4DF4-9328-10A73A7FDF95</gtr:id><gtr:title>Recent developments in automated lip-reading</gtr:title><gtr:parentPublicationTitle>Optics and Photonics for Counterterrorism and Crime Fighting IX</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/ce0d37f6c831118dcb2b42a83ae26a0a"><gtr:id>ce0d37f6c831118dcb2b42a83ae26a0a</gtr:id><gtr:otherNames>Bowden R</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/3521D3A2-9909-422A-8002-7B5141CBB178"><gtr:id>3521D3A2-9909-422A-8002-7B5141CBB178</gtr:id><gtr:title>Limitations of visual speech recognition</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/d87ca8b13584a58c99d59a74d733b041"><gtr:id>d87ca8b13584a58c99d59a74d733b041</gtr:id><gtr:otherNames>Barry-John Theobald</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/F76EC660-F65E-4835-A423-885971EA3732"><gtr:id>F76EC660-F65E-4835-A423-885971EA3732</gtr:id><gtr:title>Insights into machine lip reading</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/8acdbd340400bc86f147347c47feedb0"><gtr:id>8acdbd340400bc86f147347c47feedb0</gtr:id><gtr:otherNames>Lan Y</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>978-1-4673-0045-2</gtr:isbn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/986C633D-F97B-4574-A142-EA96F7E606C0"><gtr:id>986C633D-F97B-4574-A142-EA96F7E606C0</gtr:id><gtr:title>Improving visual features for lip-reading</gtr:title><gtr:parentPublicationTitle>Proceedings International Conference on Auditory-Visual Speech Processing</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/8acdbd340400bc86f147347c47feedb0"><gtr:id>8acdbd340400bc86f147347c47feedb0</gtr:id><gtr:otherNames>Lan Y</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2010-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/89B702B4-9AF2-4371-BE49-3272400697C1"><gtr:id>89B702B4-9AF2-4371-BE49-3272400697C1</gtr:id><gtr:title>Some observations on computer lip-reading: moving from the dream to the reality</gtr:title><gtr:parentPublicationTitle>SPIE Proceedings 9253a: Optics and Photonics for Counterterrorsim, Crime Fighting and Defence</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/39d7490e19504604a61a86743c1f73a3"><gtr:id>39d7490e19504604a61a86743c1f73a3</gtr:id><gtr:otherNames>Bear H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/E028047/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>