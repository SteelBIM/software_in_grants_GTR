<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.rcuk.ac.uk/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.rcuk.ac.uk:80/organisation/AE58F21F-3622-4382-97BB-1359BD183E9F"><gtr:id>AE58F21F-3622-4382-97BB-1359BD183E9F</gtr:id><gtr:name>University of Glasgow</gtr:name><gtr:department>School of Psychology</gtr:department><gtr:address><gtr:line1>University Avenue</gtr:line1><gtr:line4>Glasgow</gtr:line4><gtr:postCode>G12 8QQ</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/AE58F21F-3622-4382-97BB-1359BD183E9F"><gtr:id>AE58F21F-3622-4382-97BB-1359BD183E9F</gtr:id><gtr:name>University of Glasgow</gtr:name><gtr:address><gtr:line1>University Avenue</gtr:line1><gtr:line4>Glasgow</gtr:line4><gtr:postCode>G12 8QQ</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/9EC955A9-7A7E-434C-BBD8-CE3B521C2821"><gtr:id>9EC955A9-7A7E-434C-BBD8-CE3B521C2821</gtr:id><gtr:name>University of Alberta</gtr:name><gtr:address><gtr:line1>University of Alberta</gtr:line1><gtr:region>Outside UK</gtr:region><gtr:country>Canada</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/F25BC0C5-5284-441F-B79F-A61707946CA0"><gtr:id>F25BC0C5-5284-441F-B79F-A61707946CA0</gtr:id><gtr:name>University of California Los Angeles</gtr:name><gtr:address><gtr:line1>405 Hillgard Avenue</gtr:line1><gtr:line2>Box 951361</gtr:line2><gtr:postCode>90095</gtr:postCode><gtr:region>Outside UK</gtr:region><gtr:country>United States</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.rcuk.ac.uk:80/person/4C5F14F0-9C55-4D77-B895-C2238785E836"><gtr:id>4C5F14F0-9C55-4D77-B895-C2238785E836</gtr:id><gtr:firstName>Rachael</gtr:firstName><gtr:otherNames>Elizabeth</gtr:otherNames><gtr:surname>Jack</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.rcuk.ac.uk:80/projects?ref=ES%2FK001973%2F1"><gtr:id>6B100A32-62C1-417D-877E-01D6CD1937E5</gtr:id><gtr:title>Mapping the Cultural Landscape of Emotions for Social Interaction</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>ES/K001973/1</gtr:grantReference><gtr:abstractText>&lt;p>With rapid globalisation, cross-cultural communication is integral to modern society, with mutual understanding of emotions central to successful social interaction. This research examines the complexities of cross-cultural communication.&lt;/p>

&lt;p>Combining eye movements, computational modelling, and state-of-the-art 4D computer graphics, Dr Jack?s work (featuring in &lt;em>National Geographic&lt;/em>, Discovery Channel Magazine) refutes universality, highlighting knowledge gaps. The aim is to bridge these gaps during the award.&lt;br />&lt;br />&lt;strong>Which emotions are primary across cultures?&lt;/strong> Using semantic network reconstruction tools, the conceptual landscape of emotions and identify the primary emotions across cultures will be mapped.&lt;br />&lt;br />&lt;strong>Which facial movements signal culture-specific emotions?&lt;/strong> Using a unique 4-D facial animation platform, dynamic mental models of a spectrum of culture-specific facial expressions wil be constructed.&lt;br />&lt;br />&lt;strong>How accurate is cross-cultural emotion communication?&lt;/strong> Conducting previously impossible research with advanced stimuli, same- and other-culture facial expression recognition will&amp;nbsp; be examined. Eye-tracking will identify facial signals supporting accurate recognition and creating confusion.&lt;br />&lt;br />&lt;strong>Is there an in-group advantage?&lt;/strong> By interchanging race of face with culture-specific emotions (eg, Eastern emotion on a white face), Dr Jack will precisely examine the in-group advantage theory.&lt;br />&lt;br />Benefits. With broad implications, this work will bridge scientific knowledge gaps and make timely contributions to the rapidly evolving communication needs of society.&lt;/p></gtr:abstractText><gtr:potentialImpactText>With rapid globalisation and cultural integration, cross-cultural communication is crucial in modern society. Yet, consistent cultural differences recognising facial expressions refute notions of a universal language of emotion. My research aims to provide a comprehensive examination of cross-cultural emotion communication, with considerable impact for science and society.
 
SOCIAL PSYCHOLOGY. Knowledge of emotion is largely limited to a single, static set of facial expressions, restricted in emotional range and ecological validity (i.e., Western specific; naturalistic dynamic signals absent). By providing a spectrum of 4-D culture-specific facial expression models, I will advance knowledge of emotion communication beyond the limits of the universality hypothesis. My work will have significant impact on key theories of emotion communication, raise new questions, and guide future research directions. As high-grade stimuli, the 4-D models can also address outstanding questions, including key brain-imaging studies [13]. 

COGNITIVE NEUROSCIENCE/VISUAL COGNITION. By demonstrating how culture shapes thought (mental models, conceptual landscapes) and biological systems (eye movements), my data will advance knowledge of the functional relationship between high-level cognition and visual systems. Culture-specific information sampling and representations also likely alters neural signals, revealing further biological differences, impacting on brain-imaging communities.

CLINICAL NEUROSCIENCE. Abnormal emotion processing creates major impairments to social relations. Modelling mental representations of facial expressions would have great impact for assessment (i.e. precisely identifying the absence of facial signals) and rehabilitation (i.e., targeting deficits in emotion representation) in various groups at risk of social isolation (e.g. autism, depression, anxiety). To this aim, I will continue liaising with my collaborator, Ralph Adolphs (CalTech, USA) - a world leader in emotion and clinical groups. 

COMMUNICATION TECHNOLOGY. Companion robots are a major investment in European funding. Advancing knowledge of cultural emotion communication will have great impact for the development of communication avatars and companion robots (designed to recognise and express emotions) in different cultures, as reflected by the broad reach of my previous work in Computer Science [45] and Human-Robot Interaction [46].

INDUSTRIAL PARTNERSHIPS. The digital economy relies on a precise understanding of social signals for automated recognition and expression. While the 3-D-movie/animation industry relies on point-light displays, my 4-D models will have greater generalisation. With my mentor, Prof. Schyns, I am now liaising with Microsoft Research Labs (http://research.microsoft.com/en-us/labs/cambridge/), Dimensional Imaging, Scotland (http://www.di3d.com/index.php) and Vicarvision (SMRgroep http://www.smr.nl/), Netherlands.

MUSEUMS/GALLERIES. My work has featured in international public forums (e.g. National Geographic, Discovery Channel Magazine, BBC News Front Page). The curator of the Hunterian Museum and Gallery plans to create an exhibit focusing on historical, biological, anthropological, medical and cultural aspects of facial expressions, using my 4-D models of facial expressions as a centerpiece. Thus, my data will continue to make central contributions to public knowledge of cross-cultural emotion communication. 

SOCIETY. Cross-cultural interactions are common in local communities (e.g. schools, businesses, social services). Providing knowledge on cultural emotions will improve communication, fostering healthy relations across society. My results will make a timely contribution to the rapidly evolving communication needs of society, benefiting the economic performance of the UK.
Thus, my data will increase awareness of cultural differences, with the long-term benefits realised by applying knowledge during social interactions.</gtr:potentialImpactText><gtr:fund><gtr:end>2015-12-30</gtr:end><gtr:funder url="http://gtr.rcuk.ac.uk:80/organisation/924BE15C-91F2-4AAD-941A-3F338324B6AE"><gtr:id>924BE15C-91F2-4AAD-941A-3F338324B6AE</gtr:id><gtr:name>ESRC</gtr:name></gtr:funder><gtr:start>2012-12-31</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>163724</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>Glow (Scottish Schools National Intranet) 	</gtr:description><gtr:form>A broadcast e.g. TV/radio/film/podcast (other than news/press)</gtr:form><gtr:geographicReach>Regional</gtr:geographicReach><gtr:id>8FB59ABB-0DA7-4474-B48B-5ABB1947B8A0</gtr:id><gtr:impact>Broadcast to primary school children a demonstration of what I do as a scientist and how I do it. A Q&amp;amp;A followed.</gtr:impact><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Schools</gtr:primaryAudience><gtr:url>https://connect.glowscotland.org.uk/</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Royal Society Summer Science Exhibition</gtr:description><gtr:form>Participation in an activity, workshop or similar</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>B31D6A3E-F015-45AF-8C44-8E77EFEBEB4D</gtr:id><gtr:impact>The University of Glasgow &amp;amp; Dimensional Imaging Reveal the Hidden Information in Faces. 

The University of Glasgow's Institute of Neuroscience and Psychology were one of the key exhibitors at the highly prestigious Royal Society Summer Science Exhibition with their 'Face Facts' exhibit.

The Royal Society Summer Science Exhibition is an annual display of the most exciting cutting-edge science and technology in the UK. The week-long festival features 22 exhibits from the forefront of innovation. Further information about the event can be found at: http://sse.royalsociety.org/2015. Over 10,000 members of the public attended. 

Using a 3D facial imaging system from pioneering Scottish technology company Dimensional Imaging (DI4D?), the exhibit highlighted how computer graphics can be used to reveal the information hidden in faces. We scanned over 700 people's faces at the event: ranging from a 10 month old baby girl to a 100 year old woman. Once each face was scanned and processed, participants were able to interact with their own face in 3D on the touchscreen - by rotating the face to view different angles, or animating it with different expressions using facial muscle 'sliders'. 

Participants could also see the location of different face movements using specific colour maps. We also used our computer graphic techniques to make composite faces with the average shape and colour information of the women, men, girls and boys we scanned. Attendees were surprised to find out how our research on dynamic facial expressions has shown that cultures interpret facial expressions differently, whilst having fun at the same time. Attendees loved having their faces scanned in 3D. Participants could also upload their animations to the 'Face Facts' website enabling them to play around with their face and even create new ones. 218 people uploaded at least one video to the website and 476 in total were uploaded at the end of the event.</gtr:impact><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:url>https://royalsociety.org/science-events-and-lectures/summer-science-exhibition/</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>80000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>ESRC collaborative studentships</gtr:description><gtr:end>2021-09-02</gtr:end><gtr:fundingOrg>Economic and Social Research Council (ESRC)</gtr:fundingOrg><gtr:id>E459F82F-F4B0-4B38-9267-5B749C0E24E4</gtr:id><gtr:sector>Public</gtr:sector><gtr:start>2017-10-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>How have your findings been used? Please provide a brief summary.

SOCIETAL/PUBLIC INTEREST/IMPACT MEDIA COVERAGE. My work has received considerable public interest, as reflected by activity on Blogs, Twitter, Weibo, Facebook, Google+, Linkedln, and YouTube videos. For Jack et al., 2014 (Current Biology) see following link for activity in each domain: http://www.altmetric.com/details.php?citation_id=2016990. The color-coded key on the left shows the level of activity in each domain. Please also see the DEMOGRAPHICS tab, which shows that 73% of Twitter posts were made by the public, 20% by scientists, 5% practitioners. My work has also featured in a number of high profile media outlets (e.g., Time Magazine, Smithsonian Magazine, Royal Society Summer Science Exhibition) and various international news outlets (e.g., BBC, Associated Press, Press Trust of India). Jack et al., 2014 (Current Biology) was the best-performing story of the month, internationally. Below is a representative (by no means exhaustive) sample of media outlets where my work has featured. Interest from across the globe: BBC, Associated Press, Press Trust of India, Xinhua News Agency, NPR (USA), News Talk radio (Ireland), Las Ultimas Noticias (Chile), and National News London. Featured in: Time Magazine, Smithsonian Magazine, BBC Online, The Scotsman, Daily Mail, Daily Express, Metro, The Herald, Courier &amp;amp; Advertiser, Red Orbit, UPI, New Indian Express, The Atlantic, Yahoo (US), Geobeats, Education UK, People's Daily, Global Times, Nature World News, Times of India, New Straits Times, IBN Live, Biospace, Psychics Universe, Medical News Today, Brightsurf.com, Medical Xpress, UK Wired News, Kelowna Now, World News, WSB-TV (Atlanta), Dayton Daily News, News Channel 4 (Oklahoma), The Onion, EurekAlert, Business Standard. Together, the extensive international coverage of my work reflects the immediate public interest in my work and its value to society. 

MUSEUMS/GALLERIES. The curator of the Hunterian Museum and Gallery plans to create an exhibit focusing on historical, biological, anthropological, medical and cultural aspects of facial expressions, using my 4-D models of facial expressions as a centerpiece. Thus, my data will continue to make central contributions to public knowledge of cross-cultural emotion communication. 

ACADEMIC BENEFICIARIES As shown in &amp;quot;Score in Context&amp;quot; here http://www.altmetric.com/details.php?citation_id=2016990 Jack et al., (2014) is amongst the highest ever scored in the journal (IF: 10.227), ranked 6th for all other articles of the same age, 38th (out of 3613) in all articles in the journal, and is in the 99th percentile of articles in all journals. Mendeley's readership includes Psychology, Computer and Information Science, Biological Sciences, Medicine, and Humanities. My work has been cited in a broad range of academic peer-reviewed journals including brain imaging (e.g., Vecchiato et al., 2014), behavioural methods in psychology for visual (e.g. Naples et al., 2014) and auditory stimuli (e.g., Choi et al., 2014), Artificial Intelligence (e.g., Kotsakis et al., 2014, Kalliris et al., 2014), postgraduate theses in Engineering (e.g., Sandel, A 2014), undergraduate theses in Psychology (e.g., Sheppard, 2014) a book (communication skills for care workers - van Alphen, 2014) and featured in the BPS Readers Digest (July 2014). Based on the success of my work, I have developed a number of international and national collaborations to build dynamic models of facial expression signals in clinical groups such as Autism Spectrum Disorder with Prof. Ralph Adolphs (CalTech, USA) and Prof. Jim Tanaka (U. Victoria, Canada) - both of whom are world leaders in emotion processing in Autism Spectrum Disorder groups - high suicide risk with Prof. Rory O'Connor (U. Glasgow, UK), across different cultures (e.g., Japan - Dr. Michiko Koeda, Nippon Medical School; China - Prof. Hongmei Yan, U. Electronic Science and Technology of China; Philippines - Miss Erin Mercado, The Mind Museum; Mozambique - Prof. Jose-Miguel Fernandez-Dols, U. Madrid, Spain), other socially relevant face signals such as smile types (Prof. Paula Niedenthal - Wisconsin-Madison, USA), emotional intensity (Prof. Daniel Messinger - Miami, USA), circumplex model of emotions (Prof. Jim Russell - Boston, USA), pain and pleasure (Prof. Jose-Miguel Fernandez-Dols, U. Madrid, Spain) and gamers (Prof. Daphne Bavelier - Rochester, USA/Geneva). Together, these demonstrate the immediate impact my work has had on the academic community, by providing new opportunities to examine dynamic facial expression signals across a broad range of fields. My work has also featured as a Case Study in Dimensional Imaging 4D (http://www.di4d.com/case-glasgow-uni.html) demonstrating how we combine computer graphics with subjective perception and psychophysics to gain a competitive edge in advancing knowledge of dynamic social signals.

Our results are now being used to design virtual humans by my international collaborator, Prof. Stacy Marsella (http://www.ccs.neu.edu/people/faculty/member/marsella/) - a world leader in the design of virtual humans. I am also applying my dynamic facial expression models to my recently acquired FurHat robot head http://www.furhatrobotics.com/ to design social robots of the future that can display realistic behaviours that reliably impact human user social judgments. 

We showcased our results and their many applications at the prestigious Royal Society Summer Science Exhibition 2015, which allowed me to disseminate my results to the public, and engage their interests in psychological research.</gtr:description><gtr:firstYearOfImpact>2014</gtr:firstYearOfImpact><gtr:id>0115C9A5-6F31-476A-9EA4-FA8B2D60F364</gtr:id><gtr:impactTypes><gtr:impactType>Cultural,Societal</gtr:impactType></gtr:impactTypes><gtr:sector>Digital/Communication/Information Technologies (including Software),Leisure Activities, including Sports, Recreation and Tourism,Culture, Heritage, Museums and Collections</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>Facial expressions are widely considered &amp;quot;the universal language of emotion.&amp;quot; Yet, many cultures misinterpret basic facial expressions, questioning their validity as universal signals. Here, we aimed to understand how different cultures signal basic emotions. 

First, using a state-of-the-art Generative Face Grammar, I reconstructed 3-D dynamic models of the 6 basic facial expressions in two cultures, to reveal clear cultural specificity in both facial expression signals and the conceptual organisation of basic emotions. My data raised new questions: how are emotions conceptually organised by different emotions? How do cultures represent these emotions as facial expressions? 

To address these questions, I used semantic network reconstruction tools to map the map the conceptual organization of a broad spectrum of emotions in two distinct cultures (Western and East Asian). I then used clustering methods to identify culture-specific conceptual groupings of emotions. For each emotion word in each culture, I then modelled and validated its corresponding dynamic facial expression, producing over 60 culturally valid facial expression models. I then pooled the resulting models and applied a multivariate data reduction technique to reveal four culturally common facial expression patterns. I also showed that each facial expression pattern communicates specific combinations of valence, arousal and dominance. Finally, I revealed the face movements that accentuate each culturally common facial expression pattern to create complex facial expressions. Here, my data questions the widely held view that six facial expression patterns are universal, instead suggesting four latent expressive patterns with direct implications for emotion communication, social psychology, cognitive neuroscience, and social robotics. 

I also examined the dynamical nature of facial expressions of emotion using information theory and Bayesian classifiers applied to models of the six basic facial expressions of emotion. Here, I showed that dynamic facial expressions transmit an evolving hierarchy of signals over time, where early, simpler face signals support the discrimination of four emotion categories - i.e., (i) happy, (ii), sad, (iii) fear/surprise, and (iv) disgust/anger - whereas later, more complex signals support discrimination of all six emotions. My data question the widely accepted notion that human emotion comprises six basic categories, instead suggesting four. This research finding won the international Innovation Award 2016 awarded by the Social and Affective Neuroscience Society. 

My results have broad implications for science and modern society. My data advances knowledge of emotion communication beyond the limited set of six Western specific static face images, thereby informing emotion communication to foster healthy societal relations, including clinical groups. My improved facial expression models provide practical benefits for Social Science research, Computer Science, Human-Robot Interaction, the commercial gaming industry, and the digital economy (e.g., development of avatars and companion robots). Together, my data makes a timely contribution to the rapidly evolving communication needs of society with great benefits for the economic performance of the UK.</gtr:description><gtr:exploitationPathways>My results have broad implications for science and modern society. My data advances knowledge of emotion communication beyond the limited set of six Western specific static face images, thereby informing emotion communication to foster healthy societal relations across different cultures, including clinical groups. My improved facial expression models provide practical benefits for Social Science research, Computer Science, Human-Robot Interaction, the commercial gaming industry, and the digital economy (e.g., development of avatars and companion robots). Together, my data makes a timely contribution to the rapidly evolving communication needs of society with great benefits for the economic performance of the UK.

For example, my dynamic facial expression models are now being transferred to virtual humans by my international collaborator, Prof. Stacy Marsella (http://www.ccs.neu.edu/people/faculty/member/marsella/) - a world leader in the design of virtual humans with a view to improving human user interaction. I am also applying my dynamic facial expression models to my recently acquired FurHat robot head http://www.furhatrobotics.com/ to design social robots of the future that can display realistic behaviours that reliably impact human user social judgments. 

We showcased our results and their many applications at the prestigious Royal Society Summer Science Exhibition 2015, which allowed me to disseminate my results to the public, and engage their interests in psychological research.</gtr:exploitationPathways><gtr:id>C8CE7539-5CBE-497A-A6E2-4825B72375EE</gtr:id><gtr:sectors><gtr:sector>Communities and Social Services/Policy,Digital/Communication/Information Technologies (including Software),Culture, Heritage, Museums and Collections</gtr:sector></gtr:sectors><gtr:url>http://www.altmetric.com/details.php?citation_id=2016990</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/34A65D7E-1029-4D89-9E68-355ED07269AF"><gtr:id>34A65D7E-1029-4D89-9E68-355ED07269AF</gtr:id><gtr:title>Four not six: Revealing culturally common facial expressions of emotion.</gtr:title><gtr:parentPublicationTitle>Journal of experimental psychology. General</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/110bc41dfcffc8cb1bb088e70bfb6c28"><gtr:id>110bc41dfcffc8cb1bb088e70bfb6c28</gtr:id><gtr:otherNames>Jack RE</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>0022-1015</gtr:issn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/636AB22D-5A13-482A-AF28-2D0D22ADC939"><gtr:id>636AB22D-5A13-482A-AF28-2D0D22ADC939</gtr:id><gtr:title>Dynamic mental models of culture-specific emotions</gtr:title><gtr:parentPublicationTitle>Journal of Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/e0583eb584340b0d99bc1c77ac6e0020"><gtr:id>e0583eb584340b0d99bc1c77ac6e0020</gtr:id><gtr:otherNames>Sun W</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/DAAA5206-9D42-438A-8BC8-ADAC26001D17"><gtr:id>DAAA5206-9D42-438A-8BC8-ADAC26001D17</gtr:id><gtr:title>Dynamic facial expressions of emotion transmit an evolving hierarchy of signals over time.</gtr:title><gtr:parentPublicationTitle>Current biology : CB</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/110bc41dfcffc8cb1bb088e70bfb6c28"><gtr:id>110bc41dfcffc8cb1bb088e70bfb6c28</gtr:id><gtr:otherNames>Jack RE</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>0960-9822</gtr:issn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/4220435B-946A-40DD-ACDD-31D7FBD262BF"><gtr:id>4220435B-946A-40DD-ACDD-31D7FBD262BF</gtr:id><gtr:title>Space-by-time manifold representation of dynamic facial expressions for emotion categorization.</gtr:title><gtr:parentPublicationTitle>Journal of vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/d7cf3d5b38d3273b5b3e04f48353b2c7"><gtr:id>d7cf3d5b38d3273b5b3e04f48353b2c7</gtr:id><gtr:otherNames>Delis I</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date><gtr:issn>1534-7362</gtr:issn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/EF678D2A-3C4D-42E0-94C1-9732B49CDDC4"><gtr:id>EF678D2A-3C4D-42E0-94C1-9732B49CDDC4</gtr:id><gtr:title>Toward a Social Psychophysics of Face Communication.</gtr:title><gtr:parentPublicationTitle>Annual review of psychology</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/110bc41dfcffc8cb1bb088e70bfb6c28"><gtr:id>110bc41dfcffc8cb1bb088e70bfb6c28</gtr:id><gtr:otherNames>Jack RE</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2017-01-01</gtr:date><gtr:issn>0066-4308</gtr:issn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/C37410B2-B2DA-4429-A4C6-F35E5FEA1173"><gtr:id>C37410B2-B2DA-4429-A4C6-F35E5FEA1173</gtr:id><gtr:title>The Human Face as a Dynamic Tool for Social Communication.</gtr:title><gtr:parentPublicationTitle>Current biology : CB</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/110bc41dfcffc8cb1bb088e70bfb6c28"><gtr:id>110bc41dfcffc8cb1bb088e70bfb6c28</gtr:id><gtr:otherNames>Jack RE</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>0960-9822</gtr:issn></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">ES/K001973/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>5858EC49-4786-4440-8352-1AB0B6DC5F23</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Psychology</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>5858EC49-4786-4440-8352-1AB0B6DC5F23</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Psychology</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>