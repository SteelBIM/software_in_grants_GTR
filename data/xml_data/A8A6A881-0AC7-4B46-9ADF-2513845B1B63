<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.rcuk.ac.uk/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.rcuk.ac.uk:80/organisation/6FF2BE17-6E12-40FB-AF0C-E9DBE2C6D20F"><gtr:id>6FF2BE17-6E12-40FB-AF0C-E9DBE2C6D20F</gtr:id><gtr:name>Technicolor</gtr:name><gtr:address><gtr:line1>1, avenue de Belle Fontaine</gtr:line1><gtr:region>Unknown</gtr:region></gtr:address></gtr:collaborator><gtr:collaborator url="http://gtr.rcuk.ac.uk:80/organisation/CF058FF3-A17B-43BF-B828-7507DD791A82"><gtr:id>CF058FF3-A17B-43BF-B828-7507DD791A82</gtr:id><gtr:name>Apical</gtr:name></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.rcuk.ac.uk:80/organisation/3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9"><gtr:id>3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9</gtr:id><gtr:name>University of Oxford</gtr:name><gtr:department>Engineering Science</gtr:department><gtr:address><gtr:line1>University Chest</gtr:line1><gtr:line2>Wellington Square</gtr:line2><gtr:line4>Oxford</gtr:line4><gtr:postCode>OX1 2JD</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9"><gtr:id>3EAE04CA-9D62-4483-B9C4-F91AD9F4C5A9</gtr:id><gtr:name>University of Oxford</gtr:name><gtr:address><gtr:line1>University Chest</gtr:line1><gtr:line2>Wellington Square</gtr:line2><gtr:line4>Oxford</gtr:line4><gtr:postCode>OX1 2JD</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/6FF2BE17-6E12-40FB-AF0C-E9DBE2C6D20F"><gtr:id>6FF2BE17-6E12-40FB-AF0C-E9DBE2C6D20F</gtr:id><gtr:name>Technicolor</gtr:name><gtr:address><gtr:line1>1, avenue de Belle Fontaine</gtr:line1><gtr:region>Unknown</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/CF058FF3-A17B-43BF-B828-7507DD791A82"><gtr:id>CF058FF3-A17B-43BF-B828-7507DD791A82</gtr:id><gtr:name>Apical</gtr:name><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/CBCF51D8-792D-4907-9E56-FD4E1BC5B8E0"><gtr:id>CBCF51D8-792D-4907-9E56-FD4E1BC5B8E0</gtr:id><gtr:name>OMG plc</gtr:name><gtr:address><gtr:line1>OMG plc</gtr:line1><gtr:line2>14 Minns Business Park</gtr:line2><gtr:line3>West Way</gtr:line3><gtr:line4>Oxford</gtr:line4><gtr:postCode>OX2 0JB</gtr:postCode><gtr:region>South East</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.rcuk.ac.uk:80/person/6D419C9B-1ACA-435A-B8B1-36FA3461BF52"><gtr:id>6D419C9B-1ACA-435A-B8B1-36FA3461BF52</gtr:id><gtr:firstName>Philip</gtr:firstName><gtr:otherNames>Hilaire</gtr:otherNames><gtr:surname>Torr</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.rcuk.ac.uk:80/projects?ref=EP%2FI001107%2F2"><gtr:id>A8A6A881-0AC7-4B46-9ADF-2513845B1B63</gtr:id><gtr:title>Scene Understanding using New Global Energy Models</gtr:title><gtr:status>Closed</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/I001107/2</gtr:grantReference><gtr:abstractText>This proposal concerns scene understanding from video. Computer vision algorithms for individual tasks such as objectrecognition, detection and segmentation has now reached some level of maturity. The next challenge is to integrate all thesealgorithms and address the problem of scene understanding. The problem of scene understanding involves explaining thewhole image by recognizing all the objects of interest within an image and their spatial extent or shape in 3D.The application to drive the research will be the problem of automated understanding of cities from video usingcomputer vision, inspired by the availability of massive new data sets such as that of Google's Street Viewhttp://maps.google.com/help/maps/streetview/, Yotta http://www.yotta.tv/index.php (who have agreed to supply OxfordBrookes with data) and Microsoft's Photosynth http://labs.live.com/photosynth/. The scenario is as follows: a van drivesaround the roads of the UK, in the van are GPS equipment and multiple calibrated cameras, synchronized to capture andstore an image every two metres; giving a massive data set. The task is to recognize objects of interest in the video, fromroad signs and other street furniture, to particular buildings, to allow them to be located exactly on maps of the environment.A second scenario would be to perform scene understanding for indoor scenes such as home or office, with video taken froma normal camera and Z-cam.</gtr:abstractText><gtr:potentialImpactText>Impact Plan The aim of this project is twofold, first to engage in basic science, second to produce a commercially useful set of outcomes that will improve UK competitiveness. The former will be a set of papers on object recognition combined with structure, models thereof and combinatorial algorithms to bring the work to fruition. The latter will be greatly helped by interaction with the Oxford Metrics Group, both 2d3 and Yotta who have undertaken to meet with the project members regularly to ensure the outcomes are commercially useful. Professor Torr's contacts with Sony and Microsoft will enable him to steer the project along lines that should provide maximum benefit to the UK economy. This project clearly relates to the Digital Economy, which is highlighted as a key area in the EPSRC Delivery plan 2008-11, and in particular to Transport and the Creative industries, which are highlighted as an area of particular importance within the Digital Economy-thus this proposal lies exactly in accord with fundamental directions outlined in the EPSRC's Delivery Plan. Transport Industry: Our starting target industry is the highways industry who are interested in their 'asset inventories', e.g. location of street furniture, heights of bridges. The UK Government has now adopted a policy to implement Resource Accounting and Budgeting and Whole Government Accounting (WGA). The use of Asset Management Plans is essential to underpinning this policy. This is currently not obligatory, but is likely to be legislated within the next couple of years (i.e. a requirement in order to apply for road maintenance funding). At that point, we would expect that every local authority in the UK will be required to provide an inventory for their entire road network, totaling about 400,000km. The inventory would need to be updated annually. A typical rate for asset inventory is about 30/km. We expect that the USA will follow within the next 10 years; the Europe market similarly. Creative Industries: The second application that would be considered would be the identification of objects in indoor scenes. The scenes might be of rooms in the home, public building or workplace. Professor Torr works closely with Sony on the EyeToy (http://en.wikipedia.org/wiki/EyeToy), the EyeToy is typically placed in the living room and being able to recognize objects within the living room would significantly help with the design of games. It is anticipated the the release of the time of flight camera as a peripheral for Microsoft's project Natal would revolutionize not only the gaming industry but research in computer vision as well. With the deep penetration of the Xbox it would be expected that over five million units would be sold. That means five million Z-cams in people's living rooms. Currently research on time of flight cameras is not the main stream, but Natal will change this and the commercial desire for such things as object recognition will be immediate in games, HCI, advertising using Z-cam data. Exploitation: Intellectual Property Rights management and exploitation will be managed by the Research and Business Development Office (RBDO) at Oxford Brookes University, which has access to financial and other resources to enable Intellectual Property and its commercial exploitation to be effectively managed, whilst maximizing the widespread dissemination of the research results. This includes, as appropriate, finance for patenting and proof of concept funding; IP, technology and market assessment; resources for defining and implementing a commercialization strategy though licensing, start-up company or other routes. Oxford Brookes Computer Vision group already has an established record for exploiting IP, and interactions with several companies. Agreements are in place with Sony and OMG.</gtr:potentialImpactText><gtr:fund><gtr:end>2016-02-29</gtr:end><gtr:funder url="http://gtr.rcuk.ac.uk:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2013-10-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>327698</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>Apical</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:description>Real time processing on Apical</gtr:description><gtr:id>F75D1C2C-5301-4B82-B976-3B1C7D902D35</gtr:id><gtr:impact>just started</gtr:impact><gtr:partnerContribution>160K fund student</gtr:partnerContribution><gtr:piContribution>tech</gtr:piContribution><gtr:sector>Private</gtr:sector><gtr:start>2014-01-01</gtr:start></gtr:collaborationOutput><gtr:collaborationOutput><gtr:collaboratingOrganisation>Technicolor</gtr:collaboratingOrganisation><gtr:country>United States of America</gtr:country><gtr:description>segmentation with technicolor</gtr:description><gtr:id>54D840EF-66FF-41D4-89F0-95A0A7D093E8</gtr:id><gtr:impact>tech</gtr:impact><gtr:partnerContribution>tech</gtr:partnerContribution><gtr:piContribution>segmentation tech</gtr:piContribution><gtr:sector>Private</gtr:sector><gtr:start>2013-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>Many mentions in News Media TV</gtr:description><gtr:form>A press release, press conference or response to a media enquiry/interview</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>A8A34114-7985-4D6D-95B0-5EEDDE4E3FD2</gtr:id><gtr:impact>many BBC, news papers etc etc

some of it listed here
http://www.robots.ox.ac.uk/~tvg/projects/SemanticPaint/index.php

google semantic paint


see also here http://www.va-st.com/smart-specs/</gtr:impact><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Media (as a channel to the public)</gtr:primaryAudience><gtr:url>http://www.va-st.com/smart-specs/</gtr:url><gtr:year>2015</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>2200000</gtr:amountPounds><gtr:country>European Union (EU)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Towards Total Scene Understanding using Structured Models</gtr:description><gtr:end>2019-01-02</gtr:end><gtr:fundingOrg>European Commission (EC)</gtr:fundingOrg><gtr:id>8E128B74-F477-44E4-9987-B21D4E2D7641</gtr:id><gtr:sector>Public</gtr:sector><gtr:start>2014-01-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs><gtr:impactSummaryOutput><gtr:description>yes

some of the face tracking s/w has been licensed by Real D maker of 3d glasses for 3d cinema

some has been used by Microsoft


we are to spin out a new company this year to help the partially sighted see http://www.va-st.com/smart-specs/</gtr:description><gtr:firstYearOfImpact>2015</gtr:firstYearOfImpact><gtr:id>684D569B-06D4-4097-A8A5-7DCFAEB4D330</gtr:id><gtr:impactTypes><gtr:impactType>Economic</gtr:impactType></gtr:impactTypes><gtr:sector>Creative Economy,Digital/Communication/Information Technologies (including Software),Healthcare</gtr:sector></gtr:impactSummaryOutput></gtr:impactSummaryOutputs><gtr:intellectualPropertyOutputs><gtr:intellectualPropertyOutput><gtr:description>face tracker</gtr:description><gtr:id>08B0F670-B87F-434F-BFA5-C67C60D74DD2</gtr:id><gtr:impact>licensed to Real D</gtr:impact><gtr:licensed>Yes</gtr:licensed><gtr:protection>Copyrighted (e.g. software)</gtr:protection><gtr:title>face tracking</gtr:title><gtr:yearProtectionGranted>2012</gtr:yearProtectionGranted></gtr:intellectualPropertyOutput></gtr:intellectualPropertyOutputs><gtr:keyFindingsOutput><gtr:description>we have worked on recognition and segmentation of objects in images

large scale coverage in media 
e.g. google semantic Paint 
in main stream news e.g. tabloids, and on BBC

also large scale coverage on media for our other work
see http://www.robots.ox.ac.uk/~szheng/CRFasRNN.html

this is best in world, beating top tech companues</gtr:description><gtr:exploitationPathways>examples include

we are building augmented reality glasses to help partially sighted have enhanced vision.

we are working with Creative industries for film and gaming (segmentation of images and recognition).

We are collaborating with 3D cinema company RealD on building the software of the future generation glasses for viewing 3D films.

We are also working with search engine company Baidu on building software for wearable devices in retail scenario and automatic-cars.

spinning out a company to help partially sighted this year

see http://www.va-st.com/smart-specs/</gtr:exploitationPathways><gtr:id>D84345C4-F4EA-4163-B98B-206306F18DCB</gtr:id><gtr:sectors><gtr:sector>Creative Economy,Education,Healthcare,Retail,Transport</gtr:sector></gtr:sectors><gtr:url>http://www.robots.ox.ac.uk/~tvg/</gtr:url></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs><gtr:spinOutOutput><gtr:companyName>oxsight</gtr:companyName><gtr:description>xSight is a University of Oxford venture that uses the latest smart glasses to improve sight for blind and partially sighted people. OxSight's aim is to develop sight enhancing technologies to improve the quality of life for blind and partially sighted people around the world. Our current commercial products can enhance vision for people affected by conditions like glaucoma, diabetes and retinitis pigmentosa as well as some other degenerative eye diseases.</gtr:description><gtr:id>3A08F88E-9234-4CD0-AA90-98D90451F1A3</gtr:id><gtr:impact>see http://smartspecs.co/</gtr:impact><gtr:url>http://smartspecs.co/</gtr:url><gtr:yearCompanyFormed>2016</gtr:yearCompanyFormed></gtr:spinOutOutput></gtr:spinOutOutputs></gtr:output><gtr:publications><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/61505FAD-B821-4C7A-A292-3A19C336E22D"><gtr:id>61505FAD-B821-4C7A-A292-3A19C336E22D</gtr:id><gtr:title>ImageSpirit</gtr:title><gtr:parentPublicationTitle>ACM Transactions on Graphics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/52528459463e99fb67f7cc1bfba26f2b"><gtr:id>52528459463e99fb67f7cc1bfba26f2b</gtr:id><gtr:otherNames>Cheng M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/682A8C84-5B04-47A8-B496-ED494EFBB59C"><gtr:id>682A8C84-5B04-47A8-B496-ED494EFBB59C</gtr:id><gtr:title>SemanticPaint</gtr:title><gtr:parentPublicationTitle>ACM Transactions on Graphics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/bc59f5fb736c3da3ed9d0d9ee91dddab"><gtr:id>bc59f5fb736c3da3ed9d0d9ee91dddab</gtr:id><gtr:otherNames>Valentin J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/818FE6C8-F164-4F02-8D58-135BD445D4A2"><gtr:id>818FE6C8-F164-4F02-8D58-135BD445D4A2</gtr:id><gtr:title>Computer Vision - ECCV 2012</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/c4e4dabbd6b89ad7e7ae0aaddcd5732f"><gtr:id>c4e4dabbd6b89ad7e7ae0aaddcd5732f</gtr:id><gtr:otherNames>Vineet V</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>978-3-642-33714-7</gtr:isbn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/B5C3D963-8BC2-49AF-B6B1-602E62877969"><gtr:id>B5C3D963-8BC2-49AF-B6B1-602E62877969</gtr:id><gtr:title>Approximate structured output learning for Constrained Local Models with application to real-time facial feature detection and tracking on low-power devices</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/2a340fbb42265c9ec6bc733e0a04c590"><gtr:id>2a340fbb42265c9ec6bc733e0a04c590</gtr:id><gtr:otherNames>Zheng S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:isbn>978-1-4673-5545-2</gtr:isbn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/1D18F233-2D65-4BE5-8C63-9FF18A5BA59F"><gtr:id>1D18F233-2D65-4BE5-8C63-9FF18A5BA59F</gtr:id><gtr:title>Dense Semantic Image Segmentation with Objects and Attributes</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/2a340fbb42265c9ec6bc733e0a04c590"><gtr:id>2a340fbb42265c9ec6bc733e0a04c590</gtr:id><gtr:otherNames>Zheng S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/8030C936-DF8F-47C4-A79A-95BAC9689542"><gtr:id>8030C936-DF8F-47C4-A79A-95BAC9689542</gtr:id><gtr:title>Distributed ADMM-based inference in large-scale random fields</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/a49a21d77d9dee3ab76e089088f5b100"><gtr:id>a49a21d77d9dee3ab76e089088f5b100</gtr:id><gtr:otherNames>Miksik O</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/15D9A46F-E688-493D-80E7-B7548F4E1399"><gtr:id>15D9A46F-E688-493D-80E7-B7548F4E1399</gtr:id><gtr:title>Conditional Random Fields as Recurrent Neural Networks</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/dd4d68c8ddfc86487d28f0aff1415374"><gtr:id>dd4d68c8ddfc86487d28f0aff1415374</gtr:id><gtr:otherNames>Zheng Shuai</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/C45856CA-7F26-4581-A503-9B1D1AEA0722"><gtr:id>C45856CA-7F26-4581-A503-9B1D1AEA0722</gtr:id><gtr:title>Mesh Based Semantic Modelling for Indoor and Outdoor Scenes</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/bc59f5fb736c3da3ed9d0d9ee91dddab"><gtr:id>bc59f5fb736c3da3ed9d0d9ee91dddab</gtr:id><gtr:otherNames>Valentin J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/F6B82CAE-F9CE-4339-A89E-414149DC22EC"><gtr:id>F6B82CAE-F9CE-4339-A89E-414149DC22EC</gtr:id><gtr:title>Exploiting uncertainty in regression forests for accurate camera relocalization</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/bc59f5fb736c3da3ed9d0d9ee91dddab"><gtr:id>bc59f5fb736c3da3ed9d0d9ee91dddab</gtr:id><gtr:otherNames>Valentin J</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/233E7FDE-0726-4270-BCB4-E3904485356D"><gtr:id>233E7FDE-0726-4270-BCB4-E3904485356D</gtr:id><gtr:title>Robust Non-parametric Data Fitting for Correspondence Modeling</gtr:title><gtr:parentPublicationTitle>IEEE International Conference on Computer Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/3eb486b2d1380e3d3f83b08315d9a119"><gtr:id>3eb486b2d1380e3d3f83b08315d9a119</gtr:id><gtr:otherNames> Wen-Yan Lin (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/4973E996-CC35-49E3-9FCC-EA7ACFC65B57"><gtr:id>4973E996-CC35-49E3-9FCC-EA7ACFC65B57</gtr:id><gtr:title>A tiered move-making algorithm for general pairwise MRFs</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/c4e4dabbd6b89ad7e7ae0aaddcd5732f"><gtr:id>c4e4dabbd6b89ad7e7ae0aaddcd5732f</gtr:id><gtr:otherNames>Vineet V</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>978-1-4673-1226-4</gtr:isbn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/0D6F2ADF-52C5-424A-827F-8AE30CCFDAE1"><gtr:id>0D6F2ADF-52C5-424A-827F-8AE30CCFDAE1</gtr:id><gtr:title>Target Identity-aware Network Flow for online multiple target tracking</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/c601a805f173a4991f1b619667ca44c2"><gtr:id>c601a805f173a4991f1b619667ca44c2</gtr:id><gtr:otherNames>Dehghan A</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/8E8FF77B-422A-4C4E-81C8-D455D1B96951"><gtr:id>8E8FF77B-422A-4C4E-81C8-D455D1B96951</gtr:id><gtr:title>Inference Methods for CRFs with Co-occurrence Statistics</gtr:title><gtr:parentPublicationTitle>International Journal of Computer Vision</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/e9c5543112c903d1af99b720b52bb280"><gtr:id>e9c5543112c903d1af99b720b52bb280</gtr:id><gtr:otherNames>Ladick? ?</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/F0F2D34C-CB08-48F2-B664-D5B21A3AAE74"><gtr:id>F0F2D34C-CB08-48F2-B664-D5B21A3AAE74</gtr:id><gtr:title>Incremental dense multi-modal 3D scene reconstruction</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/a49a21d77d9dee3ab76e089088f5b100"><gtr:id>a49a21d77d9dee3ab76e089088f5b100</gtr:id><gtr:otherNames>Miksik O</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/FDE6CB6E-3074-4FD3-905C-E2429B131151"><gtr:id>FDE6CB6E-3074-4FD3-905C-E2429B131151</gtr:id><gtr:title>Prototypical Priors: From Improving Classification to Zero-Shot Learning</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/070c2a14edf7c337aef2e21116cb7c90"><gtr:id>070c2a14edf7c337aef2e21116cb7c90</gtr:id><gtr:otherNames>Tetley S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/B36C7E7C-44DC-473F-BF0D-09DBDD6F148D"><gtr:id>B36C7E7C-44DC-473F-BF0D-09DBDD6F148D</gtr:id><gtr:title>The Semantic Paintbrush</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/a49a21d77d9dee3ab76e089088f5b100"><gtr:id>a49a21d77d9dee3ab76e089088f5b100</gtr:id><gtr:otherNames>Miksik O</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:isbn>9781450331456</gtr:isbn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/F1B18E7E-D192-42E7-AF13-D631A6FBCE6C"><gtr:id>F1B18E7E-D192-42E7-AF13-D631A6FBCE6C</gtr:id><gtr:title>Staple: Complementary Learners for Real-Time Tracking</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/74c187c793077cf4ecad1345363499f3"><gtr:id>74c187c793077cf4ecad1345363499f3</gtr:id><gtr:otherNames>Bertinetto L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/311EBB0F-0A59-432A-91B2-D5373A0AF3C5"><gtr:id>311EBB0F-0A59-432A-91B2-D5373A0AF3C5</gtr:id><gtr:title>Urban 3D semantic modelling using stereo vision</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/fc70088db3e5c706d92a1402b1affd20"><gtr:id>fc70088db3e5c706d92a1402b1affd20</gtr:id><gtr:otherNames>Sengupta S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date><gtr:isbn>978-1-4673-5641-1</gtr:isbn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/0F831372-3145-4016-B5A3-78D352243EAA"><gtr:id>0F831372-3145-4016-B5A3-78D352243EAA</gtr:id><gtr:title>Associative Hierarchical Random Fields.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on pattern analysis and machine intelligence</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/b487cb2a06228c4f89ba259ef32f29ba"><gtr:id>b487cb2a06228c4f89ba259ef32f29ba</gtr:id><gtr:otherNames>Ladick? L</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2014-01-01</gtr:date><gtr:issn>0098-5589</gtr:issn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/AAED78A6-C25C-4A02-BD73-3F422A11F3F1"><gtr:id>AAED78A6-C25C-4A02-BD73-3F422A11F3F1</gtr:id><gtr:title>SalientShape: group saliency in image collections</gtr:title><gtr:parentPublicationTitle>The Visual Computer</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/52528459463e99fb67f7cc1bfba26f2b"><gtr:id>52528459463e99fb67f7cc1bfba26f2b</gtr:id><gtr:otherNames>Cheng M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2013-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/17F3C289-AC92-4107-B45C-32EF7F6F71C3"><gtr:id>17F3C289-AC92-4107-B45C-32EF7F6F71C3</gtr:id><gtr:title>Struck: Structured Output Tracking with Kernels.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on pattern analysis and machine intelligence</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/2425a4d328ce2edcb596c40787925c07"><gtr:id>2425a4d328ce2edcb596c40787925c07</gtr:id><gtr:otherNames>Hare S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>0098-5589</gtr:issn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/3D8931B1-8664-486A-883B-8E03BD295808"><gtr:id>3D8931B1-8664-486A-883B-8E03BD295808</gtr:id><gtr:title>Automatic dense visual semantic mapping from street-level imagery</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/fc70088db3e5c706d92a1402b1affd20"><gtr:id>fc70088db3e5c706d92a1402b1affd20</gtr:id><gtr:otherNames>Sengupta S</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date><gtr:isbn>978-1-4673-1737-5</gtr:isbn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/C7D0A411-F1A9-4556-B609-E04AD9B32E5C"><gtr:id>C7D0A411-F1A9-4556-B609-E04AD9B32E5C</gtr:id><gtr:title>Very high frame rate volumetric integration of depth images on mobile devices.</gtr:title><gtr:parentPublicationTitle>IEEE transactions on visualization and computer graphics</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/539791ebfe9edb5a4d8b2203f75df96e"><gtr:id>539791ebfe9edb5a4d8b2203f75df96e</gtr:id><gtr:otherNames>K?hler O</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date><gtr:issn>1077-2626</gtr:issn></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/D7E3D810-72AE-4899-9A95-73EB2298C7C7"><gtr:id>D7E3D810-72AE-4899-9A95-73EB2298C7C7</gtr:id><gtr:title>DenseCut: Densely Connected CRFs for Realtime GrabCut</gtr:title><gtr:parentPublicationTitle>Computer Graphics Forum</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/52528459463e99fb67f7cc1bfba26f2b"><gtr:id>52528459463e99fb67f7cc1bfba26f2b</gtr:id><gtr:otherNames>Cheng M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/D422872D-215C-4F7F-9533-758D1FE93784"><gtr:id>D422872D-215C-4F7F-9533-758D1FE93784</gtr:id><gtr:title>Improved Initialisation and Gaussian Mixture Pairwise Terms for Dense Random Fields with Mean-field Inference</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/0fd42485365b0c4c1a85081cd2c15689"><gtr:id>0fd42485365b0c4c1a85081cd2c15689</gtr:id><gtr:otherNames>Vibhav Vineet (Author)</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2012-01-01</gtr:date></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/I001107/2</gtr:identifier></gtr:identifiers><gtr:projectHierarchy><gtr:parents><gtr:parent><gtr:id>8858F7A9-A6BD-48B3-8F36-2CA57F3BE94F</gtr:id><gtr:grantRef>EP/I001107/1</gtr:grantRef><gtr:amount>439228.41</gtr:amount><gtr:start>2011-09-07</gtr:start><gtr:end>2013-09-30</gtr:end><gtr:children><gtr:child rel="Transfer"><gtr:id>A8A6A881-0AC7-4B46-9ADF-2513845B1B63</gtr:id><gtr:grantRef>EP/I001107/2</gtr:grantRef><gtr:amount>327698.99</gtr:amount><gtr:start>2013-10-01</gtr:start><gtr:end>2016-02-29</gtr:end><gtr:children/></gtr:child></gtr:children></gtr:parent></gtr:parents></gtr:projectHierarchy><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>96B4D986-4762-4E29-9962-0B2240D10CE2</gtr:id><gtr:percentage>100</gtr:percentage><gtr:text>Image &amp; Vision Computing</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>