<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.rcuk.ac.uk/api"><gtr:projectComposition><gtr:collaborations/><gtr:leadResearchOrganisation url="http://gtr.rcuk.ac.uk:80/organisation/D1774113-D5D2-4B7C-A412-66A90FE4B96F"><gtr:id>D1774113-D5D2-4B7C-A412-66A90FE4B96F</gtr:id><gtr:name>University of Cambridge</gtr:name><gtr:department>Engineering</gtr:department><gtr:address><gtr:line1>Lensfield Road</gtr:line1><gtr:line4>Cambridge</gtr:line4><gtr:postCode>CB2 1EW</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/D1774113-D5D2-4B7C-A412-66A90FE4B96F"><gtr:id>D1774113-D5D2-4B7C-A412-66A90FE4B96F</gtr:id><gtr:name>University of Cambridge</gtr:name><gtr:address><gtr:line1>Lensfield Road</gtr:line1><gtr:line4>Cambridge</gtr:line4><gtr:postCode>CB2 1EW</gtr:postCode><gtr:region>East of England</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/6A0C199D-25A8-4C51-A917-ED60FB72E028"><gtr:id>6A0C199D-25A8-4C51-A917-ED60FB72E028</gtr:id><gtr:name>VocalIQ Limited</gtr:name><gtr:address><gtr:line1>Mount Pleasant House</gtr:line1><gtr:line2>2 Mount Pleasant</gtr:line2><gtr:postCode>CB3 0RN</gtr:postCode><gtr:region>Unknown</gtr:region></gtr:address><gtr:roles><gtr:role><gtr:name>PROJECT_PARTNER</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.rcuk.ac.uk:80/person/752EB7CE-19A3-4739-ABCF-DDB8033C01F7"><gtr:id>752EB7CE-19A3-4739-ABCF-DDB8033C01F7</gtr:id><gtr:firstName>Stephen</gtr:firstName><gtr:otherNames>John</gtr:otherNames><gtr:surname>Young</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.rcuk.ac.uk:80/projects?ref=EP%2FM018946%2F1"><gtr:id>09B4E060-AE65-435F-8856-5D2D0A5C32C1</gtr:id><gtr:title>Open Domain Statistical Spoken Dialogue Systems</gtr:title><gtr:status>Active</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>EP/M018946/1</gtr:grantReference><gtr:abstractText>Spoken Dialogue Systems (SDS) encompass the technologies required to build effective man-machine interfaces which depend primarily on voice. To date they have mostly been deployed in telephone-based call centre applications such as banking, billing queries and travel information and they are built using hand-crafted rules.

The recent introduction of Apple Siri and Google Now has moved voice-based interfaces into the main-stream. These virtual personal assistants (VPAs) offer the potential to revolutionise the way we interact with machines, and they open the way to properly control and manage the emerging Internet of Things - the rapidly growing network of smart devices which lack any form of conventional user interface. However, current personal assistants are built using the same technology as limited domain spoken dialogue systems. They are not capable of sustaining conversational dialogues except within the selected limited domains which they have been explicitly programmed to handle.

Very recent work on statistical SDS has demonstrated that it is not only possible for such a system to adapt and improve performance within the domain for which it has been designed but it is also possible for the system to automatically extend its coverage to include new, hitherto unseen concepts. This suggests that it should be possible to build on the progress achieved in the development of limited domain statistical SDS to design a radically new form of spoken dialogue system (and hence VPA) which is able to extend and adapt with use to cover an ever-wider range of conversational topics. The design of such a system is the focus of this research proposal.

The key idea is to integrate the latest statistical dialogue technology into a wide coverage knowledge graph (such as freebase) which contains not only ontological information about entities but also the operations that can be applied to those entities (e.g. find flight information, book a hotel room, buy an ebook, etc. ).

The implementation of a single monolithic spoken dialogue system capable of interpreting and responding to every conceivable user request is simply not practicable. Hence, rather than simply trying to broaden the coverage of existing SDS, a novel distributed system architecture is proposed with three key features:

1. the three essential components of an SDS (semantic decoder, dialogue manager and response generator) are distributed across the knowledge-graph. In essence, every node in the graph has the capability to recognise when it is being referred to and have the capability to respond appropriately.

2. when the user speaks, all semantic decoders are listening, based on the activation levels of the decoder outputs, a topic tracker identifies which concept is in focus and activates its dialogue policy.

3. all components are statistical enabling them to be adapted automatically on-line using unsupervised adaptation. Data sparsity is managed by ensuring that the top level nodes in the class hierarchy have well-trained components. Initially, lower level more specialised concepts simply inherit the required statistical models from their super-classes. As the system interacts with users and more data is collected, lower level components acquire sufficient data to train their own dedicated statistical models.

The end result is a system that continually learns on-line. It starts with a limited and stilted conversational style, but the more it is used, the more fluent it becomes, and as users explore new topics, the system learns to adapt and extend its capability to handle those new topics. Since many users can be using the system simultaneously, learning can be fast and capable of accommodating live updates of the underlying data, all of which are characteristics that a virtual personal assistant must have to be genuinely useful.</gtr:abstractText><gtr:potentialImpactText>The principal goal of this research is to extend the theory and practice of spoken dialogue systems to support conversational interaction in unrestricted open domains. This enabling technology is critical to the development of accessible and widely available general purpose human computer interfaces, especially virtual personal assistants (VPAs).

VPAs offer the potential to revolutionise the way we interact with machines. They are being introduced to the public via smart phones, but they are actually independent -- all that they really need is an audio channel to a remote server via the internet. This is a disruptive technology with a relatively low barrier to entry and high impact. VPAs have the potential to change not only the way we interact with machines, but also the infrastructure and economic models that underly much of the digital economy since they provide an opportunity to capture users in much the same way that Facebook and Amazon try to capture their users today. This work therefore has the potential to have a direct impact on UK competitiveness.

VPAs will also become essential because speech is the only way to properly control and manage the emerging Internet of Things -- the rapidly growing network of smart devices for which conventional user interfaces are either ergonomically difficult (e.g. Apple iWatch, Google glass) or inappropriate (e.g. home devices such as thermostats, fridges, etc.). Furthermore, users will need all of these devices to be integrated into their highly personalised digital worlds, with a consistent single-point of contact. VPAs are the obvious way to achieve this. Given the UK's dependence on service industry and the knowledge economy, it is essential that it has the technology and expertise to compete in this space.

In order to ensure that the research outputs of this project can be exploited to the benefit of the UK, the Cambridge Dialogue Group is working closely with a Cambridge-based SME called VocalIQ Ltd (www.vocaliq.com) in which Cambridge University is a major share holder. VocalIQ is developing automatic self-learning spoken interfaces for applications in a variety of areas including automobiles, home automation and education. VocalIQ will collaborate on system development, provide advice on commercial deployment issues and provide access to a platform to allow the prototype system to be tested on real users. This direct interaction with a local SME should ensure that the benefits of the research outputs are realised during and soon after project completion.

Through recent and current research projects, the group also has working collaborations with Yahoo Iberia (Mika), Toshiba Cambridge Research Laboratory (Stylianou) and General Motors Advanced Technical Centre (Tzirkel-Hancock).

The two members of research staff working directly on the project will further enhance their skills in machine learning, natural language processing, human-computer interaction and the system skills needed to make complex real-time systems accessible to large segments of the public. There will also be 3 research students associated with the project. All will develop similar skills which will eventually feed into the workforce.

Finally, Cambridge will be launching a new MPhil in Machine Learning and Speech and Language Technology in October 2015 and this project will provide a catalyst for Masters projects and eventually for further PhD projects.</gtr:potentialImpactText><gtr:fund><gtr:end>2018-03-31</gtr:end><gtr:funder url="http://gtr.rcuk.ac.uk:80/organisation/798CB33D-C79E-4578-83F2-72606407192C"><gtr:id>798CB33D-C79E-4578-83F2-72606407192C</gtr:id><gtr:name>EPSRC</gtr:name></gtr:funder><gtr:start>2015-04-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>603424</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs/><gtr:disseminationOutputs/><gtr:exploitationOutputs/><gtr:furtherFundingOutputs/><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs/><gtr:spinOutOutputs/></gtr:output><gtr:publications><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/86BA56EA-859D-4CB4-A49A-E5D4E536E642"><gtr:id>86BA56EA-859D-4CB4-A49A-E5D4E536E642</gtr:id><gtr:title>Dialogue manager domain adaptation using Gaussian process reinforcement learning</gtr:title><gtr:parentPublicationTitle>Computer Speech &amp; Language</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/47a4c3f6707228f612aee5c9b696bfb1"><gtr:id>47a4c3f6707228f612aee5c9b696bfb1</gtr:id><gtr:otherNames>Ga?ic M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/2E40E33E-A265-406D-B1B4-2852539222EA"><gtr:id>2E40E33E-A265-406D-B1B4-2852539222EA</gtr:id><gtr:title>Distributed dialogue policies for multi-domain statistical dialogue management</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/b30006377d33cfe85439ef711b618a27"><gtr:id>b30006377d33cfe85439ef711b618a27</gtr:id><gtr:otherNames>Gasic M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/83FF5678-C3F0-41BA-B960-23F5BE02E6CA"><gtr:id>83FF5678-C3F0-41BA-B960-23F5BE02E6CA</gtr:id><gtr:title>On-line Active Reward Learning for Policy Optimisation in Spoken Dialogue Systems</gtr:title><gtr:parentPublicationTitle>ArXiv e-prints</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/63545c38d9c3b406f80e4f39e5631df5"><gtr:id>63545c38d9c3b406f80e4f39e5631df5</gtr:id><gtr:otherNames>Su Pei-Hao</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/4E329B26-B8AB-4BD2-AC16-4634E44A5C38"><gtr:id>4E329B26-B8AB-4BD2-AC16-4634E44A5C38</gtr:id><gtr:title>Multi-domain dialogue success classifiers for policy training</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/e0714e1ffd1e51c746f029bb34001933"><gtr:id>e0714e1ffd1e51c746f029bb34001933</gtr:id><gtr:otherNames>Vandyke D</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/27CF52A6-72E4-4B2A-9BB4-C82B7EC1226C"><gtr:id>27CF52A6-72E4-4B2A-9BB4-C82B7EC1226C</gtr:id><gtr:title>Counter-fitting Word Vectors to Linguistic Constraints</gtr:title><gtr:parentPublicationTitle>ArXiv e-prints</gtr:parentPublicationTitle><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/655f74c023488a3af663bcc5d4ce64e7"><gtr:id>655f74c023488a3af663bcc5d4ce64e7</gtr:id><gtr:otherNames>Mrk</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2016-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/ED4D75E2-B7D5-4ECE-89A1-103683FD351B"><gtr:id>ED4D75E2-B7D5-4ECE-89A1-103683FD351B</gtr:id><gtr:title>Multi-domain Dialog State Tracking using Recurrent Neural Networks</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/4f188062de31efa82643d07984e466d1"><gtr:id>4f188062de31efa82643d07984e466d1</gtr:id><gtr:otherNames>Mrksic N</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/0D214D74-6598-46D5-B975-6881ABB351C8"><gtr:id>0D214D74-6598-46D5-B975-6881ABB351C8</gtr:id><gtr:title>Reward Shaping with Recurrent Neural Networks for Speeding up On-Line Policy Learning in Spoken Dialogue Systems</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/bdf9b40a8c75f46acb11e720d4cc8da1"><gtr:id>bdf9b40a8c75f46acb11e720d4cc8da1</gtr:id><gtr:otherNames>Su P-H</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/54B4CCD0-DEF2-4C5E-A8C4-547502EB4690"><gtr:id>54B4CCD0-DEF2-4C5E-A8C4-547502EB4690</gtr:id><gtr:title>Learning from real users: rating dialogue success with neural networks for reinforcement learning in spoken dialogue systems</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/b45f4d2518e22ad41863b1015345321a"><gtr:id>b45f4d2518e22ad41863b1015345321a</gtr:id><gtr:otherNames>Pei-hao Su</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date></gtr:publication><gtr:publication url="http://gtr.rcuk.ac.uk:80/publication/170C636D-5662-4254-9E18-71B0AC4EE75C"><gtr:id>170C636D-5662-4254-9E18-71B0AC4EE75C</gtr:id><gtr:title>Policy committee for adaptation in multi-domain spoken dialogue systems</gtr:title><gtr:authors><gtr:author url="http://gtr.rcuk.ac.uk:80/person/b30006377d33cfe85439ef711b618a27"><gtr:id>b30006377d33cfe85439ef711b618a27</gtr:id><gtr:otherNames>Gasic M</gtr:otherNames></gtr:author></gtr:authors><gtr:date>2015-01-01</gtr:date></gtr:publication></gtr:publications><gtr:identifiers><gtr:identifier type="RCUK">EP/M018946/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>EB5F16BB-2772-4DDE-BD6C-3B7A6914B64C</gtr:id><gtr:percentage>80</gtr:percentage><gtr:text>Info. &amp; commun. Technol.</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>B94A2498-60DA-4055-A957-686B6CB42654</gtr:id><gtr:percentage>20</gtr:percentage><gtr:text>Linguistics</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>41593421-CFAC-411D-94A7-E144022B0E6D</gtr:id><gtr:percentage>30</gtr:percentage><gtr:text>Artificial Intelligence</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>15BC6F17-6453-42B4-836A-01286E6D8068</gtr:id><gtr:percentage>20</gtr:percentage><gtr:text>Comput./Corpus Linguistics</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>699D8438-2A43-4BCF-B1A4-6240ED82CEEE</gtr:id><gtr:percentage>50</gtr:percentage><gtr:text>Human Communication in ICT</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>