<?xml version="1.0" encoding="UTF-8"?>
<gtr:projectOverview xmlns:gtr="http://gtr.rcuk.ac.uk/api"><gtr:projectComposition><gtr:collaborations><gtr:collaborator url="http://gtr.rcuk.ac.uk:80/organisation/818CD6C9-61EE-41F2-9F37-0C7A8F43E25D"><gtr:id>818CD6C9-61EE-41F2-9F37-0C7A8F43E25D</gtr:id><gtr:name>University of Birmingham</gtr:name><gtr:address><gtr:line1>Edgbaston Park Road</gtr:line1><gtr:line2>Edgbaston</gtr:line2><gtr:postCode>B15 2TT</gtr:postCode><gtr:region>West Midlands</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address></gtr:collaborator></gtr:collaborations><gtr:leadResearchOrganisation url="http://gtr.rcuk.ac.uk:80/organisation/544242A5-6640-4FD5-87C5-348557ED5307"><gtr:id>544242A5-6640-4FD5-87C5-348557ED5307</gtr:id><gtr:name>University of Abertay Dundee</gtr:name><gtr:department>Sch of Social and Health Sciences</gtr:department><gtr:address><gtr:line1>Bell Street</gtr:line1><gtr:line4>Dundee</gtr:line4><gtr:line5>Angus</gtr:line5><gtr:postCode>DD1 1HG</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:typeInd>RO</gtr:typeInd></gtr:leadResearchOrganisation><gtr:organisationRoles><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/544242A5-6640-4FD5-87C5-348557ED5307"><gtr:id>544242A5-6640-4FD5-87C5-348557ED5307</gtr:id><gtr:name>University of Abertay Dundee</gtr:name><gtr:address><gtr:line1>Bell Street</gtr:line1><gtr:line4>Dundee</gtr:line4><gtr:line5>Angus</gtr:line5><gtr:postCode>DD1 1HG</gtr:postCode><gtr:region>Scotland</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>LEAD_RO</gtr:name></gtr:role></gtr:roles></gtr:organisationRole><gtr:organisationRole url="http://gtr.rcuk.ac.uk:80/organisation/818CD6C9-61EE-41F2-9F37-0C7A8F43E25D"><gtr:id>818CD6C9-61EE-41F2-9F37-0C7A8F43E25D</gtr:id><gtr:name>University of Birmingham</gtr:name><gtr:address><gtr:line1>Edgbaston Park Road</gtr:line1><gtr:line2>Edgbaston</gtr:line2><gtr:postCode>B15 2TT</gtr:postCode><gtr:region>West Midlands</gtr:region><gtr:country>United Kingdom</gtr:country></gtr:address><gtr:roles><gtr:role><gtr:name>COLLABORATOR</gtr:name></gtr:role></gtr:roles></gtr:organisationRole></gtr:organisationRoles><gtr:personRoles><gtr:personRole url="http://gtr.rcuk.ac.uk:80/person/A2E622D2-DF66-4AF8-9494-A1E5629245A4"><gtr:id>A2E622D2-DF66-4AF8-9494-A1E5629245A4</gtr:id><gtr:firstName>Paul</gtr:firstName><gtr:otherNames>George</gtr:otherNames><gtr:surname>Lovell</gtr:surname><gtr:roles><gtr:role><gtr:name>PRINCIPAL_INVESTIGATOR</gtr:name></gtr:role></gtr:roles></gtr:personRole></gtr:personRoles><gtr:project url="http://gtr.rcuk.ac.uk:80/projects?ref=BB%2FN005945%2F1"><gtr:id>D215C836-7720-4E00-BE19-9465D507A9F4</gtr:id><gtr:title>What makes an effective warning signal?</gtr:title><gtr:status>Active</gtr:status><gtr:grantCategory>Research Grant</gtr:grantCategory><gtr:grantReference>BB/N005945/1</gtr:grantReference><gtr:abstractText>Many animals exhibit camouflage patterns that help them remain hidden against their background and be less visible to predators. But some animals have vivid, bright colouring that makes them stand out in their environment. Whilst some conspicuous patterns are known to be involved in attracting a mate, others have evolved to warn a predator that the animal is poisonous or unpalatable. These warning patterns are described as being 'aposematic', and are commonly found in insects, for example, monarch butterflies, ladybirds and wasps. Although aposematic patterns look like they might attract predators, in fact they act as a deterrent. Predators are wary towards prey that are warningly coloured, such as being red or yellow, and are quick to learn that warning colours signal danger and avoid aposematically coloured prey. Although warning signals, like camouflage patterns, have fascinated biologists for more than 150 years, there is still no clear understanding of what features make these patterns distinctive and effective against predators, or how they are designed to exploit the ways in which predators see the world. Indeed, definitions of aposematic patterns are loosely descriptive, for example, they are described as 'striking', 'conspicuous' or 'distinctive'. But what is it that sets these patterns apart from others in the natural world: what makes an effective warning signal?

In this project, we propose to tackle this important question through mathematical modeling, and experiments with chicks and humans. For the first time, we will measure the patterns of aposematic and non-aposematic species, and use image processing techniques to quantify the characteristics of aposematic patterns. We will collect photographs of butterflies, moths and beetles from museum collections using techniques that allow us to 'see' the patterns as a foraging bird would. For example, we know what colours and patterns avian visual systems are most sensitive to, and can calculate how warning signals stimulate their visual systems. Once the models have made their predictions we can test them using behavioural experiments that measure how birds react to the aposematic patterns and what features enhance prey survival.

We also plan to test a novel hypothesis for why aposematic patterns act as warning signals. Humans find particular classes of pattern (e.g. stripes or spots of specific sizes and arrangements) aversive or uncomfortable. It has been suggested that these patterns could 'overload' the brain, and make these signals aversive. We will build a computational model of the early stages of visual processing in the brain, and test if aposematic patterns do deliver excessive responses. We will then test the model by choosing patterns that should visually overload humans, and taking classic visual discomfort measures.

The project will allow us to understand the form and function of aposematic patterns, and finally give us a precise and working definition. The work will broad appeal to the general public, and potentially improve the efficacy of visual alerts and avian deterrents.</gtr:abstractText><gtr:technicalSummary>Aposematic species use 'distinctive' and 'conspicuous' colour patterns to advertise a defence, such as being toxic or unprofitable. Understanding why such vibrant warning signals have evolved has provided a test bed for evolutionary theory for more than 150 years. It is therefore surprising that we have no analytical understanding of signal design, i.e. what makes natural warning signals 'conspicuous' or 'distinct'. In this project we take a novel approach to characterise real aposematic patterns, model their effects on the perceptual systems of predators, and test what makes them 'special' via behavioural experiments. 
We will develop a database of hyperspectral images of aposematic (AP) and non-aposematic (Non-AP) species. This will allow us to compare the patterns of AP and Non-AP species in a principled way to identify key features of AP patterns. By mathematically analysing the database, we will deliver a set of specific visual features (colour and spatial pattern) that will define a pattern as being AP. We will test which of these features contribute to the efficacy of warning signals using avian predators (via unlearned wariness and avoidance learning paradigms). We will also predict and test the distance at which aposematic patterns become effective against predators.
 Recent developments in neuroscience suggest there may be more to AP patterns than being distinctive. Warning patterns may be effective because they are highly unusual in natural scenes, stimulating high levels of neural activity. We will test this 'visual overload hypothesis', using modeling and experiments, and establish if AP patterns trigger stronger brain activity than other conspicuous patterns.
 We will disseminate the work across relevant academic disciplines, to the public via school visits, science festivals and museum exhibits, and to stakeholders involved in development of high-visibility clothing and deterrents for birds.</gtr:technicalSummary><gtr:potentialImpactText>Our proposal is for core evolutionary and computational biology research with no immediate application to UK-plc. However, we have identified three groups of possible indirect stakeholders, and two groups of direct stakeholders.
(1) Technical developers of high-visibility systems
Our work could be of use to those interested in increasing the visibility of workers and sportsmen through high-visibility clothing. We have obtained interest in the idea from two companies specialising in high-performance sports-wear, Tineli and Raceskin. We will discuss our project findings with them to explore how to improve the design of highly visible sports clothing.
(2) Medical professionals and technologists
Our work will be of relevance to medical professionals interested in visual discomfort and in photo-sensitive epilepsy and allow the prediction of uncomfortable images. Harris has collaborative links with the Department of Optometry at Bradford University (Barratt, Bloj). She will arrange a talk to optometry professionals there on the implications of our work for understanding the symptoms of visual stress and discomfort.
(3) Developers of avian deterrents
Birds can cause significant damage to infrastructure and crops. Our data could lead to significant improvements in the design of avian deterrents, for example, on farmland, at airports and on powerlines. We have established contacts in industry and in wildlife control that we will exploit to explore the development and testing of novel deterrents.
(4) The general public
Our research topic is highly engaging for the general public, who have a passion for wildlife, particularly insects and birds. We will engage the public through school visits and the development of a linked classroom activity, events at local zoos and museums, natural history societies, public displays at Science Fairs, and interactive web-based demonstrations our laboratory websites. 
(5) Project researchers
The proposal is an interdisciplinary collaboration. Penacchio has a background in pure maths, but is on his way to becoming an interdisciplinary computational biologist. This project will add another string to his biology bow. Halpin is an expert in animal behaviour and cognition, particularly in how predators learn about what to eat. This project will enable her to better understand the sensory and neural processes underlying signal processing, and allow her to develop new skills in image acquisition and analysis. Both researchers will benefit from the development of new skills and knowledge that transcend traditional disciplinary boundaries.</gtr:potentialImpactText><gtr:fund><gtr:end>2019-03-31</gtr:end><gtr:funder url="http://gtr.rcuk.ac.uk:80/organisation/2512EF1C-401B-4222-9869-A770D4C5FAC7"><gtr:id>2512EF1C-401B-4222-9869-A770D4C5FAC7</gtr:id><gtr:name>BBSRC</gtr:name></gtr:funder><gtr:start>2016-04-01</gtr:start><gtr:type>INCOME_ACTUAL</gtr:type><gtr:valuePounds>8294</gtr:valuePounds></gtr:fund><gtr:output><gtr:artisticAndCreativeProductOutputs/><gtr:collaborationOutputs><gtr:collaborationOutput><gtr:collaboratingOrganisation>University of Birmingham</gtr:collaboratingOrganisation><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:department>School of Psychology Birmingham</gtr:department><gtr:description>ViiHM network: Visual image interpretation in humans and machines</gtr:description><gtr:id>1C2A5E22-2420-41FE-95D8-A3588CC15053</gtr:id><gtr:impact>None yet</gtr:impact><gtr:partnerContribution>A network funded by EPSRC -- Schofield at Birmingham is the PI</gtr:partnerContribution><gtr:piContribution>EPSRC funded network, I am a member.</gtr:piContribution><gtr:sector>Academic/University</gtr:sector><gtr:start>2014-01-01</gtr:start></gtr:collaborationOutput></gtr:collaborationOutputs><gtr:disseminationOutputs><gtr:disseminationOutput><gtr:description>Explorathon 2016:  Harris lab exhibit at this European Researchers night event, highlighting research activity on vision.</gtr:description><gtr:form>Participation in an open day or visit at my research institution</gtr:form><gtr:geographicReach>Regional</gtr:geographicReach><gtr:id>E5D9CC5A-1B11-4B73-9F25-350B0B218494</gtr:id><gtr:impact>Part of a European Researchers Night event, showing academic research taking place at the University of St. Andrews</gtr:impact><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Harris talk at Perceptual Representation of Illumination, Shape &amp; Material Conference 2016</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>International</gtr:geographicReach><gtr:id>BB5D5562-8B15-4F9A-990E-CEF52394D122</gtr:id><gtr:impact>Invited talk to EU postgraduate training network and industrial collaborators. Title: Counter-shading camouflage: shape from shading in nature.</gtr:impact><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Industry/Business</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Talk at local school</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>Local</gtr:geographicReach><gtr:id>752B9BED-1A19-41C5-A595-30C3251C9FE1</gtr:id><gtr:impact>Talk to GCSE and lower 6th form students on animal camouflage, followed by presentation and discussion on careers in biology.</gtr:impact><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Schools</gtr:primaryAudience><gtr:year>2017</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>International Women's Day - Women in Science</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>Local</gtr:geographicReach><gtr:id>9C7CB74C-ED3A-448C-920A-0798CCB61BAE</gtr:id><gtr:impact>The invited talk was to undergraduates and postgraduates at Newcastle University, with the aim of supporting women in their scientific career aspirations. I spoke about my career in science (including my BBSRC-funded research), the challenges associated with being a parent, and what I advice I would give my younger self.</gtr:impact><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Undergraduate students</gtr:primaryAudience><gtr:year>2017</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Invited talk at a science education conference.</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>National</gtr:geographicReach><gtr:id>D9B1FF51-B15B-4D32-8924-AEE06DFADCAD</gtr:id><gtr:impact>Invited Talk at the Association for Science Education Conference (Dundee, 2017)&amp;gt;

https://www.ase.org.uk/conferences/scotland2017/</gtr:impact><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Schools</gtr:primaryAudience><gtr:url>https://www.ase.org.uk/conferences/scotland2017/</gtr:url><gtr:year>2017</gtr:year></gtr:disseminationOutput><gtr:disseminationOutput><gtr:description>Harris talk at Cafe Scientifique, Dunkeld and Pitlochry , Scotland</gtr:description><gtr:form>A talk or presentation</gtr:form><gtr:geographicReach>Regional</gtr:geographicReach><gtr:id>36228D88-AF89-45DF-A7C3-095DB520BCF1</gtr:id><gtr:impact>Harris talk at Cafe Scientifique, Dunkeld and Pitlochry , Scotland. Talk was primarily on basic 3D vision, but included discussion of how camouflage and animal patterning affects predation.</gtr:impact><gtr:partOfOfficialScheme>false</gtr:partOfOfficialScheme><gtr:primaryAudience>Public/other audiences</gtr:primaryAudience><gtr:year>2016</gtr:year></gtr:disseminationOutput></gtr:disseminationOutputs><gtr:exploitationOutputs/><gtr:furtherFundingOutputs><gtr:furtherFundingOutput><gtr:amountPounds>75000</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Newcastle University Research Excellence Fund</gtr:description><gtr:end>2019-09-02</gtr:end><gtr:fundingOrg>Newcastle University</gtr:fundingOrg><gtr:id>2A5212C5-65FF-4B90-A0C6-B003EC15328C</gtr:id><gtr:sector>Academic/University</gtr:sector><gtr:start>2016-10-01</gtr:start></gtr:furtherFundingOutput><gtr:furtherFundingOutput><gtr:amountPounds>700</gtr:amountPounds><gtr:country>United Kingdom of Great Britain &amp; Northern Ireland (UK)</gtr:country><gtr:currCode>GBP</gtr:currCode><gtr:currCountryCode>United Kingdom</gtr:currCountryCode><gtr:currLang>en_GB</gtr:currLang><gtr:description>Applied Vision Association postdoc travel award (Penacchio)</gtr:description><gtr:end>2016-06-02</gtr:end><gtr:fundingOrg>Applied Vision Association</gtr:fundingOrg><gtr:id>FFD47442-E68A-4580-A7D9-514939775A4E</gtr:id><gtr:sector>Private</gtr:sector><gtr:start>2016-06-01</gtr:start></gtr:furtherFundingOutput></gtr:furtherFundingOutputs><gtr:impactSummaryOutputs/><gtr:intellectualPropertyOutputs/><gtr:keyFindingsOutput><gtr:description>Our new study in virtual reality has shown that visual patterns associated with some forms of visual discomfort in a subset of individuals are avoided in a VR navigation task by ALL participants. This is finding that is unreported elsewhere.</gtr:description><gtr:exploitationPathways>The patterning that cause wariness in our participants could be adopted in the creation of safety wear and warning signals.
They also inform decisions about patterns that might cause discomfort to those suffering visual migraine.</gtr:exploitationPathways><gtr:id>BEED2697-F4AD-4582-9BD7-D9055423B876</gtr:id><gtr:sectors><gtr:sector>Aerospace, Defence and Marine,Leisure Activities, including Sports, Recreation and Tourism,Retail</gtr:sector></gtr:sectors></gtr:keyFindingsOutput><gtr:otherResearchOutputs/><gtr:policyInfluenceOutputs/><gtr:productOutputs/><gtr:researchDatabaseAndModelOutputs/><gtr:researchMaterialOutputs/><gtr:softwareAndTechnicalProductOutputs><gtr:softwareAndTechnicalProductOutput><gtr:description>With industrial partners we have developed new software to present virtual scenes containing patterned objects. This software will be used to explore behaviour in search and detection tasks as we manipulate warning colouration.</gtr:description><gtr:id>B5D01913-B821-4846-98C1-7D9F91C776B0</gtr:id><gtr:impact>We are still in the trial stage, experiments will be undertaking shortly.</gtr:impact><gtr:openSourceLicense>true</gtr:openSourceLicense><gtr:title>SCAMPERvr (Search ColourAtion Movement PattERn Virtual Reality)</gtr:title><gtr:type>Software</gtr:type><gtr:yearFirstProvided>2017</gtr:yearFirstProvided></gtr:softwareAndTechnicalProductOutput></gtr:softwareAndTechnicalProductOutputs><gtr:spinOutOutputs/></gtr:output><gtr:publications/><gtr:identifiers><gtr:identifier type="RCUK">BB/N005945/1</gtr:identifier></gtr:identifiers><gtr:healthCategories/><gtr:researchActivities/><gtr:researchSubjects><gtr:researchSubject><gtr:id>48D25546-6ADF-479A-8877-478CCDB1DC1F</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Animal Science</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>1A1A6805-9DC4-4BCE-BC70-9F2AA4FD093B</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Ecol, biodivers. &amp; systematics</gtr:text></gtr:researchSubject><gtr:researchSubject><gtr:id>5858EC49-4786-4440-8352-1AB0B6DC5F23</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Psychology</gtr:text></gtr:researchSubject></gtr:researchSubjects><gtr:researchTopics><gtr:researchTopic><gtr:id>790AD28C-6380-4025-83C2-6881B93C4602</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Animal behaviour</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>685A8D5E-BD8A-4D8D-BAC5-607439217156</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Behavioural Ecology</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>E1AC33C6-9927-41AC-B23B-2EED8F593588</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Experimental Psychology</gtr:text></gtr:researchTopic><gtr:researchTopic><gtr:id>F439A20B-A9B0-4A68-B703-7F6AE7570E39</gtr:id><gtr:percentage>0</gtr:percentage><gtr:text>Systems neuroscience</gtr:text></gtr:researchTopic></gtr:researchTopics><gtr:rcukProgrammes/></gtr:project></gtr:projectComposition></gtr:projectOverview>